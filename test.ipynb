{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!createdb -h fonduer-postgres-dev -U postgres jkracht\n",
    "#!dropdb -h fonduer-postgres-dev -U postgres jkracht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/site-packages (from click->nltk) (5.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[2023-05-24 13:35:02,083][INFO] fonduer.meta:53 - Logging was already initialized to use logs/2023-05-24_09-02-18.  To configure logging manually, call fonduer.init_logging before initialiting Meta.\n",
      "[2023-05-24 13:35:02,085][INFO] fonduer.meta:135 - Connecting user:postgres to fonduer-postgres-dev:5432/jkracht\n",
      "[2023-05-24 13:35:02,087][INFO] fonduer.meta:162 - Initializing the storage schema\n",
      "[2023-05-24 13:35:02,137][INFO] fonduer.utils.udf:67 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabbdfed088a456681e463e5ab24a0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'nltk'])\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from fonduer.candidates.matchers import LambdaFunctionMatcher, Union, Intersect, RegexMatchSpan\n",
    "\n",
    "from fonduer import Meta, init_logging\n",
    "from fonduer.parser.preprocessors import HTMLDocPreprocessor\n",
    "from fonduer.parser import Parser\n",
    "from fonduer.parser.models import Document, Sentence\n",
    "\n",
    "PARALLEL = 1\n",
    "ATTRIBUTE = \"jkracht\"\n",
    "conn_string = f'postgresql://postgres@fonduer-postgres-dev:5432/{ATTRIBUTE}'\n",
    "\n",
    "init_logging(log_dir=\"logs\")\n",
    "\n",
    "session = Meta.init(conn_string).Session()\n",
    "\n",
    "# Document parser\n",
    "docs_path = \"data/test_collection2/\"\n",
    "doc_preprocessor = HTMLDocPreprocessor(docs_path)\n",
    "\n",
    "corpus_parser = Parser(session, structural=True, lingual=True)\n",
    "corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Docs: 2\n",
      "Num Sections: 4\n",
      "Num Tables: 0\n",
      "Num Cells: 0\n",
      "Num Sentences: 3470\n",
      "Num Paragraphs: 2591\n",
      "Num Figures: 40\n",
      "Num Captions: 0\n"
     ]
    }
   ],
   "source": [
    "from fonduer.parser.models import Document, Section, Table, Cell, Paragraph, Sentence, Figure, Caption\n",
    "print(f\"Num Docs: {session.query(Document).count()}\")\n",
    "\n",
    "print(f\"Num Sections: {session.query(Section).count()}\")\n",
    "print(f\"Num Tables: {session.query(Table).count()}\")\n",
    "print(f\"Num Cells: {session.query(Cell).count()}\")\n",
    "print(f\"Num Sentences: {session.query(Sentence).count()}\")\n",
    "print(f\"Num Paragraphs: {session.query(Paragraph).count()}\")\n",
    "\n",
    "print(f\"Num Figures: {session.query(Figure).count()}\")\n",
    "print(f\"Num Captions: {session.query(Caption).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-24 14:12:56,062][INFO] fonduer.candidates.mentions:467 - Clearing table: task\n",
      "[2023-05-24 14:12:56,071][INFO] fonduer.utils.udf:67 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658b23a40d074b88a441ce505c0c6680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tasks: 28\n"
     ]
    }
   ],
   "source": [
    "# mention\n",
    "from fonduer.candidates.models import mention_subclass\n",
    "from fonduer.candidates import MentionExtractor\n",
    "from fonduer.candidates import MentionNgrams\n",
    "from fonduer.candidates.mentions import MentionSentences, MentionCaptions, MentionParagraphs, MentionDocuments, MentionTables\n",
    "\n",
    "Task = mention_subclass(\"Task\")\n",
    "\n",
    "\n",
    "list_of_headlines = [\"Acknowledgements\", \"Acknowledgement\", \"acknowledgements\", \"acknowledgement\", \n",
    "                     #\"Contributions\", \"Contribution\", \"contribution\", \"contributions\",\n",
    "                     #\"Credits\", \"Credit\", \"credits\", \"credit\",\n",
    "                     \"Überschrift 1\"]\n",
    "    \n",
    "def mention_span_in_acknowledments_matches_verb(mention):\n",
    "    span_string = mention.get_span()\n",
    "    try:\n",
    "        # get last paragraphs first sentence (headline of the paragraph)\n",
    "        headline_of_last_paragraph = session.query(Paragraph).get(mention.sentence.paragraph_id-1).sentences[0].text\n",
    "\n",
    "        # check if last headline is listed to extract mentions of\n",
    "        #if headline_of_last_paragraph in list_of_headlines:\n",
    "        if any(option in headline_of_last_paragraph for option in list_of_headlines):\n",
    "\n",
    "            #test if span is a verb\n",
    "            for word in wn.synsets(span_string):\n",
    "                if word.pos() == \"v\": # and word.name().split(\".\")[0] == span_string.lower():\n",
    "                    return True # case: span is a ver in a wanted paragraph\n",
    "\n",
    "            return False # case: span is not a verb\n",
    "        \n",
    "        else:\n",
    "            return False # case: span not in wanted paragraph\n",
    "    except:\n",
    "        return False # case: no prior paragraph\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "matcher_task1 = LambdaFunctionMatcher(func = mention_span_in_acknowledments_matches_verb)\n",
    "\n",
    "\n",
    "#task_sentences = MentionSentences() ## hier muss noch die Mention-Space eingetragen werden\n",
    "#task_paragraph = MentionParagraphs()\n",
    "task_space = MentionNgrams(n_min=1, n_max=1)\n",
    "\n",
    "matchers_task = Intersect(\n",
    "    matcher_task1,\n",
    ")\n",
    "\n",
    "docs = session.query(Document).all()\n",
    "\n",
    "mention_extractor = MentionExtractor(\n",
    "    session,\n",
    "    [Task],\n",
    "    [task_space], #task_sentences #task_space\n",
    "    [matchers_task],\n",
    "    parallelism=PARALLEL,\n",
    ")\n",
    "\n",
    "mention_extractor.apply(docs, parallelism=PARALLEL, clear=True)\n",
    "print(\n",
    "    f\"Number of Tasks: {session.query(Task).count()}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 10.1101.001768\n",
      "Document 10.1101.2019.12.12.873844\n"
     ]
    }
   ],
   "source": [
    "for i in session.query(Document).all():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are\n",
      "advise\n",
      "thank\n",
      "providing\n",
      "are\n",
      "participated\n",
      "modeling\n",
      "work\n",
      "Work\n",
      "was\n",
      "funded\n",
      "Research\n",
      "grants\n",
      "Research\n",
      "received\n",
      "Fund\n",
      "acknowledges\n",
      "support\n",
      "Engineering\n",
      "Chair\n",
      "Emerging\n",
      "programme\n",
      "acknowledges\n",
      "support\n",
      "Research\n",
      "Engineering\n",
      "Research\n",
      "well\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for mention in session.query(Task).all():\n",
    "    print(mention.context.get_span())\n",
    "    #print(\"\\n\")\n",
    "    if a > 100:\n",
    "        break\n",
    "    a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstract__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__mapper__', '__mapper_args__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__table__', '__table_args__', '__tablename__', '__weakref__', '_sa_class_manager', '_sa_instance_state', '_sa_registry', 'caption', 'caption_id', 'cell', 'cell_id', 'document', 'document_id', 'id', 'implicit_spans', 'metadata', 'name', 'position', 'registry', 'section', 'section_id', 'sentences', 'spans', 'stable_id', 'type']\n"
     ]
    }
   ],
   "source": [
    "a = session.query(Paragraph).get(3325)\n",
    "\n",
    "print(dir(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15446\n",
      "Überschrift 1\n",
      "15447\n",
      "I got this paragraph!\n",
      "15448\n",
      "Überschrift 2\n",
      "15449\n",
      "Diesen Abschnitt will ich nicht haben!\n",
      "15458\n",
      "Überschrift 1\n",
      "15459\n",
      "I have this paragraph!\n",
      "15460\n",
      "Überschrift 2\n",
      "15461\n",
      "Diesen Abschnitt will ich nicht haben!\n",
      "15470\n",
      "Überschrift 1\n",
      "15471\n",
      "I want this paragraph!\n",
      "15472\n",
      "Überschrift 2\n",
      "15473\n",
      "Diesen Abschnitt will ich nicht haben!\n"
     ]
    }
   ],
   "source": [
    "for a in session.query(Paragraph).all():\n",
    "    print(a.id)\n",
    "    print(a.sentences[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testbereich\n",
    "\n",
    "def mention_span_in_acknowledments_matches_verb(mention):\n",
    "    # read in span/word\n",
    "    span_string = mention\n",
    "\n",
    "    # Task(SpanMention(\"RefWorks Tagged\", sentence=61332, chars=[0,14], words=[0,1]))\n",
    "    \n",
    "    for word in span_string.split(): # mention.get_span()[0]\n",
    "        #print(word)\n",
    "        word_filtered = word.replace(\"!\",\"\")\n",
    "        try:\n",
    "            if wn.synsets(word_filtered)[0].pos() == \"v\":\n",
    "                #print(word)\n",
    "                return word_filtered\n",
    "        except:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "a = \"I played soccer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'played'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_span_in_acknowledments_matches_verb(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"want\")[0].pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "privation\n",
      "n\n",
      "lack\n",
      "n\n",
      "need\n",
      "n\n",
      "wish\n",
      "v\n",
      "desire\n",
      "v\n",
      "want\n",
      "v\n",
      "want\n",
      "v\n",
      "want\n",
      "v\n",
      "want\n"
     ]
    }
   ],
   "source": [
    "for a in wn.synsets(\"want\"):\n",
    "    print(a.pos())\n",
    "    print(a.name().split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sentence_of_span = mention.sentence.text\n",
    "    #print(sentence_of_span)\n",
    "    #x = mention._get_table()\n",
    "    # print(mention.sentence.paragraph_id)\n",
    "    # #print(mention.document_id)\n",
    "    # print(\"x\")\n",
    "    # print(mention.sentence.paragraph_id-1)\n",
    "    #print(mention.sentence.paragraph_id-1)\n",
    "    # a = session.query(Paragraph).get(mention.sentence.paragraph_id-1)\n",
    "    #print(a.sentences[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if any in \n",
    "\n",
    "a = [\"abc\", \"bbb\", \"adf\"]\n",
    "headline_of_last_paragraph = \"Acknowledgment\"\n",
    "\n",
    "if any(option in headline_of_last_paragraph for option in list_of_headlines):\n",
    "    print(\"T\")\n",
    "else:\n",
    "    print(\"F\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
