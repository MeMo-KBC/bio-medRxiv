{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!createdb -h fonduer-postgres-dev -U postgres jkracht\n",
    "#!dropdb -h fonduer-postgres-dev -U postgres jkracht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/site-packages (from click->nltk) (5.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[2023-05-24 21:03:04,370][INFO] fonduer.meta:53 - Logging was already initialized to use logs/2023-05-24_09-02-18.  To configure logging manually, call fonduer.init_logging before initialiting Meta.\n",
      "[2023-05-24 21:03:04,373][INFO] fonduer.meta:135 - Connecting user:postgres to fonduer-postgres-dev:5432/jkracht\n",
      "[2023-05-24 21:03:04,373][INFO] fonduer.meta:162 - Initializing the storage schema\n",
      "[2023-05-24 21:03:04,420][INFO] fonduer.utils.udf:67 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4405ceb51164e00bc1eec5fcbc560da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'nltk'])\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from fonduer.candidates.matchers import LambdaFunctionMatcher, Union, Intersect, RegexMatchSpan\n",
    "\n",
    "from fonduer import Meta, init_logging\n",
    "from fonduer.parser.preprocessors import HTMLDocPreprocessor\n",
    "from fonduer.parser import Parser\n",
    "from fonduer.parser.models import Document, Sentence\n",
    "\n",
    "PARALLEL = 3\n",
    "ATTRIBUTE = \"jkracht\"\n",
    "conn_string = f'postgresql://postgres@fonduer-postgres-dev:5432/{ATTRIBUTE}'\n",
    "\n",
    "init_logging(log_dir=\"logs\")\n",
    "\n",
    "session = Meta.init(conn_string).Session()\n",
    "\n",
    "# Document parser\n",
    "docs_path = \"data/test_collection1/\"\n",
    "doc_preprocessor = HTMLDocPreprocessor(docs_path)\n",
    "\n",
    "corpus_parser = Parser(session, structural=True, lingual=True)\n",
    "corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Docs: 1\n",
      "Num Sections: 1\n",
      "Num Tables: 0\n",
      "Num Cells: 0\n",
      "Num Sentences: 8\n",
      "Num Paragraphs: 4\n",
      "Num Figures: 0\n",
      "Num Captions: 0\n"
     ]
    }
   ],
   "source": [
    "from fonduer.parser.models import Document, Section, Table, Cell, Paragraph, Sentence, Figure, Caption\n",
    "print(f\"Num Docs: {session.query(Document).count()}\")\n",
    "\n",
    "print(f\"Num Sections: {session.query(Section).count()}\")\n",
    "print(f\"Num Tables: {session.query(Table).count()}\")\n",
    "print(f\"Num Cells: {session.query(Cell).count()}\")\n",
    "print(f\"Num Sentences: {session.query(Sentence).count()}\")\n",
    "print(f\"Num Paragraphs: {session.query(Paragraph).count()}\")\n",
    "\n",
    "print(f\"Num Figures: {session.query(Figure).count()}\")\n",
    "print(f\"Num Captions: {session.query(Caption).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-24 21:12:11,148][INFO] fonduer.candidates.mentions:467 - Clearing table: task\n",
      "[2023-05-24 21:12:11,150][INFO] fonduer.utils.udf:67 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4ca9bf5d2141d5a7a048df7770120d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "\n",
      "2\n",
      "x\n",
      "\n",
      "3\n",
      "x\n",
      "\n",
      "4\n",
      "x\n",
      "\n",
      "5\n",
      "x\n",
      "\n",
      "6\n",
      "x\n",
      "\n",
      "!\n",
      "x\n",
      "\n",
      "7\n",
      "x\n",
      "\n",
      "8\n",
      "x\n",
      "\n",
      "9\n",
      "x\n",
      "\n",
      "10\n",
      "x\n",
      "\n",
      "!\n",
      "x\n",
      "\n",
      "11\n",
      "x\n",
      "\n",
      "12\n",
      "x\n",
      "\n",
      "!\n",
      "x\n",
      "\n",
      "13\n",
      "x\n",
      "\n",
      "14\n",
      "x\n",
      "\n",
      "!\n",
      "x\n",
      "\n",
      "11\n",
      "x\n",
      "\n",
      "12\n",
      "x\n",
      "\n",
      "13\n",
      "x\n",
      "\n",
      "14\n",
      "x\n",
      "\n",
      "!\n",
      "x\n",
      "\n",
      "15\n",
      "x\n",
      "\n",
      "16\n",
      "x\n",
      "\n",
      "17\n",
      "x\n",
      "\n",
      "18\n",
      "x\n",
      "\n",
      "19\n",
      "x\n",
      "\n",
      "20\n",
      "x\n",
      "\n",
      ".\n",
      "Number of Tasks: 0\n"
     ]
    }
   ],
   "source": [
    "# mention\n",
    "from fonduer.candidates.models import mention_subclass\n",
    "from fonduer.candidates import MentionExtractor\n",
    "from fonduer.candidates import MentionNgrams\n",
    "from fonduer.candidates.mentions import MentionSentences, MentionCaptions, MentionParagraphs, MentionDocuments, MentionTables\n",
    "from fonduer.utils.data_model_utils.textual import get_neighbor_sentence_ngrams, get_left_ngrams\n",
    "Task = mention_subclass(\"Task\")\n",
    "\n",
    "\n",
    "list_of_headlines = [\"Acknowledgements\", \"Acknowledgement\", \"acknowledgements\", \"acknowledgement\", \n",
    "                     #\"Contributions\", \"Contribution\", \"contribution\", \"contributions\",\n",
    "                     #\"Credits\", \"Credit\", \"credits\", \"credit\",\n",
    "                     \"Überschrift 1\"]\n",
    "    \n",
    "def mention_span_in_acknowledments_matches_verb(mention):\n",
    "    span_string = mention.get_span()\n",
    "\n",
    "    try:\n",
    "        # get last paragraphs first sentence (headline of the paragraph)\n",
    "        headline_of_last_paragraph = session.query(Paragraph).get(mention.sentence.paragraph_id-1).sentences[0].text\n",
    "\n",
    "        #x = get_neighbor_sentence_ngrams(mention, attrib=\"words\", n_min=1, n_max=1)\n",
    "        x = get_left_ngrams(mention, attrib=\"words\", n_min=1, n_max=1)\n",
    "        print(\"x\\n\")\n",
    "        print(span_string)\n",
    "        print(list(x))\n",
    "        #get_neighbor_sentence_ngrams()\n",
    "        # sentence_of_span = mention.sentence.text\n",
    "        # x = get_left_ngrams(mention, n = 1)\n",
    "        # print(sentence_of_span.x)\n",
    "\n",
    "\n",
    "        # check if last headline is listed to extract mentions of\n",
    "        #if headline_of_last_paragraph in list_of_headlines:\n",
    "        if any(option in headline_of_last_paragraph for option in list_of_headlines):\n",
    "\n",
    "            #test if span is a verb\n",
    "            for word in wn.synsets(span_string):\n",
    "                if word.pos() == \"v\": # and word.name().split(\".\")[0] == span_string.lower():\n",
    "                    return True # case: span is a ver in a wanted paragraph\n",
    "\n",
    "            return False # case: span is not a verb\n",
    "        \n",
    "        else:\n",
    "            return False # case: span not in wanted paragraph\n",
    "    except:\n",
    "        return False # case: no prior paragraph\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "matcher_task1 = LambdaFunctionMatcher(func = mention_span_in_acknowledments_matches_verb)\n",
    "\n",
    "\n",
    "#task_sentences = MentionSentences() ## hier muss noch die Mention-Space eingetragen werden\n",
    "#task_paragraph = MentionParagraphs()\n",
    "task_space = MentionNgrams(n_min=1, n_max=1)\n",
    "\n",
    "matchers_task = Intersect(\n",
    "    matcher_task1,\n",
    ")\n",
    "\n",
    "docs = session.query(Document).all()\n",
    "\n",
    "mention_extractor = MentionExtractor(\n",
    "    session,\n",
    "    [Task],\n",
    "    [task_space], #task_sentences #task_space\n",
    "    [matchers_task],\n",
    "    parallelism=PARALLEL,\n",
    ")\n",
    "\n",
    "mention_extractor.apply(docs, parallelism=PARALLEL, clear=True)\n",
    "print(\n",
    "    f\"Number of Tasks: {session.query(Task).count()}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document test copy\n",
      "Document test copy 2\n",
      "Document test\n"
     ]
    }
   ],
   "source": [
    "for i in session.query(Document).all():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will\n",
      "Blub\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for mention in session.query(Task).all():\n",
    "    print(mention.context.get_span())\n",
    "    #print(\"\\n\")\n",
    "    if a > 100:\n",
    "        break\n",
    "    a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__bool__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n"
     ]
    }
   ],
   "source": [
    "a = session.query(Paragraph).get(3325)\n",
    "\n",
    "print(dir(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28984\n",
      "Überschrift 1\n",
      "28987\n",
      "I have this paragraph!\n",
      "28989\n",
      "Überschrift 2\n",
      "28992\n",
      "Diesen Abschnitt will ich nicht haben!\n",
      "28985\n",
      "Überschrift 1\n",
      "28988\n",
      "I got this paragraph!\n",
      "28990\n",
      "Überschrift 2\n",
      "28993\n",
      "Diesen Abschnitt will ich nicht haben!\n",
      "28991\n",
      "Überschrift 1\n",
      "28994\n",
      "I want this paragraph!\n",
      "28995\n",
      "Überschrift 2\n",
      "28997\n",
      "Diesen Abschnitt will ich nicht haben!\n"
     ]
    }
   ],
   "source": [
    "for a in session.query(Paragraph).all():\n",
    "    print(a.id)\n",
    "    print(a.sentences[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1869\n",
      "1870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object get_neighbor_sentence_ngrams at 0x7f456fb490d0>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = session.query(Task).all()\n",
    "for b in a:\n",
    "    print(b.id)\n",
    "\n",
    "\n",
    "# b = a.sentences[0].text\n",
    "\n",
    "get_neighbor_sentence_ngrams(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bla Blub.'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = session.query(Sentence).get(29011)\n",
    "a.text\n",
    "a.get_neighbor_sentence_ngrams(window = 1, n= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sentence_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_813606/3424386646.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmention_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_neighbor_sentence_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sentence_id'"
     ]
    }
   ],
   "source": [
    "mention_id = 1\n",
    "window = 2  # Die Anzahl der vorherigen und nachfolgenden Sätze, die berücksichtigt werden sollen\n",
    "n = 3  # Die Anzahl der Wörter in jedem n-Gramm\n",
    "\n",
    "mention = session.query(Task).filter(Task.id == mention_id).first()\n",
    "sentence = session.query(Sentence).filter(Sentence.id == mention.sentence_id).first()\n",
    "\n",
    "ngrams = sentence.get_neighbor_sentence_ngrams(window=window, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testbereich\n",
    "\n",
    "def mention_span_in_acknowledments_matches_verb(mention):\n",
    "    # read in span/word\n",
    "    span_string = mention\n",
    "\n",
    "    # Task(SpanMention(\"RefWorks Tagged\", sentence=61332, chars=[0,14], words=[0,1]))\n",
    "    \n",
    "    for word in span_string.split(): # mention.get_span()[0]\n",
    "        #print(word)\n",
    "        word_filtered = word.replace(\"!\",\"\")\n",
    "        try:\n",
    "            if wn.synsets(word_filtered)[0].pos() == \"v\":\n",
    "                #print(word)\n",
    "                return word_filtered\n",
    "        except:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "a = \"I played soccer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'played'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_span_in_acknowledments_matches_verb(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"want\")[0].pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "privation\n",
      "n\n",
      "lack\n",
      "n\n",
      "need\n",
      "n\n",
      "wish\n",
      "v\n",
      "desire\n",
      "v\n",
      "want\n",
      "v\n",
      "want\n",
      "v\n",
      "want\n",
      "v\n",
      "want\n"
     ]
    }
   ],
   "source": [
    "for a in wn.synsets(\"want\"):\n",
    "    print(a.pos())\n",
    "    print(a.name().split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sentence_of_span = mention.sentence.text\n",
    "    #print(sentence_of_span)\n",
    "    #x = mention._get_table()\n",
    "    # print(mention.sentence.paragraph_id)\n",
    "    # #print(mention.document_id)\n",
    "    # print(\"x\")\n",
    "    # print(mention.sentence.paragraph_id-1)\n",
    "    #print(mention.sentence.paragraph_id-1)\n",
    "    # a = session.query(Paragraph).get(mention.sentence.paragraph_id-1)\n",
    "    #print(a.sentences[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if any in \n",
    "\n",
    "a = [\"abc\", \"bbb\", \"adf\"]\n",
    "headline_of_last_paragraph = \"Acknowledgment\"\n",
    "\n",
    "if any(option in headline_of_last_paragraph for option in list_of_headlines):\n",
    "    print(\"T\")\n",
    "else:\n",
    "    print(\"F\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
