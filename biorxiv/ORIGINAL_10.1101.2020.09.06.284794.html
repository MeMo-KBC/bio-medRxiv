<html lang="en" dir="ltr" xmlns="http://www.w3.org/1999/xhtml" xmlns:mml="http://www.w3.org/1998/Math/MathML">
 <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book#">
  <link rel="dns-prefetch" href="//d33xdlntwy0kbs.cloudfront.net"/>
  <link rel="dns-prefetch" href="//www.google.com"/>
  <link rel="dns-prefetch" href="//scholar.google.com"/>
  <link rel="dns-prefetch" href="//www.google-analytics.com"/>
  <link rel="dns-prefetch" href="//stats.g.doubleclick.net"/>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <link rel="shortcut icon" href="https://www.biorxiv.org/sites/default/files/images/favicon.ico" type="image/vnd.microsoft.icon"/>
  <link rel="alternate" type="application/pdf" title="Full Text (PDF)" href="/content/10.1101/2020.09.06.284794v2.full.pdf"/>
  <link rel="alternate" type="text/plain" title="Full Text (Plain)" href="/content/10.1101/2020.09.06.284794v2.full.txt"/>
  <link rel="alternate" type="application/vnd.ms-powerpoint" title="Powerpoint" href="/content/10.1101/2020.09.06.284794v2.ppt"/>
  <meta name="article_thumbnail" content="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-1.gif"/>
  <meta name="type" content="article"/>
  <meta name="category" content="article"/>
  <meta name="HW.identifier" content="/biorxiv/early/2022/03/22/2020.09.06.284794.atom"/>
  <meta name="HW.pisa" content="biorxiv;2020.09.06.284794v2"/>
  <meta name="DC.Format" content="text/html"/>
  <meta name="DC.Language" content="en"/>
  <meta name="DC.Title" content="Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity"/>
  <meta name="DC.Identifier" content="10.1101/2020.09.06.284794"/>
  <meta name="DC.Date" content="2022-03-22"/>
  <meta name="DC.Publisher" content="Cold Spring Harbor Laboratory"/>
  <meta name="DC.Rights" content="© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/"/>
  <meta name="DC.AccessRights" content="restricted"/>
  <meta name="DC.Description" content="Reconstructing natural images and decoding their semantic category from fMRI brain recordings is challenging. Acquiring sufficient pairs of images and their corresponding fMRI responses, which span the huge space of natural images, is prohibitive. We present a novel self-supervised approach that goes well beyond the scarce paired data, for achieving both: (i) state-of-the art fMRI-to-image reconstruction, and (ii) first-ever large-scale semantic classification from fMRI responses. By imposing cycle consistency between a pair of deep neural networks (from image-to-fMRI & from fMRI-to-image), we train our image reconstruction network on a large number of “unpaired” natural images (images without fMRI recordings) from many novel semantic categories. This enables to adapt our reconstruction network to a very rich semantic coverage without requiring any explicit semantic supervision. Specifically, we find that combining our self-supervised training with high-level perceptual losses , gives rise to new reconstruction & classification capabilities. In particular, this perceptual training enables to classify well fMRIs of never-before-seen semantic classes, without requiring any class labels during training . This gives rise to: (i) Unprecedented image-reconstruction from fMRI of never-before-seen images (evaluated by image metrics and human testing), and (ii) Large-scale semantic classification of categories that were never-before-seen during network training. Such large-scale (1000-way) semantic classification from fMRI recordings has never been demonstrated before . Finally, we provide evidence for the biological consistency of our learned model.

### Competing Interest Statement

The authors have declared no competing interest."/>
  <meta name="DC.Contributor" content="Guy Gaziv"/>
  <meta name="DC.Contributor" content="Roman Beliy"/>
  <meta name="DC.Contributor" content="Niv Granot"/>
  <meta name="DC.Contributor" content="Assaf Hoogi"/>
  <meta name="DC.Contributor" content="Francesca Strappini"/>
  <meta name="DC.Contributor" content="Tal Golan"/>
  <meta name="DC.Contributor" content="Michal Irani"/>
  <meta name="article:published_time" content="2022-03-22"/>
  <meta name="article:section" content="New Results"/>
  <meta name="citation_title" content="Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity"/>
  <meta name="citation_abstract" lang="en" content="<h3>Abstract</h3>
<p>Reconstructing natural images and decoding their semantic category from fMRI brain recordings is challenging. Acquiring sufficient pairs of images and their corresponding fMRI responses, which span the huge space of natural images, is prohibitive. We present a novel <i>self-supervised</i> approach that goes well beyond the scarce paired data, for achieving both: (i) state-of-the art fMRI-to-image reconstruction, and (ii) first-ever large-scale semantic classification from fMRI responses. By imposing cycle consistency between a pair of deep neural networks (from image-to-fMRI &amp; from fMRI-to-image), we train our image reconstruction network on a large number of “unpaired” natural images (images without fMRI recordings) from many novel semantic categories. This enables to adapt our reconstruction network to a very rich semantic coverage without requiring any explicit semantic supervision. Specifically, we find that combining our self-supervised training with <i>high-level perceptual losses</i>, gives rise to new reconstruction &amp; classification capabilities. In particular, this perceptual training enables to classify well fMRIs of never-before-seen semantic classes, <i>without requiring any class labels during training</i>. This gives rise to: (i) Unprecedented image-reconstruction from fMRI of never-before-seen images (evaluated by image metrics and human testing), and (ii) Large-scale semantic classification of categories that were never-before-seen during network training. <i>Such large-scale (1000-way) semantic classification from fMRI recordings has never been demonstrated before</i>. Finally, we provide evidence for the biological consistency of our learned model.</p>"/>
  <meta name="citation_journal_title" content="bioRxiv"/>
  <meta name="citation_publisher" content="Cold Spring Harbor Laboratory"/>
  <meta name="citation_publication_date" content="2022/01/01"/>
  <meta name="citation_mjid" content="biorxiv;2020.09.06.284794v2"/>
  <meta name="citation_id" content="2020.09.06.284794v2"/>
  <meta name="citation_public_url" content="https://www.biorxiv.org/content/10.1101/2020.09.06.284794v2"/>
  <meta name="citation_abstract_html_url" content="https://www.biorxiv.org/content/10.1101/2020.09.06.284794v2.abstract"/>
  <meta name="citation_full_html_url" content="https://www.biorxiv.org/content/10.1101/2020.09.06.284794v2.full"/>
  <meta name="citation_pdf_url" content="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794.full.pdf"/>
  <meta name="citation_doi" content="10.1101/2020.09.06.284794"/>
  <meta name="citation_num_pages" content="26"/>
  <meta name="citation_article_type" content="Article"/>
  <meta name="citation_section" content="New Results"/>
  <meta name="citation_firstpage" content="2020.09.06.284794"/>
  <meta name="citation_author" content="Guy Gaziv"/>
  <meta name="citation_author_institution" content="Dept. of Computer Science and Applied Math, Weizmann Institute of Science"/>
  <meta name="citation_author_orcid" content="http://orcid.org/0000-0003-0890-6002"/>
  <meta name="citation_author" content="Roman Beliy"/>
  <meta name="citation_author_institution" content="Dept. of Computer Science and Applied Math, Weizmann Institute of Science"/>
  <meta name="citation_author" content="Niv Granot"/>
  <meta name="citation_author_institution" content="Dept. of Computer Science and Applied Math, Weizmann Institute of Science"/>
  <meta name="citation_author" content="Assaf Hoogi"/>
  <meta name="citation_author_institution" content="Dept. of Computer Science and Applied Math, Weizmann Institute of Science"/>
  <meta name="citation_author" content="Francesca Strappini"/>
  <meta name="citation_author_institution" content="Dept. of Neurobiology, Weizmann Institute of Science"/>
  <meta name="citation_author" content="Tal Golan"/>
  <meta name="citation_author_institution" content="Zuckerman Institute, Columbia University"/>
  <meta name="citation_author_orcid" content="http://orcid.org/0000-0002-7940-7473"/>
  <meta name="citation_author" content="Michal Irani"/>
  <meta name="citation_author_institution" content="Dept. of Computer Science and Applied Math, Weizmann Institute of Science"/>
  <meta name="citation_author_email" content="michal.irani@weizmann.ac.il"/>
  <meta name="citation_reference" content="B. Thirion, E. Duchesnay, E. Hubbard, J. Dubois, J.-B. Poline, D. Lebihan, and S. Dehaene, “Inverse retinotopy: Inferring the visual content of images from brain activation patterns,” NeuroImage, vol. 33, pp. 1104–1116, 12 2006."/>
  <meta name="citation_reference" content="Y. Miyawaki, H. Uchida, O. Yamashita, M.-a. Sato, Y. Morito, H. C. Tanabe, N. Sadato, and Y. Kamitani, “Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders,” Neuron, vol. 60, pp. 915–929, 12 2008."/>
  <meta name="citation_reference" content="J.-D. Haynes and G. Rees, “Decoding mental states from brain activity in humans,” Nature Reviews Neuroscience, vol. 7, pp. 523–534, 7 2006."/>
  <meta name="citation_reference" content="Y. Kamitani and F. Tong, “Decoding the visual and subjective contents of the human brain,” Nature Neuroscience, vol. 8, pp. 679–685, 5 2005."/>
  <meta name="citation_reference" content="Y. Kamitani and F. Tong, “Decoding seen and attended motion directions from activity in the human visual cortex,” Current biology : CB, vol. 16, pp. 1096–1102, 6 2006."/>
  <meta name="citation_reference" content="J. V. Haxby, M. I. Gobbini, M. L. Furey, A. Ishai, J. L. Schouten, and P. Pietrini, “Distributed and overlapping representations of faces and objects in ventral temporal cortex,” Science, vol. 293, pp. 2425–2430, 9 2001."/>
  <meta name="citation_reference" content="D. D. Cox and R. L. Savoy, “Functional magnetic resonance imaging (fMRI) ”brain reading”: detecting and classifying distributed patterns of fMRI activity in human visual cortex.,” NeuroImage, vol. 19, pp. 261–70, 6 2003."/>
  <meta name="citation_reference" content="K. M. O’Craven and N. Kanwisher, “Mental imagery of faces and places activates corresponding stiimulus-specific brain regions,” Journal of cognitive neuroscience, vol. 12, no. 6, pp. 1013–1023, 2000."/>
  <meta name="citation_reference" content="T. Naselaris, K. N. Kay, S. Nishimoto, and J. L. Gallant, “Encoding and decoding in fMRI.,” NeuroImage, vol. 56, pp. 400–10, 5 2011."/>
  <meta name="citation_reference" content="M. N. Hebart and C. I. Baker, “Deconstructing multivariate decoding for the study of brain function,” NeuroImage, vol. 180, pp. 4–18, 10 2018."/>
  <meta name="citation_reference" content="T. Naselaris, C. A. Olman, D. E. Stansbury, K. Ugurbil, and J. L. Gallant, “A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes,” NeuroImage, vol. 105, pp. 215–228, 1 2015."/>
  <meta name="citation_reference" content="G. Shen, T. Horikawa, K. Majima, and Y. Kamitani, “Deep image reconstruction from human brain activity,” PLOS Computational Biology, vol. 15, p. e1006633, 1 2019."/>
  <meta name="citation_reference" content="G. Shen, K. Dwivedi, K. Majima, T. Horikawa, and Y. Kamitani, “End-to-end deep image reconstruction from human brain activity,” Frontiers in Computational Neuroscience, vol. 13, p. 21, 4 2019."/>
  <meta name="citation_reference" content="R. M. Cichy, J. Heinzle, and J.-D. Haynes, “Imagery and Perception Share Cortical Representations of Content and Location,” Cerebral Cortex, vol. 22, pp. 372–380, 2 2012."/>
  <meta name="citation_reference" content="J. Pearson, T. Naselaris, E. A. Holmes, and S. M. Kosslyn, “Mental Imagery: Functional Mechanisms and Clinical Applications,” Trends in Cognitive Sciences, vol. 19, pp. 590–602, 10 2015."/>
  <meta name="citation_reference" content="T. Horikawa, M. Tamaki, Y. Miyawaki, and Y. Kamitani, “Neural Decoding of Visual Imagery During Sleep,” Science, vol. 340, no. 6132, 2013."/>
  <meta name="citation_reference" content="T. Horikawa and Y. Kamitani, “Hierarchical Neural Representation of Dreamed Objects Revealed by Brain Decoding with Deep Neural Network Features,” Frontiers in Computational Neuroscience, vol. 11, 2017."/>
  <meta name="citation_reference" content="K. Tripathy, Z. E. Markow, A. K. Fishell, A. Sherafati, T. M. Burns-Yocum, M. L. Schroeder, A. M. Svoboda, A. T. Eggebrecht, M. A. Anastasio, B. L. Schlaggar, and J. P. Culver, “Decoding visual information from high-density diffuse optical tomography neuroimaging data,” NeuroImage, vol. 226, p. 117516, 2 2021."/>
  <meta name="citation_reference" content="T. Milekovic, A. A. Sarma, D. Bacher, J. D. Simeral, J. Saab, C. Pandarinath, B. L. Sorice, C. Blabe, E. M. Oakley, K. R. Tringale, E. Eskandar, S. S. Cash, J. M. Henderson, K. V. Shenoy, J. P. Donoghue, and L. R. Hochberg, “Stable long-term BCI-enabled communication in ALS and locked-in syndrome using LFP signals,” Journal of neurophysiology, vol. 120, pp. 343–360, 7 2018."/>
  <meta name="citation_reference" content="L. Naci, M. M. Monti, D. Cruse, A. Kübler, B. Sorger, R. Goebel, B. Kotchoubey, and A. M. Owen, “Brain–computer interfaces for communication with nonresponsive patients,” Annals of Neurology, vol. 72, pp. 312–323, 9 2012."/>
  <meta name="citation_reference" content="M. M. Monti, A. Vanhaudenhuyse, M. R. Coleman, M. Boly, J. D. Pickard, L. Tshibanda, A. M. Owen, and S. Laureys, “Willful Modulation of Brain Activity in Disorders of Consciousness,” New England Journal of Medicine, vol. 362, pp. 579–589, 2 2010."/>
  <meta name="citation_reference" content="A. M. Owen, M. R. Coleman, M. Boly, M. H. Davis, S. Laureys, and J. D. Pickard, “Detecting awareness in the vegetative state,” Science, vol. 313, p. 1402, 9 2006."/>
  <meta name="citation_reference" content="A. Lage-Castellanos, G. Valente, E. Formisano, and F. De Martino, “Methods for computing the maximum performance of computational models of fMRI responses,” PLOS Computational Biology, vol. 15, p. e1006397, 3 2019."/>
  <meta name="citation_reference" content="G. H. Glover, “Overview of functional magnetic resonance imaging,” 4 2011."/>
  <meta name="citation_reference" content="M. F. Glasser, S. N. Sotiropoulos, J. A. Wilson, T. S. Coalson, B. Fischl, J. L. Andersson, J. Xu, S. Jbabdi, M. Webster, J. R. Polimeni, D. C. Van Essen, M. Jenkinson, and WU-Minn HCP Consortium, “The minimal preprocessing pipelines for the Human Connectome Project,” NeuroImage, vol. 80, pp. 105–124, 10 2013."/>
  <meta name="citation_reference" content="K. N. Kay, T. Naselaris, R. J. Prenger, and J. L. Gallant, “Identifying natural images from human brain activity,” Nature, vol. 452, pp. 352–355, 3 2008."/>
  <meta name="citation_reference" content="T. Naselaris, R. J. Prenger, K. N. Kay, M. Oliver, and J. L. Gallant, “Bayesian Reconstruction of Natural Images from Human Brain Activity,” Neuron, vol. 63, pp. 902–915, 9 2009."/>
  <meta name="citation_reference" content="S. Nishimoto, A. T. Vu, T. Naselaris, Y. Benjamini, B. Yu, and J. L. Gallant, “Reconstructing visual experiences from brain activity evoked by natural movies.,” Current biology : CB, vol. 21, pp. 1641–6, 10 2011."/>
  <meta name="citation_reference" content="H. Wen, J. Shi, Y. Zhang, K.-H. Lu, J. Cao, and Z. Liu, “Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision,” Cerebral Cortex, vol. 28, pp. 4136–4160, 12 2018."/>
  <meta name="citation_reference" content="U. Guclu, M. A. J. van Gerven, U. Güçlü, and M. A. J. van Gerven, “Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream,” Journal of Neuroscience, vol. 35, pp. 10005–10014, 7 2015."/>
  <meta name="citation_reference" content="C. Zhang, K. Qiao, L. Wang, L. Tong, Y. Zeng, and B. Yan, “Constraint-Free Natural Image Reconstruction From fMRI Signals Based on Convolutional Neural Network,” Frontiers in Human Neuroscience, vol. 12, p. 242, 6 2018."/>
  <meta name="citation_reference" content="K. Han, H. Wen, J. Shi, K.-H. Lu, Y. Zhang, D. Fu, and Z. Liu, “Variational autoencoder: An unsupervised model for encoding and decoding fMRI activity in visual cortex,” NeuroImage, vol. 198, pp. 125–136, 9 2019."/>
  <meta name="citation_reference" content="Z. Ren, J. Li, X. Xue, X. Li, F. Yang, Z. Jiao, and X. Gao, “Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning,” NeuroImage, vol. 228, p. 117602, 3 2021."/>
  <meta name="citation_reference" content="M. Mozafari, L. Reddy, and R. Vanrullen, “Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN,” tech. rep."/>
  <meta name="citation_reference" content="K. Qiao, J. Chen, L. Wang, C. Zhang, L. Tong, and B. Yan, “BigGAN-based Bayesian Reconstruction of Natural Images from Human Brain Activity,” Neuroscience, vol. 444, pp. 92–105, 7 2020."/>
  <meta name="citation_reference" content="G. St-Yves and T. Naselaris, “Generative Adversarial Networks Conditioned on Brain Activity Reconstruct Seen Images,” in Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018, pp. 1054–1061, Institute of Electrical and Electronics Engineers Inc., 1 2019."/>
  <meta name="citation_reference" content="K. Seeliger, U. Güçlü, L. Ambrogioni, Y. Güçlütürk, and M. A. van Gerven, “Generative adversarial networks for reconstructing natural images from brain activity,” NeuroImage, vol. 181, pp. 775–785, 2018."/>
  <meta name="citation_reference" content="Y. Lin, J. Li, H. Wang, and S. Jiao, “DCNN-GAN: Reconstructing Realistic Image from fMRI,” tech. rep., 2019."/>
  <meta name="citation_reference" content="M. Eickenberg, A. Gramfort, G. Varoquaux, and B. Thirion, “Seeing it all: Convolutional network layers map the function of the human visual system,” NeuroImage, vol. 152, pp. 184–194, 2017."/>
  <meta name="citation_reference" content="T. Konkle and A. Caramazza, “Tripartite organization of the ventral stream by animacy and object size,” Journal of Neuroscience, vol. 33, pp. 10235–10242, 6 2013."/>
  <meta name="citation_reference" content="H. Wen, J. Shi, W. Chen, and Z. Liu, “Deep Residual Network Predicts Cortical Representation and Organization of Visual Features for Rapid Categorization,” Scientific Reports, vol. 8, p. 3752, 12 2018."/>
  <meta name="citation_reference" content="K. Qiao, J. Chen, L. Wang, C. Zhang, L. Zeng, L. Tong, and B. Yan, “Category decoding of visual stimuli from human brain activity using a bidirectional recurrent neural network to simulate bidirectional information flows in human visual cortices,” Frontiers in Neuroscience, vol. 13, no. JUL, 2019."/>
  <meta name="citation_reference" content="T. Horikawa and Y. Kamitani, “Generic decoding of seen and imagined objects using hierarchical visual features,” Nature Communications, vol. 8, pp. 1–15, 5 2017."/>
  <meta name="citation_reference" content="J. Deng, W. Dong, R. Socher, L.-J. Li, Kai Li, and Li Fei-Fei, “ImageNet: A large-scale hierarchical image database,” in 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255, IEEE, 6 2009."/>
  <meta name="citation_reference" content="R. Beliy, G. Gaziv, A. Hoogi, F. Strappini, T. Golan, and M. Irani, “From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI,” in Advances in Neural Information Processing Systems, 2019."/>
  <meta name="citation_reference" content="R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018."/>
  <meta name="citation_reference" content="K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” 9 2014."/>
  <meta name="citation_reference" content="G. St-Yves and T. Naselaris, “The feature-weighted receptive field: an interpretable encoding model for complex feature spaces,” 2017."/>
  <meta name="citation_reference" content="X. Glorot and Y. Bengio, “Understanding the difficulty of training deep feedforward neural networks,” 3 2010."/>
  <meta name="citation_reference" content="G. St-Yves and T. Naselaris, “The feature-weighted receptive field: an interpretable encoding model for complex feature spaces,” NeuroImage, vol. 180, pp. 188–202, 10 2018."/>
  <meta name="citation_reference" content="V. B. Mountcastle, “Modality and topographic properties of single neurons of cat’s somatic sensory cortex.,” Journal of neurophysiology, vol. 20, pp. 408–34, 7 1957."/>
  <meta name="citation_reference" content="D. Y. Ts’o, M. Zarella, and G. Burkitt, “Whither the hypercolumn?,” The Journal of physiology, vol. 587, pp. 2791–805, 6 2009."/>
  <meta name="citation_reference" content="T. Bonhoeffer and A. Grinvald, “Iso-orientation domains in cat visual cortex are arranged in pinwheel-like patterns,” Nature, vol. 353, pp. 429–431, 10 1991."/>
  <meta name="citation_reference" content="B. M. Dow, “Orientation and Color Columns in Monkey Visual Cortex,” Cerebral Cortex, vol. 12, pp. 1005–1015, 10 2002."/>
  <meta name="citation_reference" content="B. A. Wandell and J. Winawer, “Computational neuroimaging and population receptive fields,” Trends in Cognitive Sciences, vol. 19, pp. 349–357, 6 2015."/>
  <meta name="citation_reference" content="J. Gomez, V. Natu, B. Jeska, M. Barnett, and K. Grill-Spector, “Development differentially sculpts receptive fields across early and high-level human visual cortex,” Nature Communications, vol. 9, p. 788, 12 2018."/>
  <meta name="citation_reference" content="R. B. Tootell, M. S. Silverman, E. Switkes, and R. L. De Valois, “Deoxyglucose analysis of retinotopic organization in primate striate cortex,” Science, vol. 218, pp. 900–901, 11 1982."/>
  <meta name="citation_reference" content="M. I. Sereno, A. M. Dale, J. B. Reppas, K. K. Kwong, J. W. Belliveau, T. J. Brady, B. R. Rosen, and R. B. Tootell, “Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging,” Science, vol. 268, no. 5212, pp. 889–893, 1995."/>
  <meta name="citation_reference" content="H. Wen, J. Shi, W. Chen, and Z. Liu, “Transferring and generalizing deep-learning-based neural encoding models across subjects,” NeuroImage, vol. 176, pp. 152–163, 8 2018."/>
  <meta name="citation_reference" content="H. Wen, K. Han, J. Shi, Y. Zhang, E. Culurciello, and Z. Liu, “Deep Predictive Coding Network for Object Recognition,” 2018."/>
  <meta name="citation_reference" content="K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps,” tech. rep."/>
  <meta name="citation_reference" content="S. Grossman, G. Gaziv, E. M. Yeagle, M. Harel, P. Mégevand, D. M. Groppe, S. Khuvis, J. L. Herrero, M. Irani, A. D. Mehta, and R. Malach, “Convergent evolution of face spaces across human face-selective neuronal groups and deep convolutional networks,” Nature Communications, vol. 10, pp. 1–13, 12 2019."/>
  <meta name="twitter:title" content="Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity"/>
  <meta name="twitter:site" content="@biorxivpreprint"/>
  <meta name="twitter:card" content="summary"/>
  <meta name="twitter:image" content="https://www.biorxiv.org/sites/default/files/images/biorxiv_logo_homepage7-5-small.png"/>
  <meta name="twitter:description" content="Reconstructing natural images and decoding their semantic category from fMRI brain recordings is challenging. Acquiring sufficient pairs of images and their corresponding fMRI responses, which span the huge space of natural images, is prohibitive. We present a novel self-supervised approach that goes well beyond the scarce paired data, for achieving both: (i) state-of-the art fMRI-to-image reconstruction, and (ii) first-ever large-scale semantic classification from fMRI responses. By imposing cycle consistency between a pair of deep neural networks (from image-to-fMRI & from fMRI-to-image), we train our image reconstruction network on a large number of “unpaired” natural images (images without fMRI recordings) from many novel semantic categories. This enables to adapt our reconstruction network to a very rich semantic coverage without requiring any explicit semantic supervision. Specifically, we find that combining our self-supervised training with high-level perceptual losses , gives rise to new reconstruction & classification capabilities. In particular, this perceptual training enables to classify well fMRIs of never-before-seen semantic classes, without requiring any class labels during training . This gives rise to: (i) Unprecedented image-reconstruction from fMRI of never-before-seen images (evaluated by image metrics and human testing), and (ii) Large-scale semantic classification of categories that were never-before-seen during network training. Such large-scale (1000-way) semantic classification from fMRI recordings has never been demonstrated before . Finally, we provide evidence for the biological consistency of our learned model.

### Competing Interest Statement

The authors have declared no competing interest."/>
  <meta name="og-title" property="og:title" content="Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity"/>
  <meta name="og-url" property="og:url" content="https://www.biorxiv.org/content/10.1101/2020.09.06.284794v2"/>
  <meta name="og-site-name" property="og:site_name" content="bioRxiv"/>
  <meta name="og-description" property="og:description" content="Reconstructing natural images and decoding their semantic category from fMRI brain recordings is challenging. Acquiring sufficient pairs of images and their corresponding fMRI responses, which span the huge space of natural images, is prohibitive. We present a novel self-supervised approach that goes well beyond the scarce paired data, for achieving both: (i) state-of-the art fMRI-to-image reconstruction, and (ii) first-ever large-scale semantic classification from fMRI responses. By imposing cycle consistency between a pair of deep neural networks (from image-to-fMRI & from fMRI-to-image), we train our image reconstruction network on a large number of “unpaired” natural images (images without fMRI recordings) from many novel semantic categories. This enables to adapt our reconstruction network to a very rich semantic coverage without requiring any explicit semantic supervision. Specifically, we find that combining our self-supervised training with high-level perceptual losses , gives rise to new reconstruction & classification capabilities. In particular, this perceptual training enables to classify well fMRIs of never-before-seen semantic classes, without requiring any class labels during training . This gives rise to: (i) Unprecedented image-reconstruction from fMRI of never-before-seen images (evaluated by image metrics and human testing), and (ii) Large-scale semantic classification of categories that were never-before-seen during network training. Such large-scale (1000-way) semantic classification from fMRI recordings has never been demonstrated before . Finally, we provide evidence for the biological consistency of our learned model.

### Competing Interest Statement

The authors have declared no competing interest."/>
  <meta name="og-type" property="og:type" content="article"/>
  <meta name="og-image" property="og:image" content="https://www.biorxiv.org/sites/default/files/images/biorxiv_logo_homepage7-5-small.png"/>
  <meta name="citation_date" content="2022-03-22"/>
  <meta name="description" content="bioRxiv - the preprint server for biology, operated by Cold Spring Harbor Laboratory, a research and educational institution"/>
  <meta name="generator" content="Drupal 7 (http://drupal.org)"/>
  <link rel="canonical" href="https://www.biorxiv.org/content/10.1101/2020.09.06.284794v2"/>
  <link rel="shortlink" href="https://www.biorxiv.org/node/2459645"/>
  <title>
   Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity | bioRxiv
  </title>
  <link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__7SC0i-kTgUlQGKuqbmyS18Sez8FDO-aG9FSHkGrLGl8__9Xf-fJpmF9h8tvWywQTaOzJzFLt4sEfx108Wh1gYmm0__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.css" media="all"/>
  <link type="text/css" rel="stylesheet" href="//cdn.jsdelivr.net/qtip2/2.2.1/jquery.qtip.min.css" media="all"/>
  <link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__JbFqFjYGp4Zx8gvmj6v5YmfNmbiFphGHPblyC9bfG5Y__OScmsb_1nSVmm_Ax3cJ5Rq7p081PahYkvF_YWQd5GtE__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.css" media="all"/>
  <style type="text/css" media="all">
   /* &lt;![CDATA[ */
.panels-flexible-new .panels-flexible-region{padding:0}.panels-flexible-new .panels-flexible-region-inside{padding-right:.5em;padding-left:.5em}.panels-flexible-new .panels-flexible-region-inside-first{padding-left:0}.panels-flexible-new .panels-flexible-region-inside-last{padding-right:0}.panels-flexible-new .panels-flexible-column{padding:0}.panels-flexible-new .panels-flexible-column-inside{padding-right:.5em;padding-left:.5em}.panels-flexible-new .panels-flexible-column-inside-first{padding-left:0}.panels-flexible-new .panels-flexible-column-inside-last{padding-right:0}.panels-flexible-new .panels-flexible-row{padding:0 0 .5em;margin:0}.panels-flexible-new .panels-flexible-row-last{padding-bottom:0}.panels-flexible-column-new-main{float:left;width:99.0000%}.panels-flexible-new-inside{padding-right:0}.panels-flexible-new{width:auto}.panels-flexible-region-new-center{float:left;width:99.0000%}.panels-flexible-row-new-main-row-inside{padding-right:0}
/* ]]&gt; */
  </style>
  <link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__PWuQ_RYRTJ4BLEKsbWQeHysPg0gOQ3571ruQa_rXvAo__cj81oxtSGUe-AIO3tlkSBJPNpKP5ZgLdB8f8awBQsWU__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.css" media="all"/>
  <style type="text/css" media="all">
   /* &lt;![CDATA[ */
#sliding-popup.sliding-popup-bottom,#sliding-popup.sliding-popup-bottom .eu-cookie-withdraw-banner,.eu-cookie-withdraw-tab{background:gray}#sliding-popup.sliding-popup-bottom.eu-cookie-withdraw-wrapper{background:transparent}#sliding-popup .popup-content #popup-text h1,#sliding-popup .popup-content #popup-text h2,#sliding-popup .popup-content #popup-text h3,#sliding-popup .popup-content #popup-text p,.eu-cookie-compliance-secondary-button,.eu-cookie-withdraw-tab{color:#fff !important}.eu-cookie-withdraw-tab{border-color:#fff}.eu-cookie-compliance-more-button{color:#fff !important}
/* ]]&gt; */
  </style>
  <link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__Wnlyen9qEpwh_Qaf9okEu4QdVGM0BDothxeqA6Nbvo8__EJmw6SZD9bYoS8jocCpPYS3JFRURpdzmuvJoAUNiI-g__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.css" media="all"/>
  <link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__fse0T7HwKBxFeJ5ZSFW6P7GHsblWECJwe3f8Eo0Byb8__GnHmvNTOB0-xZRDC4PFtbOee_g-W6li7rpWRuc9O4B4__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.css" media="all"/>
  <link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__2WBMox6sOrN42ss5lCnH7WWVRdFdJCxtTKnQJYRwTE4__yqNvNYLvMpjy3ffuJrjjm9uW2i-Me1c23KLYuWHaqio__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.css" media="all"/>
  <link type="text/css" rel="stylesheet" href="https://www.biorxiv.org/sites/default/files/advagg_css/css__g_BFrOIH2jRr6w2kKikVPe0XcLWrQStWL-Z3vs6TbJQ__5c1ZiN_pM9tjxvpyA9VpEacScF0S_W4R222pC-s_-Pk__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.css" media="all"/>
  <link type="text/css" rel="stylesheet" href="https://d33xdlntwy0kbs.cloudfront.net/cshl_custom.css" media="all"/>
  <script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__BKYqkKToQ7EjirB7eIdMEH5521EU3da9IpoOs8Ex2XI__aSjVoX8giBmLhN2EbCgIGQJNu89Mh5aVu1LvI_gkJ7Y__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.js">
  </script>
  <script type="text/javascript" src="//cdn.jsdelivr.net/qtip2/2.2.1/jquery.qtip.min.js">
  </script>
  <script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__4Cn2dxvNlsJ-sHe6QOTLREaQvcqb0Yh0Zm9tTOHtQow__JeZEUjzbaj_yX6UjCI8eBbXy_J64ZVuoWmc2fSpLZHo__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.js">
  </script>
  <script type="text/javascript" src="https://www.google.com/recaptcha/api.js?hl=en&render=explicit&onload=drupalRecaptchaOnload">
  </script>
  <script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__dGWpV57YWu3sX6UOe04RMH-iP9jSkEP7Ajt0caYXZZk__u2clNMaOO3LCM0tpFNpsQhnHrJ4_SUXG1gFb00eB1iU__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.js">
  </script>
  <script type="text/javascript" async="async" src="https://scholar.google.com/scholar_js/casa.js">
  </script>
  <script type="text/javascript">
   &lt;!--//--&gt;&lt;![CDATA[//&gt;&lt;!--
/*!
 * yepnope1.5.4
 * (c) WTFPL, GPLv2
 */
(function(a,b,c){function d(a){return"[object Function]"==o.call(a)}function e(a){return"string"==typeof a}function f(){}function g(a){return!a||"loaded"==a||"complete"==a||"uninitialized"==a}function h(){var a=p.shift();q=1,a?a.t?m(function(){("c"==a.t?B.injectCss:B.injectJs)(a.s,0,a.a,a.x,a.e,1)},0):(a(),h()):q=0}function i(a,c,d,e,f,i,j){function k(b){if(!o&amp;&amp;g(l.readyState)&amp;&amp;(u.r=o=1,!q&amp;&amp;h(),l.onload=l.onreadystatechange=null,b)){"img"!=a&amp;&amp;m(function(){t.removeChild(l)},50);for(var d in y[c])y[c].hasOwnProperty(d)&amp;&amp;y[c][d].onload()}}var j=j||B.errorTimeout,l=b.createElement(a),o=0,r=0,u={t:d,s:c,e:f,a:i,x:j};1===y[c]&amp;&amp;(r=1,y[c]=[]),"object"==a?l.data=c:(l.src=c,l.type=a),l.width=l.height="0",l.onerror=l.onload=l.onreadystatechange=function(){k.call(this,r)},p.splice(e,0,u),"img"!=a&amp;&amp;(r||2===y[c]?(t.insertBefore(l,s?null:n),m(k,j)):y[c].push(l))}function j(a,b,c,d,f){return q=0,b=b||"j",e(a)?i("c"==b?v:u,a,b,this.i++,c,d,f):(p.splice(this.i++,0,a),1==p.length&amp;&amp;h()),this}function k(){var a=B;return a.loader={load:j,i:0},a}var l=b.documentElement,m=a.setTimeout,n=b.getElementsByTagName("script")[0],o={}.toString,p=[],q=0,r="MozAppearance"in l.style,s=r&amp;&amp;!!b.createRange().compareNode,t=s?l:n.parentNode,l=a.opera&amp;&amp;"[object Opera]"==o.call(a.opera),l=!!b.attachEvent&amp;&amp;!l,u=r?"object":l?"script":"img",v=l?"script":u,w=Array.isArray||function(a){return"[object Array]"==o.call(a)},x=[],y={},z={timeout:function(a,b){return b.length&amp;&amp;(a.timeout=b[0]),a}},A,B;B=function(a){function b(a){var a=a.split("!"),b=x.length,c=a.pop(),d=a.length,c={url:c,origUrl:c,prefixes:a},e,f,g;for(f=0;f&lt;d;f++)g=a[f].split("="),(e=z[g.shift()])&amp;&amp;(c=e(c,g));for(f=0;f&lt;b;f++)c=x[f](c);return c}function g(a,e,f,g,h){var i=b(a),j=i.autoCallback;i.url.split(".").pop().split("?").shift(),i.bypass||(e&amp;&amp;(e=d(e)?e:e[a]||e[g]||e[a.split("/").pop().split("?")[0]]),i.instead?i.instead(a,e,f,g,h):(y[i.url]?i.noexec=!0:y[i.url]=1,f.load(i.url,i.forceCSS||!i.forceJS&amp;&amp;"css"==i.url.split(".").pop().split("?").shift()?"c":c,i.noexec,i.attrs,i.timeout),(d(e)||d(j))&amp;&amp;f.load(function(){k(),e&amp;&amp;e(i.origUrl,h,g),j&amp;&amp;j(i.origUrl,h,g),y[i.url]=2})))}function h(a,b){function c(a,c){if(a){if(e(a))c||(j=function(){var a=[].slice.call(arguments);k.apply(this,a),l()}),g(a,j,b,0,h);else if(Object(a)===a)for(n in m=function(){var b=0,c;for(c in a)a.hasOwnProperty(c)&amp;&amp;b++;return b}(),a)a.hasOwnProperty(n)&amp;&amp;(!c&amp;&amp;!--m&amp;&amp;(d(j)?j=function(){var a=[].slice.call(arguments);k.apply(this,a),l()}:j[n]=function(a){return function(){var b=[].slice.call(arguments);a&amp;&amp;a.apply(this,b),l()}}(k[n])),g(a[n],j,b,n,h))}else!c&amp;&amp;l()}var h=!!a.test,i=a.load||a.both,j=a.callback||f,k=j,l=a.complete||f,m,n;c(h?a.yep:a.nope,!!i),i&amp;&amp;c(i)}var i,j,l=this.yepnope.loader;if(e(a))g(a,0,l,0);else if(w(a))for(i=0;i&lt;a.length;i++)j=a[i],e(j)?g(j,0,l,0):w(j)?B(j):Object(j)===j&amp;&amp;h(j,l);else Object(a)===a&amp;&amp;h(a,l)},B.addPrefix=function(a,b){z[a]=b},B.addFilter=function(a){x.push(a)},B.errorTimeout=1e4,null==b.readyState&amp;&amp;b.addEventListener&amp;&amp;(b.readyState="loading",b.addEventListener("DOMContentLoaded",A=function(){b.removeEventListener("DOMContentLoaded",A,0),b.readyState="complete"},0)),a.yepnope=k(),a.yepnope.executeStack=h,a.yepnope.injectJs=function(a,c,d,e,i,j){var k=b.createElement("script"),l,o,e=e||B.errorTimeout;k.src=a;for(o in d)k.setAttribute(o,d[o]);c=j?h:c||f,k.onreadystatechange=k.onload=function(){!l&amp;&amp;g(k.readyState)&amp;&amp;(l=1,c(),k.onload=k.onreadystatechange=null)},m(function(){l||(l=1,c(1))},e),i?k.onload():n.parentNode.insertBefore(k,n)},a.yepnope.injectCss=function(a,c,d,e,g,i){var e=b.createElement("link"),j,c=i?h:c||f;e.href=a,e.rel="stylesheet",e.type="text/css";for(j in d)e.setAttribute(j,d[j]);g||(n.parentNode.insertBefore(e,n),m(c,0))}})(this,document);

//--&gt;&lt;!]]&gt;
  </script>
  <script type="text/javascript">
   &lt;!--//--&gt;&lt;![CDATA[//&gt;&lt;!--
yepnope({
  test: Modernizr.matchmedia,
  nope: '/sites/all/libraries/media-match/media.match.min.js'
});
//--&gt;&lt;!]]&gt;
  </script>
  <script type="text/javascript">
   &lt;!--//--&gt;&lt;![CDATA[//&gt;&lt;!--
var _prum=[['id', '612e3d94173350001100005d'], ['mark', 'firstbyte',
      (new Date()).getTime()]]; (function() {
      var s=document.getElementsByTagName('script')[0],
      p=document.createElement('script');
      p.async='async'; p.src='//rum-static.pingdom.net/prum.min.js';s.parentNode.insertBefore(p,s);})();
//--&gt;&lt;!]]&gt;
  </script>
  <script type="text/javascript">
   &lt;!--//--&gt;&lt;![CDATA[//&gt;&lt;!--
if(typeof window.MathJax === "undefined") window.MathJax = { menuSettings: { zoom: "Click" } };
//--&gt;&lt;!]]&gt;
  </script>
  <script type="text/javascript">
   &lt;!--//--&gt;&lt;![CDATA[//&gt;&lt;!--
(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,"script","//www.google-analytics.com/analytics.js","ga");ga("create", "UA-45638026-1", {"cookieDomain":"auto"});ga("set", "page", location.pathname + location.search + location.hash);ga("send", "pageview");ga('create', 'UA-189672-38', 'auto', {'name': 'hwTracker'});
ga('set', 'anonymizeIp', true);
ga('hwTracker.send', 'pageview');
//--&gt;&lt;!]]&gt;
  </script>
  <script type="text/javascript">
   &lt;!--//--&gt;&lt;![CDATA[//&gt;&lt;!--
jQuery.extend(Drupal.settings,{"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"jcore_1","theme_token":"Srz88Zh1A574NiPb2X6BLfkq_xRFq9FveaR4HmRBwCk"},"colorbox":{"opacity":"0.85","current":"{current} of {total}","previous":"\u00ab Prev","next":"Next \u00bb","close":"Close","maxWidth":"98%","maxHeight":"98%","fixed":true,"mobiledetect":true,"mobiledevicewidth":"480px"},"highwire":{"markup":[{"requested":"short-text","variant":"abstract","view":"abstract","pisa":"biorxiv;2020.09.06.284794v2"},{"requested":"short-text","variant":"abstract","view":"abstract","pisa":"biorxiv;2020.09.06.284794v2"},{"requested":"full-text","variant":"full-text","view":"full","pisa":"biorxiv;2020.09.06.284794v2"}],"nid":"2459645","apath":"\/biorxiv\/early\/2022\/03\/22\/2020.09.06.284794.atom","pisa":"biorxiv;2020.09.06.284794v2","processed":["highwire_math"],"modal_window_width":"560","share_modal_width":"560","share_modal_title":"Share this Article"},"jcarousel":{"ajaxPath":"\/jcarousel\/ajax\/views"},"instances":"{\u0022highwire_abstract_tooltip\u0022:{\u0022content\u0022:{\u0022text\u0022:\u0022\u0022},\u0022style\u0022:{\u0022tip\u0022:{\u0022width\u0022:20,\u0022height\u0022:20,\u0022border\u0022:1,\u0022offset\u0022:0,\u0022corner\u0022:true},\u0022classes\u0022:\u0022qtip-custom hw-tooltip hw-abstract-tooltip qtip-shadow qtip-rounded\u0022,\u0022classes_custom\u0022:\u0022hw-tooltip hw-abstract-tooltip\u0022},\u0022position\u0022:{\u0022at\u0022:\u0022right center\u0022,\u0022my\u0022:\u0022left center\u0022,\u0022viewport\u0022:true,\u0022adjust\u0022:{\u0022method\u0022:\u0022shift\u0022}},\u0022show\u0022:{\u0022event\u0022:\u0022mouseenter click \u0022,\u0022solo\u0022:true},\u0022hide\u0022:{\u0022event\u0022:\u0022mouseleave \u0022,\u0022fixed\u0022:1,\u0022delay\u0022:\u0022100\u0022}},\u0022highwire_author_tooltip\u0022:{\u0022content\u0022:{\u0022text\u0022:\u0022\u0022},\u0022style\u0022:{\u0022tip\u0022:{\u0022width\u0022:15,\u0022height\u0022:15,\u0022border\u0022:1,\u0022offset\u0022:0,\u0022corner\u0022:true},\u0022classes\u0022:\u0022qtip-custom hw-tooltip hw-author-tooltip qtip-shadow qtip-rounded\u0022,\u0022classes_custom\u0022:\u0022hw-tooltip hw-author-tooltip\u0022},\u0022position\u0022:{\u0022at\u0022:\u0022top center\u0022,\u0022my\u0022:\u0022bottom center\u0022,\u0022viewport\u0022:true,\u0022adjust\u0022:{\u0022method\u0022:\u0022\u0022}},\u0022show\u0022:{\u0022event\u0022:\u0022mouseenter \u0022,\u0022solo\u0022:true},\u0022hide\u0022:{\u0022event\u0022:\u0022mouseleave \u0022,\u0022fixed\u0022:1,\u0022delay\u0022:\u0022100\u0022}},\u0022highwire_reflinks_tooltip\u0022:{\u0022content\u0022:{\u0022text\u0022:\u0022\u0022},\u0022style\u0022:{\u0022tip\u0022:{\u0022width\u0022:15,\u0022height\u0022:15,\u0022border\u0022:1,\u0022mimic\u0022:\u0022top center\u0022,\u0022offset\u0022:0,\u0022corner\u0022:true},\u0022classes\u0022:\u0022qtip-custom hw-tooltip hw-ref-link-tooltip qtip-shadow qtip-rounded\u0022,\u0022classes_custom\u0022:\u0022hw-tooltip hw-ref-link-tooltip\u0022},\u0022position\u0022:{\u0022at\u0022:\u0022bottom left\u0022,\u0022my\u0022:\u0022top left\u0022,\u0022viewport\u0022:true,\u0022adjust\u0022:{\u0022method\u0022:\u0022flip\u0022}},\u0022show\u0022:{\u0022event\u0022:\u0022mouseenter \u0022,\u0022solo\u0022:true},\u0022hide\u0022:{\u0022event\u0022:\u0022mouseleave \u0022,\u0022fixed\u0022:1,\u0022delay\u0022:\u0022100\u0022}}}","qtipDebug":"{\u0022leaveElement\u0022:0}","panel_ajax_tab":{"path":"sites\/all\/modules\/contrib\/panels_ajax_tab"},"disqus":{"domain":"biorxivstage","url":"https:\/\/www.biorxiv.org\/content\/10.1101\/2020.09.06.284794v2","title":"Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity","identifier":"node\/2459645"},"panels_ajax_pane":{"new-28877068-b9cb-4641-830b-b6b4638c98bb":"{\u0022encrypted\u0022:\u0022{\\\u0022encrypted\\\u0022:\\\u0022iRza52jEZ5MyyIrb72yUahVIQAZsh5gFgMOXg\\\\\\\/HCqm42Yi0dP2pYV\\\\\\\/mcHYfTt5r++WOS1FEw+Agq\\\\\\\/+Lo569c6suwQQ8F\\\\\\\/BL\\\\\\\/P4vbrH9vFp7t\\\\\\\/Vdb3VRohwNWbrlrVD1qr6N1iFTyg2gzK1IZwOi5Kg3c43QYVjGiWB09vANiZefh4uCuswzKOnI6REDw4lLFx6cfYlUTfFlHMNzZJVn8ZMOPpdIuJvge+mm2SrKIKjyZqvNNb30Oi37Xf7zXwKPZBX8qFZyhvc7\\\\\\\/TRpc\\\\\\\/E1uwJ4BoHGPGg34jltET7MxUGWlReNHq4ewq7Cw9\\\\\\\/Gi9mQf44iQNouNQ9wEQ2VgYChinKlo4aJKkpI648+busAYAEdN87Ddms48QD93dVWA\\\\\\\/GhbbeQVKgoNt5jzX9Jde4Pr9wjbN3GkbjzKrHGT3fZRq7nN7peJ7ABsS7An\\\\\\\/N080xCeLiimmOMRaOr2YJLU8NOE44Fi8yhK3wNF43USx7nWVGdmPMypwBi7tO04qaRWVvM8Jw4vJyde+7lOLHBTM7qjDubmk\\\\\\\/qg\\\\\\\/P2qfPTiuszS2pLRoJXmfi2txvCBzdRG+2AJenJCjgPErO2kmJwV2z76pD8vwettKnFlPi0MWnTnM6WXOpdh5Ga\\\\\\\/X45AcWlKef0uAiyOKQI8zoHYpl8fQuPt+zJ\\\\\\\/YIqHtePyvCmT+D3aA5gfLtw0NITK6fEsSsBSzD4RuQEik6tto21pSigl8DnOxrepP08iPWwP2gxKrLWChSZcRoVmT\\\\\\\/tWm5M3PzabBnyKpzS0S55WKiM47hkmT+bfassNnQ5PqbJj6zZVgFanVcA1AP6kmQunSVDW2Q\\\\\\\/kIn2kv0bg3Pr1ouQCOja6Jq6heilFSvT3UeoSFE60emlX\\\\\\\/eCRV7yTE4DlukLrCJbLZpcHtJiitvg\\\\\\\/7lw6K\\\\\\\/yxehF8lkg9ku5nUwNCnunzHNxVZItDa2Cm3mvoNutgGrwfMy+gwNUd4eXhMU6nVBGbu35N03TFHCmj1VaP74rQzzi\\\\\\\/PayaEC5oZA2VIE+CEOJ9PZjiuWLpaPBXqfe6GAW2bpz6pKU9+6LYXec9IDpuEAPOYwUpc5o4QyQyit68YXUZdFZWSvE4Vm+gssSwie8c8DdGQVOl3yN6l\\\\\\\/w3lm4GmdJYcKKvRUkU1IVB3WrBqLRzb7IYGxQxiNLwo5UoLlSB2EVoOEZ17aCGZsPYKp7FCBJmB7k+Kja7bfJbibL\\\\\\\/NYcGFLlZoU6P3yijw0ph55M\\\\\\\/5pqg1dyrHCBdVkypqt6SpTQFJtiHNSKQdaMqKcVcVRUBnKvUYMBQs8n3pP61WjIqBw0bE0TX1Nt8ivLWMVk5XbGKRqnv4Tez2J2X5y6K6GX6wZ6LslWfVWIQp8IOFCXBUZZM9Ik4LUEbI31\\\\\\\/7N+bvvnq64M=\\\u0022,\\\u0022iv\\\u0022:\\\u0022EL56ordhZltAfiql3eZzWg==\\\u0022,\\\u0022salt\\\u0022:\\\u0022f9d8845712f72c9fee5db7d9ff111eab\\\u0022}\u0022,\u0022hmac\u0022:\u0022133f6c10bf50b2139d47013694fd35f927164fcd933613ce8b2949f2a66ce10b\u0022}"},"urlIsAjaxTrusted":{"\/content\/10.1101\/2020.09.06.284794v2.full":true},"ws_fl":{"width":100,"height":21},"ws_gpo":{"size":"","annotation":"","lang":"","callback":"","width":300},"color":{"logo":"https:\/\/www.biorxiv.org\/sites\/default\/files\/bioRxiv_article.jpg"},"highwire_list_expand":{"is_collapsed":"1"},"highwireResponsive":{"enquire_enabled":1,"breakpoints_configured":1,"breakpoints":{"zero":"all and (min-width: 0px)","xsmall":"all and (min-width: 380px)","narrow":"all and (min-width: 768px) and (min-device-width: 768px), (max-device-width: 800px) and (min-width: 768px) and (orientation:landscape)","normal":"all and (min-width: 980px) and (min-device-width: 980px), all and (max-device-width: 1024px) and (min-width: 1024px) and (orientation:landscape)","wide":"all and (min-width: 1220px)"}},"eu_cookie_compliance":{"popup_enabled":1,"popup_agreed_enabled":0,"popup_hide_agreed":0,"popup_clicking_confirmation":1,"popup_scrolling_confirmation":false,"popup_html_info":"\u003Cdiv\u003E\n  \u003Cdiv class =\u0022popup-content info\u0022\u003E\n    \u003Cdiv id=\u0022popup-text\u0022\u003E\n      \u003Cp\u003EWe use cookies on this site to enhance your user experience. By clicking any link on this page you are giving your consent for us to set cookies.\u003C\/p\u003E\n    \u003C\/div\u003E\n    \u003Cdiv id=\u0022popup-buttons\u0022\u003E\n      \u003Cbutton type=\u0022button\u0022 class=\u0022agree-button eu-cookie-compliance-default-button\u0022\u003EContinue\u003C\/button\u003E\n                    \u003Cbutton type=\u0022button\u0022 class=\u0022find-more-button eu-cookie-compliance-more-button\u0022\u003EFind out more\u003C\/button\u003E\n          \u003C\/div\u003E\n  \u003C\/div\u003E\n\u003C\/div\u003E","use_mobile_message":false,"mobile_popup_html_info":"\u003Cdiv\u003E\n  \u003Cdiv class =\u0022popup-content info\u0022\u003E\n    \u003Cdiv id=\u0022popup-text\u0022\u003E\n          \u003C\/div\u003E\n    \u003Cdiv id=\u0022popup-buttons\u0022\u003E\n      \u003Cbutton type=\u0022button\u0022 class=\u0022agree-button eu-cookie-compliance-default-button\u0022\u003EContinue\u003C\/button\u003E\n                    \u003Cbutton type=\u0022button\u0022 class=\u0022find-more-button eu-cookie-compliance-more-button\u0022\u003EFind out more\u003C\/button\u003E\n          \u003C\/div\u003E\n  \u003C\/div\u003E\n\u003C\/div\u003E\n","mobile_breakpoint":"768","popup_html_agreed":"\u003Cdiv\u003E\n  \u003Cdiv class=\u0022popup-content agreed\u0022\u003E\n    \u003Cdiv id=\u0022popup-text\u0022\u003E\n      \u003Ch2\u003EThank you for accepting cookies\u003C\/h2\u003E\u003Cp\u003EYou can now hide this message or find out more about cookies.\u003C\/p\u003E    \u003C\/div\u003E\n    \u003Cdiv id=\u0022popup-buttons\u0022\u003E\n      \u003Cbutton type=\u0022button\u0022 class=\u0022hide-popup-button eu-cookie-compliance-hide-button\u0022\u003EHide\u003C\/button\u003E\n              \u003Cbutton type=\u0022button\u0022 class=\u0022find-more-button eu-cookie-compliance-more-button-thank-you\u0022 \u003EMore info\u003C\/button\u003E\n          \u003C\/div\u003E\n  \u003C\/div\u003E\n\u003C\/div\u003E","popup_use_bare_css":false,"popup_height":"auto","popup_width":"100%","popup_delay":1000,"popup_link":"\/help\/cookie-policy","popup_link_new_window":1,"popup_position":null,"popup_language":"en","store_consent":false,"better_support_for_screen_readers":0,"reload_page":0,"domain":"","popup_eu_only_js":0,"cookie_lifetime":"365","cookie_session":false,"disagree_do_not_show_popup":0,"method":"default","whitelisted_cookies":"","withdraw_markup":"\u003Cbutton type=\u0022button\u0022 class=\u0022eu-cookie-withdraw-tab\u0022\u003EPrivacy settings\u003C\/button\u003E\n\u003Cdiv class=\u0022eu-cookie-withdraw-banner\u0022\u003E\n  \u003Cdiv class=\u0022popup-content info\u0022\u003E\n    \u003Cdiv id=\u0022popup-text\u0022\u003E\n      \u003Cp\u003E\u0026lt;h2\u0026gt;We use cookies on this site to enhance your user experience\u0026lt;\/h2\u0026gt;\u0026lt;p\u0026gt;You have given your consent for us to set cookies.\u0026lt;\/p\u0026gt;\u003C\/p\u003E\n    \u003C\/div\u003E\n    \u003Cdiv id=\u0022popup-buttons\u0022\u003E\n      \u003Cbutton type=\u0022button\u0022 class=\u0022eu-cookie-withdraw-button\u0022\u003EWithdraw consent\u003C\/button\u003E\n    \u003C\/div\u003E\n  \u003C\/div\u003E\n\u003C\/div\u003E\n","withdraw_enabled":false},"googleanalytics":{"trackOutbound":1,"trackMailto":1,"trackDownload":1,"trackDownloadExtensions":"7z|aac|arc|arj|asf|asx|avi|bin|csv|doc(x|m)?|dot(x|m)?|exe|flv|gif|gz|gzip|hqx|jar|jpe?g|js|mp(2|3|4|e?g)|mov(ie)?|msi|msp|pdf|phps|png|ppt(x|m)?|pot(x|m)?|pps(x|m)?|ppam|sld(x|m)?|thmx|qtm?|ra(m|r)?|sea|sit|tar|tgz|torrent|txt|wav|wma|wmv|wpd|xls(x|m|b)?|xlt(x|m)|xlam|xml|z|zip","trackColorbox":1,"trackUrlFragments":1},"jnl_biorxiv_styles":{"defaultJCode":"biorxiv"},"omega":{"layouts":{"primary":"normal","order":["narrow","normal","wide"],"queries":{"narrow":"all and (min-width: 768px) and (min-device-width: 768px), (max-device-width: 800px) and (min-width: 768px) and (orientation:landscape)","normal":"all and (min-width: 980px) and (min-device-width: 980px), all and (max-device-width: 1024px) and (min-width: 1024px) and (orientation:landscape)","wide":"all and (min-width: 1220px)"}}}});
//--&gt;&lt;!]]&gt;
  </script>
 </head>
 <body class="html not-front not-logged-in page-node page-node- page-node-2459645 node-type-highwire-article context-content hw-default-jcode-biorxiv hw-article-type-article hw-article-category-new-results">
  <noscript>
   <iframe src="//www.googletagmanager.com/ns.html?id=GTM-M677548" height="0" width="0" style="display:none;visibility:hidden">
   </iframe>
  </noscript>
  <script type="text/javascript">
   (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0];var j=d.createElement(s);var dl=l!='dataLayer'?'&amp;l='+l:'';j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;j.type='text/javascript';j.async=true;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-M677548');
  </script>
  <div id="skip-link">
   <a href="#main-content" class="element-invisible element-focusable">
    Skip to main content
   </a>
  </div>
  <div class="page clearfix page-box-shadows footer-borders panels-page panels-layout-jcore_2col" id="page">
   <header id="section-header" class="section section-header">
    <div id="zone-branding" class="zone zone-branding clearfix print-display-block container-30">
     <div class="grid-15 prefix-1 region region-branding print-display-block" id="region-branding">
      <div class="region-inner region-branding-inner">
       <div class="branding-data clearfix">
        <div class="logo-img">
         <a href="/" rel="home" class="" data-icon-position="" data-hide-link-title="0">
          <img alt="bioRxiv" src="https://www.biorxiv.org/sites/default/files/bioRxiv_article.jpg"/>
         </a>
        </div>
       </div>
      </div>
     </div>
     <div class="grid-11 suffix-1 region region-branding-second print-hidden" id="region-branding-second">
      <div class="region-inner region-branding-second-inner">
       <div class="block block-system block-menu block-main-menu block-system-main-menu odd block-without-title" id="block-system-main-menu">
        <div class="block-inner clearfix">
         <div class="content clearfix">
          <nav class="menubar-nav">
           <ul class="menu" role="menu">
            <li class="first leaf" role="menuitem">
             <a href="/" title="" class="" data-icon-position="" data-hide-link-title="0">
              Home
             </a>
            </li>
            <li class="leaf" role="menuitem">
             <a href="/about-biorxiv" class="" data-icon-position="" data-hide-link-title="0">
              About
             </a>
            </li>
            <li class="leaf" role="menuitem">
             <a href="/submit-a-manuscript" class="" data-icon-position="" data-hide-link-title="0">
              Submit
             </a>
            </li>
            <li class="last leaf" role="menuitem">
             <a href="/content/alertsrss" title="" class="" data-icon-position="" data-hide-link-title="0">
              ALERTS / RSS
             </a>
            </li>
           </ul>
          </nav>
         </div>
        </div>
       </div>
       <div class="block block-panels-mini block-biorxiv-search-box block-panels-mini-biorxiv-search-box even block-without-title" id="block-panels-mini-biorxiv-search-box">
        <div class="block-inner clearfix">
         <div class="content clearfix">
          <div class="panel-display panel-1col clearfix" id="mini-panel-biorxiv_search_box">
           <div class="panel-panel panel-col">
            <div>
             <div class="panel-pane pane-highwire-seach-quicksearch">
              <div class="pane-content">
               <form class="highwire-quicksearch button-style-mini button-style-mini" action="/content/10.1101/2020.09.06.284794v2.full" method="post" id="highwire-search-quicksearch-form-0" accept-charset="UTF-8">
                <div>
                 <div class="form-item form-item-label-invisible form-type-textfield form-item-keywords">
                  <label class="element-invisible" for="search_rightsidebar_keywords_756600687">
                   Search for this keyword
                  </label>
                  <input placeholder="Search" type="text" id="search_rightsidebar_keywords_756600687" name="keywords" value="" size="60" maxlength="128" class="form-text"/>
                 </div>
                 <div class="button-wrapper button-mini">
                  <span class="icon-search">
                  </span>
                  <input data-icon-only="1" data-font-icon="icon-search" data-icon-position="after" type="submit" id="search_rightsidebar_submit_827070882" name="op" value="Search" class="form-submit"/>
                 </div>
                 <input type="hidden" name="form_build_id" value="form-wo7AbOeyot2-Q6tXXl9EIqJEqQ5a510kH8cidVCJr38"/>
                 <input type="hidden" name="form_id" value="highwire_search_quicksearch_form_0"/>
                </div>
               </form>
              </div>
             </div>
             <div class="panel-separator">
             </div>
             <div class="panel-pane pane-custom pane-2 advanced-search-link">
              <div class="pane-content">
               <a href="/search">
                Advanced Search
               </a>
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
    <div id="zone-header" class="zone zone-header clearfix container-30">
    </div>
   </header>
   <section id="section-content" class="section section-content">
    <div id="zone-content" class="zone zone-content clearfix container-30">
     <div class="grid-28 suffix-1 prefix-1 region region-content" id="region-content">
      <div class="region-inner region-content-inner">
       <a id="main-content">
       </a>
       <div class="block block-system block-main block-system-main odd block-without-title" id="block-system-main">
        <div class="block-inner clearfix">
         <div class="content clearfix">
          <div class="panel-display panels-960-layout jcore-2col-layout">
           <div class="panel-row-wrapper clearfix">
            <div class="main-content-wrapper grid-17 suffix-1 alpha">
             <div class="panel-panel panel-region-content">
              <div class="inside">
               <div class="panel-pane pane-highwire-article-citation">
                <div class="pane-content">
                 <div class="highwire-article-citation highwire-citation-type-highwire-article node2459645" data-node-nid="2459645" id="node-2459645--2739180317" data-pisa="biorxiv;2020.09.06.284794v2" data-pisa-master="biorxiv;2020.09.06.284794" data-apath="/biorxiv/early/2022/03/22/2020.09.06.284794.atom" data-hw-author-tooltip-instance="highwire_author_tooltip">
                  <div class="highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-top clearfix has-author-tooltip">
                   <span class="biorxiv-article-type">
                    New Results
                   </span>
                   <h1 class="highwire-cite-title" id="page-title">
                    Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity
                   </h1>
                   <div class="highwire-cite-authors">
                    <span class="highwire-citation-authors">
                     <span class="highwire-citation-author first hw-author-orcid-logo-wrapper" data-delta="0">
                      <a href="http://orcid.org/0000-0003-0890-6002" target="_blank" class="hw-author-orcid-logo link-icon-only link-icon">
                       <span class="hw-icon-orcid hw-icon-color-orcid">
                       </span>
                       <span class="title element-invisible">
                        View ORCID Profile
                       </span>
                      </a>
                      <span class="nlm-given-names">
                       Guy
                      </span>
                      <span class="nlm-surname">
                       Gaziv
                      </span>
                     </span>
                     ,
                     <span class="highwire-citation-author" data-delta="1">
                      <span class="nlm-given-names">
                       Roman
                      </span>
                      <span class="nlm-surname">
                       Beliy
                      </span>
                     </span>
                     ,
                     <span class="highwire-citation-author" data-delta="2">
                      <span class="nlm-given-names">
                       Niv
                      </span>
                      <span class="nlm-surname">
                       Granot
                      </span>
                     </span>
                     ,
                     <span class="highwire-citation-author" data-delta="3">
                      <span class="nlm-given-names">
                       Assaf
                      </span>
                      <span class="nlm-surname">
                       Hoogi
                      </span>
                     </span>
                     ,
                     <span class="highwire-citation-author" data-delta="4">
                      <span class="nlm-given-names">
                       Francesca
                      </span>
                      <span class="nlm-surname">
                       Strappini
                      </span>
                     </span>
                     ,
                     <span class="highwire-citation-author hw-author-orcid-logo-wrapper" data-delta="5">
                      <a href="http://orcid.org/0000-0002-7940-7473" target="_blank" class="hw-author-orcid-logo link-icon-only link-icon">
                       <span class="hw-icon-orcid hw-icon-color-orcid">
                       </span>
                       <span class="title element-invisible">
                        View ORCID Profile
                       </span>
                      </a>
                      <span class="nlm-given-names">
                       Tal
                      </span>
                      <span class="nlm-surname">
                       Golan
                      </span>
                     </span>
                     ,
                     <span class="highwire-citation-author" data-delta="6">
                      <span class="nlm-given-names">
                       Michal
                      </span>
                      <span class="nlm-surname">
                       Irani
                      </span>
                     </span>
                    </span>
                   </div>
                   <div class="highwire-cite-metadata">
                    <span class="highwire-cite-metadata-doi highwire-cite-metadata">
                     <span class="label">
                      doi:
                     </span>
                     https://doi.org/10.1101/2020.09.06.284794
                    </span>
                   </div>
                  </div>
                  <div id="hw-article-author-popups-node-2459645--2739180317" style="display: none;">
                   <div class="author-tooltip-0">
                    <div class="author-tooltip-name">
                     Guy Gaziv
                    </div>
                    <div class="author-tooltip-affiliation">
                     <span class="author-tooltip-text">
                      <div class="author-affiliation">
                       <span class="nlm-sup">
                        1
                       </span>
                       <span class="nlm-institution">
                        Dept. of Computer Science and Applied Math, Weizmann Institute of Science
                       </span>
                       , Rehovot,
                       <span class="nlm-country">
                        Israel
                       </span>
                      </div>
                     </span>
                    </div>
                    <ul class="author-tooltip-find-more">
                     <li class="author-tooltip-gs-link first">
                      <a href="/lookup/google-scholar?link_type=googlescholar&gs_type=author&author%5B0%5D=Guy%2BGaziv%2B" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on Google Scholar
                      </a>
                     </li>
                     <li class="author-tooltip-pubmed-link">
                      <a href="/lookup/external-ref?access_num=Gaziv%20G&link_type=AUTHORSEARCH" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on PubMed
                      </a>
                     </li>
                     <li class="author-site-search-link">
                      <a href="/search/author1%3AGuy%2BGaziv%2B" rel="nofollow" class="" data-icon-position="" data-hide-link-title="0">
                       Search for this author on this site
                      </a>
                     </li>
                     <li class="author-orcid-link last">
                      <a href="http://orcid.org/0000-0003-0890-6002" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       ORCID record for Guy Gaziv
                      </a>
                     </li>
                    </ul>
                   </div>
                   <div class="author-tooltip-1">
                    <div class="author-tooltip-name">
                     Roman Beliy
                    </div>
                    <div class="author-tooltip-affiliation">
                     <span class="author-tooltip-text">
                      <div class="author-affiliation">
                       <span class="nlm-sup">
                        1
                       </span>
                       <span class="nlm-institution">
                        Dept. of Computer Science and Applied Math, Weizmann Institute of Science
                       </span>
                       , Rehovot,
                       <span class="nlm-country">
                        Israel
                       </span>
                      </div>
                     </span>
                    </div>
                    <ul class="author-tooltip-find-more">
                     <li class="author-tooltip-gs-link first">
                      <a href="/lookup/google-scholar?link_type=googlescholar&gs_type=author&author%5B0%5D=Roman%2BBeliy%2B" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on Google Scholar
                      </a>
                     </li>
                     <li class="author-tooltip-pubmed-link">
                      <a href="/lookup/external-ref?access_num=Beliy%20R&link_type=AUTHORSEARCH" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on PubMed
                      </a>
                     </li>
                     <li class="author-site-search-link last">
                      <a href="/search/author1%3ARoman%2BBeliy%2B" rel="nofollow" class="" data-icon-position="" data-hide-link-title="0">
                       Search for this author on this site
                      </a>
                     </li>
                    </ul>
                   </div>
                   <div class="author-tooltip-2">
                    <div class="author-tooltip-name">
                     Niv Granot
                    </div>
                    <div class="author-tooltip-affiliation">
                     <span class="author-tooltip-text">
                      <div class="author-affiliation">
                       <span class="nlm-sup">
                        1
                       </span>
                       <span class="nlm-institution">
                        Dept. of Computer Science and Applied Math, Weizmann Institute of Science
                       </span>
                       , Rehovot,
                       <span class="nlm-country">
                        Israel
                       </span>
                      </div>
                     </span>
                    </div>
                    <ul class="author-tooltip-find-more">
                     <li class="author-tooltip-gs-link first">
                      <a href="/lookup/google-scholar?link_type=googlescholar&gs_type=author&author%5B0%5D=Niv%2BGranot%2B" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on Google Scholar
                      </a>
                     </li>
                     <li class="author-tooltip-pubmed-link">
                      <a href="/lookup/external-ref?access_num=Granot%20N&link_type=AUTHORSEARCH" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on PubMed
                      </a>
                     </li>
                     <li class="author-site-search-link last">
                      <a href="/search/author1%3ANiv%2BGranot%2B" rel="nofollow" class="" data-icon-position="" data-hide-link-title="0">
                       Search for this author on this site
                      </a>
                     </li>
                    </ul>
                   </div>
                   <div class="author-tooltip-3">
                    <div class="author-tooltip-name">
                     Assaf Hoogi
                    </div>
                    <div class="author-tooltip-affiliation">
                     <span class="author-tooltip-text">
                      <div class="author-affiliation">
                       <span class="nlm-sup">
                        1
                       </span>
                       <span class="nlm-institution">
                        Dept. of Computer Science and Applied Math, Weizmann Institute of Science
                       </span>
                       , Rehovot,
                       <span class="nlm-country">
                        Israel
                       </span>
                      </div>
                     </span>
                    </div>
                    <ul class="author-tooltip-find-more">
                     <li class="author-tooltip-gs-link first">
                      <a href="/lookup/google-scholar?link_type=googlescholar&gs_type=author&author%5B0%5D=Assaf%2BHoogi%2B" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on Google Scholar
                      </a>
                     </li>
                     <li class="author-tooltip-pubmed-link">
                      <a href="/lookup/external-ref?access_num=Hoogi%20A&link_type=AUTHORSEARCH" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on PubMed
                      </a>
                     </li>
                     <li class="author-site-search-link last">
                      <a href="/search/author1%3AAssaf%2BHoogi%2B" rel="nofollow" class="" data-icon-position="" data-hide-link-title="0">
                       Search for this author on this site
                      </a>
                     </li>
                    </ul>
                   </div>
                   <div class="author-tooltip-4">
                    <div class="author-tooltip-name">
                     Francesca Strappini
                    </div>
                    <div class="author-tooltip-affiliation">
                     <span class="author-tooltip-text">
                      <div class="author-affiliation">
                       <span class="nlm-sup">
                        2
                       </span>
                       <span class="nlm-institution">
                        Dept. of Neurobiology, Weizmann Institute of Science
                       </span>
                       , Rehovot,
                       <span class="nlm-country">
                        Israel
                       </span>
                      </div>
                     </span>
                    </div>
                    <ul class="author-tooltip-find-more">
                     <li class="author-tooltip-gs-link first">
                      <a href="/lookup/google-scholar?link_type=googlescholar&gs_type=author&author%5B0%5D=Francesca%2BStrappini%2B" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on Google Scholar
                      </a>
                     </li>
                     <li class="author-tooltip-pubmed-link">
                      <a href="/lookup/external-ref?access_num=Strappini%20F&link_type=AUTHORSEARCH" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on PubMed
                      </a>
                     </li>
                     <li class="author-site-search-link last">
                      <a href="/search/author1%3AFrancesca%2BStrappini%2B" rel="nofollow" class="" data-icon-position="" data-hide-link-title="0">
                       Search for this author on this site
                      </a>
                     </li>
                    </ul>
                   </div>
                   <div class="author-tooltip-5">
                    <div class="author-tooltip-name">
                     Tal Golan
                    </div>
                    <div class="author-tooltip-affiliation">
                     <span class="author-tooltip-text">
                      <div class="author-affiliation">
                       <span class="nlm-sup">
                        3
                       </span>
                       <span class="nlm-institution">
                        Zuckerman Institute, Columbia University
                       </span>
                       , New York, NY,
                       <span class="nlm-country">
                        USA
                       </span>
                      </div>
                     </span>
                    </div>
                    <ul class="author-tooltip-find-more">
                     <li class="author-tooltip-gs-link first">
                      <a href="/lookup/google-scholar?link_type=googlescholar&gs_type=author&author%5B0%5D=Tal%2BGolan%2B" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on Google Scholar
                      </a>
                     </li>
                     <li class="author-tooltip-pubmed-link">
                      <a href="/lookup/external-ref?access_num=Golan%20T&link_type=AUTHORSEARCH" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on PubMed
                      </a>
                     </li>
                     <li class="author-site-search-link">
                      <a href="/search/author1%3ATal%2BGolan%2B" rel="nofollow" class="" data-icon-position="" data-hide-link-title="0">
                       Search for this author on this site
                      </a>
                     </li>
                     <li class="author-orcid-link last">
                      <a href="http://orcid.org/0000-0002-7940-7473" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       ORCID record for Tal Golan
                      </a>
                     </li>
                    </ul>
                   </div>
                   <div class="author-tooltip-6">
                    <div class="author-tooltip-name">
                     Michal Irani
                    </div>
                    <div class="author-tooltip-affiliation">
                     <span class="author-tooltip-text">
                      <div class="author-affiliation">
                       <span class="nlm-sup">
                        1
                       </span>
                       <span class="nlm-institution">
                        Dept. of Computer Science and Applied Math, Weizmann Institute of Science
                       </span>
                       , Rehovot,
                       <span class="nlm-country">
                        Israel
                       </span>
                      </div>
                     </span>
                    </div>
                    <ul class="author-tooltip-find-more">
                     <li class="author-tooltip-gs-link first">
                      <a href="/lookup/google-scholar?link_type=googlescholar&gs_type=author&author%5B0%5D=Michal%2BIrani%2B" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on Google Scholar
                      </a>
                     </li>
                     <li class="author-tooltip-pubmed-link">
                      <a href="/lookup/external-ref?access_num=Irani%20M&link_type=AUTHORSEARCH" target="_blank" class="" data-icon-position="" data-hide-link-title="0">
                       Find this author on PubMed
                      </a>
                     </li>
                     <li class="author-site-search-link">
                      <a href="/search/author1%3AMichal%2BIrani%2B" rel="nofollow" class="" data-icon-position="" data-hide-link-title="0">
                       Search for this author on this site
                      </a>
                     </li>
                     <li class="author-corresp-email-link last">
                      <span>
                       For correspondence:
                       <a href="mailto:michal.irani@weizmann.ac.il" class="" data-icon-position="" data-hide-link-title="0">
                        michal.irani@weizmann.ac.il
                       </a>
                      </span>
                     </li>
                    </ul>
                   </div>
                  </div>
                 </div>
                </div>
               </div>
               <div class="panel-separator">
               </div>
               <div class="panel-pane pane-highwire-panel-tabs pane-panels-ajax-tab-tabs">
                <div class="pane-content">
                 <div class="item-list">
                  <ul class="tabs inline panels-ajax-tab">
                   <li class="first">
                    <a href="/content/10.1101/2020.09.06.284794v2" class="panels-ajax-tab-tab" data-panel-name="biorxiv_tab_art" data-target-id="highwire_article_tabs" data-entity-context="node:2459645" data-trigger="" data-url-enabled="1">
                     Abstract
                    </a>
                    <a href="/panels_ajax_tab/biorxiv_tab_art/node:2459645/1" rel="nofollow" style="display:none" class="js-crawler-link">
                    </a>
                   </li>
                   <li>
                    <a href="/content/10.1101/2020.09.06.284794v2.full-text" class="panels-ajax-tab-tab" data-panel-name="article_tab_full_text" data-target-id="highwire_article_tabs" data-entity-context="node:2459645" data-trigger="full-text" data-url-enabled="1">
                     Full Text
                    </a>
                    <a href="/panels_ajax_tab/article_tab_full_text/node:2459645/1" rel="nofollow" style="display:none" class="js-crawler-link">
                    </a>
                   </li>
                   <li>
                    <a href="/content/10.1101/2020.09.06.284794v2.article-info" class="panels-ajax-tab-tab" data-panel-name="biorxiv_tab_info" data-target-id="highwire_article_tabs" data-entity-context="node:2459645" data-trigger="article-info" data-url-enabled="1">
                     Info/History
                    </a>
                    <a href="/panels_ajax_tab/biorxiv_tab_info/node:2459645/1" rel="nofollow" style="display:none" class="js-crawler-link">
                    </a>
                   </li>
                   <li>
                    <a href="/content/10.1101/2020.09.06.284794v2.article-metrics" class="panels-ajax-tab-tab" data-panel-name="article_tab_metrics" data-target-id="highwire_article_tabs" data-entity-context="node:2459645" data-trigger="article-metrics" data-url-enabled="1">
                     Metrics
                    </a>
                    <a href="/panels_ajax_tab/article_tab_metrics/node:2459645/1" rel="nofollow" style="display:none" class="js-crawler-link">
                    </a>
                   </li>
                   <li>
                    <a href="/content/10.1101/2020.09.06.284794v2.supplementary-material" class="panels-ajax-tab-tab" data-panel-name="biorxiv_tab_data" data-target-id="highwire_article_tabs" data-entity-context="node:2459645" data-trigger="supplementary-material" data-url-enabled="1">
                     Supplementary material
                    </a>
                    <a href="/panels_ajax_tab/biorxiv_tab_data/node:2459645/1" rel="nofollow" style="display:none" class="js-crawler-link">
                    </a>
                   </li>
                   <li>
                    <a href="/content/10.1101/2020.09.06.284794v2.external-links" class="panels-ajax-tab-tab" data-panel-name="article_tab_data_code" data-target-id="highwire_article_tabs" data-entity-context="node:2459645" data-trigger="external-links" data-url-enabled="1">
                     Data/Code
                    </a>
                    <a href="/panels_ajax_tab/article_tab_data_code/node:2459645/1" rel="nofollow" style="display:none" class="js-crawler-link">
                    </a>
                   </li>
                   <li class="last">
                    <a href="/content/10.1101/2020.09.06.284794v2.full.pdf+html" class="panels-ajax-tab-tab" data-panel-name="biorxiv_tab_pdf" data-target-id="highwire_article_tabs" data-entity-context="node:2459645" data-trigger="full.pdf+html" data-url-enabled="1">
                     <i class="icon-file-alt">
                     </i>
                     Preview PDF
                    </a>
                    <a href="/panels_ajax_tab/biorxiv_tab_pdf/node:2459645/1" rel="nofollow" style="display:none" class="js-crawler-link">
                    </a>
                   </li>
                  </ul>
                 </div>
                </div>
               </div>
               <div class="panel-separator">
               </div>
               <div class="panel-pane pane-highwire-panel-tabs-container">
                <div class="pane-content">
                 <div data-panels-ajax-tab-preloaded="article_tab_full_text" id="panels-ajax-tab-container-highwire_article_tabs" class="panels-ajax-tab-container">
                  <div class="panels-ajax-tab-loading" style="display:none">
                   <img class="loading" src="https://www.biorxiv.org/sites/all/modules/contrib/panels_ajax_tab/images/loading.gif" alt="Loading" title="Loading"/>
                  </div>
                  <div class="panels-ajax-tab-wrap-article_tab_full_text">
                   <div class="panel-display panel-1col clearfix">
                    <div class="panel-panel panel-col">
                     <div>
                      <div class="panel-pane pane-highwire-markup">
                       <div class="pane-content">
                        <div class="highwire-markup">
                         <div xmlns="http://www.w3.org/1999/xhtml" data-highwire-cite-ref-tooltip-instance="highwire_reflinks_tooltip" class="content-block-markup" xmlns:xhtml="http://www.w3.org/1999/xhtml">
                          <div class="article fulltext-view">
                           <span class="highwire-journal-article-marker-start">
                           </span>
                           <div class="section abstract" id="abstract-1">
                            <h2 class="">
                             Abstract
                            </h2>
                            <p id="p-3">
                             Reconstructing natural images and decoding their semantic category from fMRI brain recordings is challenging. Acquiring sufficient pairs of images and their corresponding fMRI responses, which span the huge space of natural images, is prohibitive. We present a novel self-supervised approach that goes well beyond the scarce paired data, for achieving both: (i) state-of-the art fMRI-to-image reconstruction, and (ii) first-ever large-scale semantic classification from fMRI responses. By imposing cycle consistency between a pair of deep neural networks (from image-to-fMRI & from fMRI-to-image), we train our image reconstruction network on a large number of “unpaired” natural images (images without fMRI recordings) from many novel semantic categories. This enables to adapt our reconstruction network to a very rich semantic coverage without requiring any explicit semantic supervision. Specifically, we find that combining our self-supervised training with high-level perceptual losses, gives rise to new reconstruction & classification capabilities. In particular, this perceptual training enables to classify well fMRIs of never-before-seen semantic classes, without requiring any class labels during training. This gives rise to: (i) Unprecedented image-reconstruction from fMRI of never-before-seen images (evaluated by image metrics and human testing), and (ii) Large-scale semantic classification of categories that were never-before-seen during network training. Such large-scale (1000-way) semantic classification from fMRI recordings has never been demonstrated before. Finally, we provide evidence for the biological consistency of our learned model.
                            </p>
                           </div>
                           <div class="section" id="sec-1">
                            <h2 class="">
                             1 Introduction
                            </h2>
                            <p id="p-8">
                             Natural images span a vastly rich visual and semantic space that humans are experts at processing and recognizing. An intriguing inverse problem is to decode images seen by a person and their semantic categories directly from brain activity (
                             <a id="xref-fig-1-1" class="xref-fig" href="#F1">
                              Fig 1a
                             </a>
                             ) [
                             <a id="xref-ref-1-1" class="xref-bibr" href="#ref-1">
                              1
                             </a>
                             ,
                             <a id="xref-ref-2-1" class="xref-bibr" href="#ref-2">
                              2
                             </a>
                             ]. Modeling the stimulus as a function of cortical activation (decoding) complements the conventional neuroscientific paradigm of modeling cortical activation as a function of the stimulus (encoding) [
                             <a id="xref-ref-1-2" class="xref-bibr" href="#ref-1">
                              1
                             </a>
                             ]. It provides us with an additional benchmark of our understanding of the cortical visual code and is thus a cornerstone in cognitive neuroscience [
                             <a id="xref-ref-3-1" class="xref-bibr" href="#ref-3">
                              3
                             </a>
                             –
                             <a id="xref-ref-10-1" class="xref-bibr" href="#ref-10">
                              10
                             </a>
                             ]. Specifically, reconstruction and classification of seen images provide a window into top-down perceptual processes whose probing was previously limited to subjective report. These include visual recall [
                             <a id="xref-ref-11-1" class="xref-bibr" href="#ref-11">
                              11
                             </a>
                             ], visual imagery in wakefulness [
                             <a id="xref-ref-11-2" class="xref-bibr" href="#ref-11">
                              11
                             </a>
                             –
                             <a id="xref-ref-15-1" class="xref-bibr" href="#ref-15">
                              15
                             </a>
                             ] and sleep [
                             <a id="xref-ref-16-1" class="xref-bibr" href="#ref-16">
                              16
                             </a>
                             ,
                             <a id="xref-ref-17-1" class="xref-bibr" href="#ref-17">
                              17
                             </a>
                             ]. Last, advancements in fMRI-based decoding may be useful for other neuroimaging modalities that provide opportunities for effective brain-machine interfaces [
                             <a id="xref-ref-18-1" class="xref-bibr" href="#ref-18">
                              18
                             </a>
                             –
                             <a id="xref-ref-20-1" class="xref-bibr" href="#ref-20">
                              20
                             </a>
                             ] or for diagnosing disorders of consciousness [
                             <a id="xref-ref-21-1" class="xref-bibr" href="#ref-21">
                              21
                             </a>
                             ,
                             <a id="xref-ref-22-1" class="xref-bibr" href="#ref-22">
                              22
                             </a>
                             ].
                            </p>
                            <div id="F1" class="fig pos-float type-fig odd">
                             <div class="highwire-figure">
                              <div class="fig-inline-img-wrapper">
                               <div class="fig-inline-img">
                                <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F1.large.jpg?width=800&height=600&carousel=1" title="Our self-supervised approach. (a) The task: reconstructing images and classifying their semantic category from evoked brain activity, recorded via fMRI. (b,c) Supervised training for decoding (b) and encoding (c) using limited training pairs. This gives rise to poor generalization. (d) Illustration of our added self-supervision, which enables training on “unpaired images” (any natural image with no fMRI recording). This self-supervision allows adapting the decoder to the statistics of natural images and many rich semantic classes." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-1196661220" data-figure-caption='<div class="highwire-markup"><div xmlns="http://www.w3.org/1999/xhtml"><span class="caption-title">Our self-supervised approach.</span> <strong>(a)</strong> The task: reconstructing images and classifying their semantic category from evoked brain activity, recorded via fMRI. <strong>(b,c)</strong> Supervised training for decoding (<strong>b</strong>) and encoding (<strong>c</strong>) using limited training pairs. This gives rise to poor generalization. <strong>(d)</strong> Illustration of our added self-supervision, which enables training on “unpaired images” (any natural image with no fMRI recording). This self-supervision allows adapting the decoder to the statistics of natural images and many rich semantic classes.</div></div>' data-icon-position="" data-hide-link-title="0">
                                 <span class="hw-responsive-img">
                                  <img class="highwire-fragment fragment-image lazyload" alt="Figure 1." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F1.medium.gif" width="440" height="307"/>
                                  <noscript>
                                   <img class="highwire-fragment fragment-image" alt="Figure 1." src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F1.medium.gif" width="440" height="307"/>
                                  </noscript>
                                 </span>
                                </a>
                               </div>
                              </div>
                              <ul class="highwire-figure-links inline">
                               <li class="download-fig first">
                                <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F1.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Figure 1." data-icon-position="" data-hide-link-title="0">
                                 Download figure
                                </a>
                               </li>
                               <li class="new-tab last">
                                <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F1.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">
                                 Open in new tab
                                </a>
                               </li>
                              </ul>
                             </div>
                             <div class="fig-caption" xmlns:xhtml="http://www.w3.org/1999/xhtml">
                              <span class="fig-label">
                               Figure 1.
                              </span>
                              <span class="caption-title">
                               Our self-supervised approach.
                              </span>
                              <p id="p-9" class="first-child">
                               <strong>
                                (a)
                               </strong>
                               The task: reconstructing images and classifying their semantic category from evoked brain activity, recorded via fMRI.
                               <strong>
                                (b,c)
                               </strong>
                               Supervised training for decoding (
                               <strong>
                                b
                               </strong>
                               ) and encoding (
                               <strong>
                                c
                               </strong>
                               ) using limited training pairs. This gives rise to poor generalization.
                               <strong>
                                (d)
                               </strong>
                               Illustration of our added self-supervision, which enables training on “unpaired images” (any natural image with no fMRI recording). This self-supervision allows adapting the decoder to the statistics of natural images and many rich semantic classes.
                              </p>
                              <div class="sb-div caption-clear">
                              </div>
                             </div>
                            </div>
                            <p id="p-10">
                             In the image reconstruction task, one attempts to decode natural images which were observed by a human subject from the induced brain activity captured by functional magnetic resonance imaging (fMRI). To learn the mapping between fMRI and image representation, typical fMRI datasets provide many pairs of images and their corresponding fMRI responses, referenced to as “paired” data. The goal is to learn an fMRI-to-image decoder which generalizes well to reconstructing images from novel “test-fMRI”, fMRI response induced by novel images from totally different semantic categories than those in the training data (referred to as “test-images”). Moreover, a complementary challenge to reconstructing the underlying image is also to decode its semantic category. However, the shortage of “paired” training data limits the generalization power of today’s fMRI decoders. The number of obtainable image-fMRI pairs is bounded by the limited time a human can spend in an MRI scanner. This also results in a limited number of semantic categories associated with fMRI data. Accordingly, most datasets provide only up to a few thousands of (Image, fMRI) pairs, which are usually obtained from a small number of image categories. Such limited data cannot span the huge space of natural images and their semantic categories, nor the space of their fMRI recordings. Moreover, the poor spatio-temporal resolution of fMRI signals, as well as their low Signal-to-Noise Ratio (SNR), reduce the reliability of the already scarce paired training data [
                             <a id="xref-ref-23-1" class="xref-bibr" href="#ref-23">
                              23
                             </a>
                             –
                             <a id="xref-ref-25-1" class="xref-bibr" href="#ref-25">
                              25
                             </a>
                             ].
                            </p>
                            <p id="p-11">
                             Reconstructing natural images from fMRI was approached by a number of methods, which can broadly be classified into three families: (i) Linear regression between fMRI data and handcrafted image-features (e.g., Gabor wavelets) [
                             <a id="xref-ref-26-1" class="xref-bibr" href="#ref-26">
                              26
                             </a>
                             –
                             <a id="xref-ref-28-1" class="xref-bibr" href="#ref-28">
                              28
                             </a>
                             ], (ii) Linear regression between fMRI data and deep (CNN-based) image-features (e.g., using pretrained AlexNet) [
                             <a id="xref-ref-12-1" class="xref-bibr" href="#ref-12">
                              12
                             </a>
                             ,
                             <a id="xref-ref-29-1" class="xref-bibr" href="#ref-29">
                              29
                             </a>
                             –
                             <a id="xref-ref-31-1" class="xref-bibr" href="#ref-31">
                              31
                             </a>
                             ], or latent spaces of pretrained generative models [
                             <a id="xref-ref-32-1" class="xref-bibr" href="#ref-32">
                              32
                             </a>
                             –
                             <a id="xref-ref-35-1" class="xref-bibr" href="#ref-35">
                              35
                             </a>
                             ], and (iii) End-to-end Deep Learning [
                             <a id="xref-ref-13-1" class="xref-bibr" href="#ref-13">
                              13
                             </a>
                             ,
                             <a id="xref-ref-36-1" class="xref-bibr" href="#ref-36">
                              36
                             </a>
                             –
                             <a id="xref-ref-38-1" class="xref-bibr" href="#ref-38">
                              38
                             </a>
                             ]. To our best knowledge, methods [
                             <a id="xref-ref-12-2" class="xref-bibr" href="#ref-12">
                              12
                             </a>
                             ] and [
                             <a id="xref-ref-36-2" class="xref-bibr" href="#ref-36">
                              36
                             </a>
                             ] are the current state-of-the-art in this field. All these methods inherently rely on the available “paired” data to train their decoder (pairs of images and their corresponding fMRI responses). When trained on limited data, purely supervised models are prone to overfitting, which leads to poor generalization to new test-data (fMRI response evoked by new images). To overcome this problem, recent methods [
                             <a id="xref-ref-12-3" class="xref-bibr" href="#ref-12">
                              12
                             </a>
                             ,
                             <a id="xref-ref-32-2" class="xref-bibr" href="#ref-32">
                              32
                             </a>
                             –
                             <a id="xref-ref-34-1" class="xref-bibr" href="#ref-34">
                              34
                             </a>
                             ,
                             <a id="xref-ref-36-3" class="xref-bibr" href="#ref-36">
                              36
                             </a>
                             –
                             <a id="xref-ref-38-2" class="xref-bibr" href="#ref-38">
                              38
                             </a>
                             ] introduced natural-image priors in the form of Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). These methods enabled leap advancement in reconstruction quality from fMRI, and tend to produce natural-looking images. Nevertheless, despite their pleasant natural appearance, their reconstructed images are often unfaithful to the actual images underlying the test-fMRI (see
                             <a id="xref-fig-8-1" class="xref-fig" href="#F8">
                              Fig 8a
                             </a>
                             ).
                            </p>
                            <p id="p-12">
                             Prior work on semantic classification of fMRI recordings induced by natural-images, can be characterized as two families: (i) Classifying new images from previously seen categories, and (ii)	Classifying new images from novel never-before-seen categories. In the first, the categories to-be-decoded (of the test data) are represented in the training data [
                             <a id="xref-ref-6-1" class="xref-bibr" href="#ref-6">
                              6
                             </a>
                             ,
                             <a id="xref-ref-39-1" class="xref-bibr" href="#ref-39">
                              39
                             </a>
                             –
                             <a id="xref-ref-42-1" class="xref-bibr" href="#ref-42">
                              42
                             </a>
                             ]. This widely-explored family is limited to decode only the few and typically coarse classes (e.g., faces, scenes, inanimate objects, and birds), which are represented in the limited “paired” data used for decoder training. The second family, which was introduced in [
                             <a id="xref-ref-43-1" class="xref-bibr" href="#ref-43">
                              43
                             </a>
                             ], addresses the much more challenging case, where the test-categories are novel, namely, not directly represented in the training data. Under this setting, decoding novel, diverse, and fine-grained semantic categories (e.g., ImageNet [
                             <a id="xref-ref-44-1" class="xref-bibr" href="#ref-44">
                              44
                             </a>
                             ]) remains a difficult task because of the narrow semantic coverage spanned by the limited paired training data.
                            </p>
                            <p id="p-13">
                             To cope with this data limitation, recent approaches [
                             <a id="xref-ref-29-2" class="xref-bibr" href="#ref-29">
                              29
                             </a>
                             ,
                             <a id="xref-ref-39-2" class="xref-bibr" href="#ref-39">
                              39
                             </a>
                             ,
                             <a id="xref-ref-41-1" class="xref-bibr" href="#ref-41">
                              41
                             </a>
                             –
                             <a id="xref-ref-43-2" class="xref-bibr" href="#ref-43">
                              43
                             </a>
                             ] harnessed a pretrained and semantically separable embedding. In this approach, voxel responses are linearly mapped to higher-level feature representation of an image classification network. Once mapped, categorization is achieved by either (i) forward-propagating the decoded representation to the classification layer [
                             <a id="xref-ref-29-3" class="xref-bibr" href="#ref-29">
                              29
                             </a>
                             ], or by (ii) nearest-neighbor classification against a gallery of category representatives, which are the mean feature representations of many natural images from that category [
                             <a id="xref-ref-43-3" class="xref-bibr" href="#ref-43">
                              43
                             </a>
                             ]. While these methods benefited from the wide-coverage of their semantic representation (which stems from pretrained image features independently of the fMRI data), their decoding method remained
                             <strong>
                              supervised
                             </strong>
                             in essence. This is because their training of the mapping from fMRI-to-feature representation relies solely on the limited “paired” training data. Consequently, they are prone to poor generalization and are still limited by the poor category coverage of the “paired” training data.
                            </p>
                            <p id="p-14">
                             We present a new approach to overcome the limitations mentioned above and inherent lack of training data, simultaneously for both tasks – image reconstruction as well as for large-scale semantic classification. We achieve this by introducing self-supervised training on
                             <span class="underline">
                              unpaired images
                             </span>
                             (images without fMRI recordings). Our approach is illustrated in
                             <a id="xref-fig-1-2" class="xref-fig" href="#F1">
                              Fig 1
                             </a>
                             . We train two types of deep neural networks: an Encoder E, to map natural images to their corresponding fMRI response, and a Decoder D, to map fMRI recordings to their corresponding images. Composing those two networks, E-D, yields a combined network whose input and output are the same image (
                             <a id="xref-fig-1-3" class="xref-fig" href="#F1">
                              Fig 1d
                             </a>
                             ).
                             <strong>
                              This allows for unsupervised training on
                              <span class="underline">
                               unpaired images
                              </span>
                             </strong>
                             (i.e., images without fMRI recordings, e.g., 50,000 natural images from 1000 semantic categories in our experiments). Such self-supervision adapts the Decoder to the statistics of novel natural images and their novel categories (including categories not represented in the “paired” training data or even in the “unpaired” natural images). Importantly, we combine our self-supervised approach with semantics learning
                             <strong>
                              without requiring any explicit class labels in the training
                             </strong>
                             . This is achieved via: (i) Using perceptual similarity reconstruction criteria. These criteria require for the reconstructed image to be similar to the original image not only at a pixel level, but also that the two images be
                             <strong>
                              perceptually similar
                             </strong>
                             , in their higher-level deep “semantic” representations. (ii) Designing an Encoder architecture that benefits from a higher-level “semantic” representation of a backbone network that was trained for image classification.
                             <a id="xref-fig-2-1" class="xref-fig" href="#F2">
                              Fig 2
                             </a>
                             exemplifies the power of adding unsupervised training on unpaired images together with the perceptual criteria. Furthermore, beyond the dramatic improvement in the image reconstruction task, we show that our combined self-supervised perceptual approach gives rise also to new capabilities in semantic category decoding (against 1000 classes), despite the scarce fMRI-based training data.
                            </p>
                            <div id="F2" class="fig pos-float type-fig odd">
                             <div class="highwire-figure">
                              <div class="fig-inline-img-wrapper">
                               <div class="fig-inline-img">
                                <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F2.large.jpg?width=800&height=600&carousel=1" title="Adding unsupervised training on “unpaired images” together with perceptual criteria improves reconstruction. (Left to Right): • The images presented to the human subjects. • Reconstruction using the training pairs only (Fig 1b). • Reconstruction when adding self-supervised training on unpaired natural images (Fig 1d), and also adding high-level perceptual criteria to the decoder and other important improvements. • Our preliminary results [45] without using the perceptual criteria and other important improvements presented here. Example results are shown for two fMRI datasets: ‘fMRI on ImageNet’ [43] and ‘vim-1’ [26]." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-1196661220" data-figure-caption='<div class="highwire-markup"><span xmlns="http://www.w3.org/1999/xhtml" class="caption-title">Adding unsupervised training on “unpaired images” together with perceptual criteria improves reconstruction.</span> (Left to Right): • The images presented to the human subjects. • Reconstruction using the training pairs only (Fig 1b). • Reconstruction when adding self-supervised training on unpaired natural images (Fig 1d), and also adding high-level perceptual criteria to the decoder and other important improvements. • Our preliminary results [45] without using the perceptual criteria and other important improvements presented here. Example results are shown for two fMRI datasets: ‘fMRI on ImageNet’ [43] and ‘vim-1’ [26].</div>' data-icon-position="" data-hide-link-title="0">
                                 <span class="hw-responsive-img">
                                  <img class="highwire-fragment fragment-image lazyload" alt="Figure 2." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F2.medium.gif" width="440" height="344"/>
                                  <noscript>
                                   <img class="highwire-fragment fragment-image" alt="Figure 2." src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F2.medium.gif" width="440" height="344"/>
                                  </noscript>
                                 </span>
                                </a>
                               </div>
                              </div>
                              <ul class="highwire-figure-links inline">
                               <li class="download-fig first">
                                <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F2.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Figure 2." data-icon-position="" data-hide-link-title="0">
                                 Download figure
                                </a>
                               </li>
                               <li class="new-tab last">
                                <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F2.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">
                                 Open in new tab
                                </a>
                               </li>
                              </ul>
                             </div>
                             <div class="fig-caption">
                              <span class="fig-label">
                               Figure 2.
                              </span>
                              <span class="caption-title">
                               Adding unsupervised training on “unpaired images” together with perceptual criteria improves reconstruction.
                              </span>
                              <p id="p-15" class="first-child">
                               (Left to Right): • The images presented to the human subjects. • Reconstruction using the training pairs only (
                               <a id="xref-fig-1-4" class="xref-fig" href="#F1">
                                Fig 1b
                               </a>
                               ). • Reconstruction when adding self-supervised training on unpaired natural images (
                               <a id="xref-fig-1-5" class="xref-fig" href="#F1">
                                Fig 1d
                               </a>
                               ), and also adding high-level perceptual criteria to the decoder and other important improvements. • Our preliminary results [
                               <a id="xref-ref-45-1" class="xref-bibr" href="#ref-45">
                                45
                               </a>
                               ] without using the perceptual criteria and other important improvements presented here. Example results are shown for two fMRI datasets: ‘fMRI on ImageNet’ [
                               <a id="xref-ref-43-4" class="xref-bibr" href="#ref-43">
                                43
                               </a>
                               ] and ‘vim-1’ [
                               <a id="xref-ref-26-2" class="xref-bibr" href="#ref-26">
                                26
                               </a>
                               ].
                              </p>
                              <div class="sb-div caption-clear">
                              </div>
                             </div>
                            </div>
                            <p id="p-16">
                             Our self-supervision leverages the cycle-consistency nature of our two networks when combined. Notably, our self-supervised training on unpaired natural images differs from an auto-encoder: When training under this configuration the Encoder weights are kept fixed, thus constraining the input images to go through the true fMRI space. Furthermore, unsupervised training on unpaired natural images was also recently proposed in [
                             <a id="xref-ref-36-4" class="xref-bibr" href="#ref-36">
                              36
                             </a>
                             ], where they used these images to produce additional surrogate fMRI-data to train their model. However, this was never addressed in the context of semantic decoding from fMRI data. Moreover, their image reconstruction criteria were limited to pixel level alone and did not include any type of perceptual or visual-features based reconstruction criteria. To the best of our knowledge, we are the first to present classification of semantic categories that are never-before-seen during training
                             <span class="underline">
                              at a large-scale
                             </span>
                             (1000-way – i.e., detecting the correct class out of more than 1000 rich classes).
                            </p>
                            <p id="p-17">
                             A preliminary version of our self-supervised approach and partial results (in the context of image reconstruction only) were previously presented in a conference proceeding [
                             <a id="xref-ref-45-2" class="xref-bibr" href="#ref-45">
                              45
                             </a>
                             ]. However, here we present our complete and advanced reconstruction algorithm that has undergone major extensions, enabling a leap improvement in the image reconstruction quality over our previous method [
                             <a id="xref-ref-45-3" class="xref-bibr" href="#ref-45">
                              45
                             </a>
                             ] (as demonstrated in
                             <a id="xref-fig-2-2" class="xref-fig" href="#F2">
                              Fig 2
                             </a>
                             ). Moreover, our new extended framework now enables also impressive semantic classification capabilities from fMRI data. As an aid for readers familiar with our previous report [
                             <a id="xref-ref-45-4" class="xref-bibr" href="#ref-45">
                              45
                             </a>
                             ], we list the four major extensions introduced in the present paper: (i) We introduced two significant improvements to our algorithm: Adding high-level perceptual criteria [
                             <a id="xref-ref-46-1" class="xref-bibr" href="#ref-46">
                              46
                             </a>
                             ] on the reconstructed images (in contrast with optimizing Mean-Square-Error loss, and on low-level features alone), and increasing the expressiveness of the Encoder architecture to include higher-level “semantic” representations by using multiple levels of the pretrained VGG network (in contrast with a single readout layer before). Our present algorithm provides state-of-the-art reconstructions compared to leading existing methods to-date, as evident through image-metric-based comparisons as well as extensive human behavioral evaluations. (ii) We extended the self-supervised approach to allow also for
                             <span class="underline">
                              semantic classification
                             </span>
                             of reconstructed images. This classification relies on and demonstrates the fidelity of the reconstructions. (iii) We analyze the contributions of different visual cortex areas to the resulting image reconstructions, and (iv) We evaluate the biological consistency of the learned models.
                            </p>
                            <p id="p-18">
                             Importantly, adding the perceptual similarity was powerful enough to shadow and marginalize the previously dominant effect of training on unpaired fMRI data in [
                             <a id="xref-ref-45-5" class="xref-bibr" href="#ref-45">
                              45
                             </a>
                             ]. Accordingly, training on unpaired fMRI is omitted in the present framework (despite being a legitimate feature enabled by our self-supervised approach).
                            </p>
                            <p>
                             Our contributions are therefore several-fold:
                            </p>
                            <ul class="list-unord" id="list-1">
                             <li id="list-item-1">
                              <p id="p-20">
                               A self-supervised approach for simultaneous image reconstruction and semantic category decoding, which can handle the inherent lack of image-fMRI training data.
                              </p>
                             </li>
                             <li id="list-item-2">
                              <p id="p-21">
                               Unprecedented state-of-the-art image-reconstruction quality from fMRI of never-before-seen images (from never-before-seen semantic categories).
                              </p>
                             </li>
                             <li id="list-item-3">
                              <p id="p-22">
                               Large-scale semantic classification (1000+ rich classes) of never-before-seen semantic categories. To the best of our knowledge, such large-scale semantic classification capabilities from fMRI data has never been demonstrated before.
                              </p>
                             </li>
                             <li id="list-item-4">
                              <p id="p-23">
                               We provide analyses showing predominance of early visual areas in image reconstruction quality, and demonstrate biologically plausible receptive field formation in our learned models (although such constrains were not imposed in the training, but rather automatically learned by our system).
                              </p>
                             </li>
                            </ul>
                           </div>
                           <div class="section" id="sec-2">
                            <h2 class="">
                             2 Overview of the Approach
                            </h2>
                            <p id="p-24">
                             This section provides a high-level overview of our approach and its key components. The technical details and the experimental datasets are described in Methods.
                            </p>
                            <div id="sec-3" class="subsection">
                             <h3>
                              2.1 Self-supervised image reconstruction from brain activity
                             </h3>
                             <p id="p-25">
                              The essence of our approach is to enrich the scarce paired image-fMRI training data with easily accessible natural images for which there are no fMRI recordings. This type of training is enabled by imposing cycle-consistency on the “unpaired images”, using two networks, which learn two inverse mappings: from images to fMRI (Encoding), and vice versa – from fMRI to images (Decoding).
                             </p>
                             <p id="p-26">
                              Our training consists of Encoder training followed by Decoder training, which we define in the two phases illustrated in
                              <a id="xref-fig-3-1" class="xref-fig" href="#F3">
                               Fig 3
                              </a>
                              . In the first phase, we apply supervised training of the Encoder E alone. We train it to predict the fMRI responses of input images using the image-fMRI training pairs (
                              <a id="xref-fig-3-2" class="xref-fig" href="#F3">
                               Fig 3a
                              </a>
                              ). In the second phase, we use the pretrained Encoder (from the first phase) and train the Decoder D, keeping the weights of E fixed (
                              <a id="xref-fig-3-3" class="xref-fig" href="#F3">
                               Fig 3b
                              </a>
                              ). D is trained using both the paired and the unpaired data, simultaneously. Here, each training batch consists of two types of training data: (i) image-fMRI pairs from the training set (
                              <a id="xref-fig-1-6" class="xref-fig" href="#F1">
                               Fig 1b
                              </a>
                              ), and (ii) unpaired natural images (with no fMRI,
                              <a id="xref-fig-1-7" class="xref-fig" href="#F1">
                               Fig 1d
                              </a>
                              ).
                             </p>
                             <div id="F3" class="fig pos-float type-fig odd">
                              <div class="highwire-figure">
                               <div class="fig-inline-img-wrapper">
                                <div class="fig-inline-img">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F3.large.jpg?width=800&height=600&carousel=1" title="Training phases. (a) The first training phase: Supervised training of the Encoder with {Image, fMRI} pairs. (b) Second phase: Training the Decoder with two types of data simultaneously: {fMRI, Image} pairs (supervised examples), and unpaired natural images (self-supervision). The pretrained Encoder from the first training phase is kept fixed in the second phase." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-1196661220" data-figure-caption='<div class="highwire-markup"><div xmlns="http://www.w3.org/1999/xhtml"><span class="caption-title">Training phases.</span> <strong>(a)</strong> The first training phase: Supervised training of the Encoder with {Image, fMRI} pairs. <strong>(b)</strong> Second phase: Training the Decoder with two types of data simultaneously: {fMRI, Image} pairs (supervised examples), and unpaired natural images (self-supervision). The pretrained Encoder from the first training phase is kept fixed in the second phase.</div></div>' data-icon-position="" data-hide-link-title="0">
                                  <span class="hw-responsive-img">
                                   <img class="highwire-fragment fragment-image lazyload" alt="Figure 3." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F3.medium.gif" width="440" height="254"/>
                                   <noscript>
                                    <img class="highwire-fragment fragment-image" alt="Figure 3." src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F3.medium.gif" width="440" height="254"/>
                                   </noscript>
                                  </span>
                                 </a>
                                </div>
                               </div>
                               <ul class="highwire-figure-links inline">
                                <li class="download-fig first">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F3.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Figure 3." data-icon-position="" data-hide-link-title="0">
                                  Download figure
                                 </a>
                                </li>
                                <li class="new-tab last">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F3.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">
                                  Open in new tab
                                 </a>
                                </li>
                               </ul>
                              </div>
                              <div class="fig-caption">
                               <span class="fig-label">
                                Figure 3.
                               </span>
                               <span class="caption-title">
                                Training phases.
                               </span>
                               <p id="p-27" class="first-child">
                                <strong>
                                 (a)
                                </strong>
                                The first training phase: Supervised training of the Encoder with {Image, fMRI} pairs.
                                <strong>
                                 (b)
                                </strong>
                                Second phase: Training the Decoder with two types of data simultaneously: {fMRI, Image} pairs (supervised examples), and unpaired natural images (self-supervision). The pretrained Encoder from the first training phase is kept fixed in the second phase.
                               </p>
                               <div class="sb-div caption-clear">
                               </div>
                              </div>
                             </div>
                             <p id="p-28">
                              Training on unpaired natural images (without fMRI) allows to augment the training with data from a much richer semantic space than the one spanned by the paired training data alone. Specifically, we draw the unpaired images from a large external database of 49K images from 980 ImageNet (“ILSVRC”) classes, which are mutually exclusive not only to the test-images contained in the fMRI (paired) dataset, but also to their underlying test classes (test categories). In principle, for optimal networks E and D, the combined E-D network should yield an output image which is identical to its input image. This should hold for any natural image (regardless if an fMRI was ever recorded for it). Importantly, our image reconstruction losses (in both ℒ
                              <sub>
                               ED
                              </sub>
                              and ℒ
                              <sub>
                               D
                              </sub>
                              ,
                              <a id="xref-fig-3-4" class="xref-fig" href="#F3">
                               Fig 3
                              </a>
                              ) require for the reconstructed image to be similar to the original image not only at a pixel level. We further require these two images to be
                              <strong>
                               perceptually similar
                              </strong>
                              , at a semantic level.
                             </p>
                             <p id="p-29">
                              Perceptual Similarity is a metric that highly correlates with human image-similarity perception, and involves a broad range of visual feature representation levels [
                              <a id="xref-ref-46-2" class="xref-bibr" href="#ref-46">
                               46
                              </a>
                              ]. Our Perceptual Similarity loss is illustrated in
                              <a id="xref-fig-4-1" class="xref-fig" href="#F4">
                               Fig 4
                              </a>
                              . We apply a pre-trained VGG classification network (a network which was trained for the task of object recognition from images [
                              <a id="xref-ref-47-1" class="xref-bibr" href="#ref-47">
                               47
                              </a>
                              ]), on the reconstructed and the original images. We then impose similarity between their corresponding deep-image-features, extracted from multiple deep layers of VGG. Using this metric as our reconstruction criterion enables to learn low-level to high-level “semantic” information from the broad semantic space, which is spanned by the external database of ‘unpaired’ images and their classes; It, thus, encourages the Decoder to output images that are not only accurate at low-level visual features, but are also semantically meaningful. Importantly, the
                              <span class="underline">
                               class labels
                              </span>
                              of the unpaired images (or the paired images)
                              <span class="underline">
                               are never used in our training process, hence may be unknown
                              </span>
                              . Adding the perceptual loss in combination with training on additional unpaired images gives rise to a leap improvement in the fMRI-to-image reconstruction quality compared to any previous method (including our previous method [
                              <a id="xref-ref-45-6" class="xref-bibr" href="#ref-45">
                               45
                              </a>
                              ] – see
                              <a id="xref-fig-2-3" class="xref-fig" href="#F2">
                               Fig 2
                              </a>
                              , as well as compared with other methods – see
                              <a id="xref-fig-8-2" class="xref-fig" href="#F8">
                               Fig 8
                              </a>
                              ). It provides a dramatic improvement in detail level and perceptual interpretability of the reconstructed images. Our new approach further enables large-scale semantic classification of fMRI into rich novel semantic categories.
                             </p>
                             <div id="F4" class="fig pos-float type-fig odd">
                              <div class="highwire-figure">
                               <div class="fig-inline-img-wrapper">
                                <div class="fig-inline-img">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F4.large.jpg?width=800&height=600&carousel=1" title="Adding high-level perceptual criteria improves reconstruction accuracy and enables large-scale semantic classification. (a) Imposing Perceptual Similarity on the reconstructed image at the Decoder’s output, as applied when training on unpaired natural images (without fMRI and without any class labels) from many novel semantic classes. This adapts the Decoder to a significantly broader semantic space despite not having any explicit semantic supervision. (b) To classify a reconstructed image to its novel semantic class we extract Deep Features using a pretrained classification network, and follow a nearest-neighbor class-centroid approach against a large-scale gallery of 1000+ ImageNet classes. (c) We define class representatives as the mean-embedding of many same-class images [43]." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-1196661220" data-figure-caption='<div class="highwire-markup"><div xmlns="http://www.w3.org/1999/xhtml"><span class="caption-title">Adding high-level perceptual criteria improves reconstruction accuracy and enables large-scale semantic classification.</span> <strong>(a)</strong> Imposing Perceptual Similarity on the reconstructed image at the Decoder’s output, as applied when training on unpaired natural images (without fMRI and without any class labels) from many novel semantic classes. This adapts the Decoder to a significantly broader semantic space despite not having any explicit semantic supervision. <strong>(b)</strong> To classify a reconstructed image to its novel semantic class we extract Deep Features using a pretrained classification network, and follow a nearest-neighbor class-centroid approach against a large-scale gallery of 1000+ ImageNet classes. <strong>(c)</strong> We define class representatives as the mean-embedding of many same-class images [43].</div></div>' data-icon-position="" data-hide-link-title="0">
                                  <span class="hw-responsive-img">
                                   <img class="highwire-fragment fragment-image lazyload" alt="Figure 4." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F4.medium.gif" width="440" height="379"/>
                                   <noscript>
                                    <img class="highwire-fragment fragment-image" alt="Figure 4." src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F4.medium.gif" width="440" height="379"/>
                                   </noscript>
                                  </span>
                                 </a>
                                </div>
                               </div>
                               <ul class="highwire-figure-links inline">
                                <li class="download-fig first">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F4.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Figure 4." data-icon-position="" data-hide-link-title="0">
                                  Download figure
                                 </a>
                                </li>
                                <li class="new-tab last">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F4.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">
                                  Open in new tab
                                 </a>
                                </li>
                               </ul>
                              </div>
                              <div class="fig-caption">
                               <span class="fig-label">
                                Figure 4.
                               </span>
                               <span class="caption-title">
                                Adding high-level perceptual criteria improves reconstruction accuracy and enables large-scale semantic classification.
                               </span>
                               <p id="p-30" class="first-child">
                                <strong>
                                 (a)
                                </strong>
                                Imposing Perceptual Similarity on the reconstructed image at the Decoder’s output, as applied when training on unpaired natural images (
                                <span class="underline">
                                 without
                                </span>
                                fMRI and
                                <span class="underline">
                                 without
                                </span>
                                any class labels) from many novel semantic classes. This adapts the Decoder to a significantly broader semantic space despite not having any explicit semantic supervision.
                                <strong>
                                 (b)
                                </strong>
                                To classify a reconstructed image to its novel semantic class we extract Deep Features using a pretrained classification network, and follow a nearest-neighbor class-centroid approach against a large-scale gallery of 1000+ ImageNet classes.
                                <strong>
                                 (c)
                                </strong>
                                We define class representatives as the mean-embedding of many same-class images [
                                <a id="xref-ref-43-5" class="xref-bibr" href="#ref-43">
                                 43
                                </a>
                                ].
                               </p>
                               <div class="sb-div caption-clear">
                               </div>
                              </div>
                             </div>
                            </div>
                            <div id="sec-4" class="subsection">
                             <h3>
                              2.2 Self-supervised image classification to semantic categories
                             </h3>
                             <p id="p-31">
                              Our new self-supervised perceptual approach extends well beyond the task of image reconstruction. It further allows for large-scale semantic classification of fMRI data. We present classification of fMRI data against a gallery of more than 1000 rich image classes, in a challenging 1000-way classification task (see
                              <a id="xref-fig-5-1" class="xref-fig" href="#F5">
                               Fig 5
                              </a>
                              ).
                              <strong>
                               Such large-scale semantic classification of fMRI data (to 1000-way, with promising results) has never been demonstrated before.
                              </strong>
                             </p>
                             <div id="F5" class="fig pos-float type-fig odd">
                              <div class="highwire-figure">
                               <div class="fig-inline-img-wrapper">
                                <div class="fig-inline-img">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F5.large.jpg?width=800&height=600&carousel=1" title="Self-supervision allows classification to rich and novel semantic categories. (a) Visual classification results showing the Top-5 predictions out of 1030 classes for reconstructed test-image. We show examples where the ground-truth class (marked in red) is ranked among the Top-5 (correct classification) or excluded from it (incorrect classification). For visualization purposes only, each class is represented by the nearest-neighbor image from the 100 randomly sampled images of the particular class. Note that ”incorrect” predicted classes are often reasonable (e.g., ”Leopard” wrongly predicted as ”Lion”; ”Duck” wrongly predicted as ”Ostrich”). (b) Top-1 Classification accuracy in an n-way classification task. Adding unsupervised training on unpaired data (Fig 1d,e) dramatically outperforms the baseline of the supervised approach (Fig 1b). (c) Ablation study of the Classification accuracy as a function of the Perceptual Similarity criterion for decoder training: Applying “partial” perceptual similarity using only the outputs of the first VGG16 block (low ”semantic” layers), and up to all its 5 blocks (high ”semantic” layers). Applying full Perceptual Similarity on higher-level VGG features substantially improves classification performance. Panels a,b show results for subject three. 95% Confidence Intervals shown on charts." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-1196661220" data-figure-caption='<div class="highwire-markup"><div xmlns="http://www.w3.org/1999/xhtml"><span class="caption-title">Self-supervision allows classification to rich and novel semantic categories.</span> <strong>(a)</strong> Visual classification results showing the Top-5 predictions out of 1030 classes for reconstructed test-image. We show examples where the ground-truth class (marked in red) is ranked among the Top-5 <strong>(correct classification)</strong> or excluded from it <strong>(incorrect classification)</strong>. For visualization purposes only, each class is represented by the nearest-neighbor image from the 100 randomly sampled images of the particular class. Note that ”incorrect” predicted classes are often reasonable (e.g., ”Leopard” wrongly predicted as ”Lion”; ”Duck” wrongly predicted as ”Ostrich”). <strong>(b)</strong> Top-1 Classification accuracy in an n-way classification task. Adding unsupervised training on unpaired data (Fig 1d,e) dramatically outperforms the baseline of the supervised approach (Fig 1b). <strong>(c)</strong> Ablation study of the Classification accuracy as a function of the Perceptual Similarity criterion for decoder training: Applying “partial” perceptual similarity using only the outputs of the first VGG16 block (low ”semantic” layers), and up to all its 5 blocks (high ”semantic” layers). Applying full Perceptual Similarity on higher-level VGG features substantially improves classification performance. Panels <strong>a,b</strong> show results for subject three. 95% Confidence Intervals shown on charts.</div></div>' data-icon-position="" data-hide-link-title="0">
                                  <span class="hw-responsive-img">
                                   <img class="highwire-fragment fragment-image lazyload" alt="Figure 5." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F5.medium.gif" width="359" height="440"/>
                                   <noscript>
                                    <img class="highwire-fragment fragment-image" alt="Figure 5." src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F5.medium.gif" width="359" height="440"/>
                                   </noscript>
                                  </span>
                                 </a>
                                </div>
                               </div>
                               <ul class="highwire-figure-links inline">
                                <li class="download-fig first">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F5.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Figure 5." data-icon-position="" data-hide-link-title="0">
                                  Download figure
                                 </a>
                                </li>
                                <li class="new-tab last">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F5.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">
                                  Open in new tab
                                 </a>
                                </li>
                               </ul>
                              </div>
                              <div class="fig-caption">
                               <span class="fig-label">
                                Figure 5.
                               </span>
                               <span class="caption-title">
                                Self-supervision allows classification to rich and
                                <span class="underline">
                                 novel
                                </span>
                                semantic categories.
                               </span>
                               <p id="p-32" class="first-child">
                                <strong>
                                 (a)
                                </strong>
                                Visual classification results showing the Top-5 predictions out of 1030 classes for reconstructed test-image. We show examples where the ground-truth class (marked in red) is ranked among the Top-5
                                <strong>
                                 (correct classification)
                                </strong>
                                or excluded from it
                                <strong>
                                 (incorrect classification)
                                </strong>
                                . For visualization purposes only, each class is represented by the nearest-neighbor image from the 100 randomly sampled images of the particular class. Note that ”incorrect” predicted classes are often reasonable (e.g., ”Leopard” wrongly predicted as ”Lion”; ”Duck” wrongly predicted as ”Ostrich”).
                                <strong>
                                 (b)
                                </strong>
                                Top-1 Classification accuracy in an n-way classification task. Adding unsupervised training on unpaired data (
                                <a id="xref-fig-1-8" class="xref-fig" href="#F1">
                                 Fig 1d,e
                                </a>
                                ) dramatically outperforms the baseline of the supervised approach (
                                <a id="xref-fig-1-9" class="xref-fig" href="#F1">
                                 Fig 1b
                                </a>
                                ).
                                <strong>
                                 (c)
                                </strong>
                                Ablation study of the Classification accuracy as a function of the Perceptual Similarity criterion for decoder training: Applying “partial” perceptual similarity using only the outputs of the first VGG16 block (low ”semantic” layers), and up to all its 5 blocks (high ”semantic” layers). Applying full Perceptual Similarity on higher-level VGG features substantially improves classification performance. Panels
                                <strong>
                                 a,b
                                </strong>
                                show results for subject three. 95% Confidence Intervals shown on charts.
                               </p>
                               <div class="sb-div caption-clear">
                               </div>
                              </div>
                             </div>
                             <p id="p-33">
                              Our classification approach is based on our self-supervised perceptual reconstruction method described above. We use our perceptually trained Decoder to reconstruct the test-images from their test-fMRI. We then classify the reconstructed images against 1000+ rich ImageNet semantic classes.
                              <a id="xref-fig-4-2" class="xref-fig" href="#F4">
                               Fig 4b,c
                              </a>
                              shows our classification approach. To classify a reconstructed image to its novel semantic class, we match a “Deep-Feature signature” extracted from the reconstructed image, against “class-representative Deep-Feature signatures” (one per class), in a gallery of 1000+ semantic categories, which also include the 50 novel test classes. More specifically: (i) We extract Deep Features from the reconstructed image at an intermediate level of a pretrained classification network (
                              <a id="xref-fig-4-3" class="xref-fig" href="#F4">
                               Fig 4b
                              </a>
                              ), (ii) Following [
                              <a id="xref-ref-43-6" class="xref-bibr" href="#ref-43">
                               43
                              </a>
                              ], for each class in the gallery, we compute a single “class representative” using 100 randomly sampled images from that class. The class representative is defined as the average Deep Features (centroid) of those 100 randomly sampled images from that class (see
                              <a id="xref-fig-4-4" class="xref-fig" href="#F4">
                               Fig 4c
                              </a>
                              ). (iii) We compute the correlation between the Deep Features extracted from the reconstructed image and each of the 1000+ class representatives. These yield 1000+ “semantic similarity” scores (
                              <a id="xref-fig-4-5" class="xref-fig" href="#F4">
                               Fig 4b
                              </a>
                              ). Ranking the gallery classes according to these similarity scores (for each reconstructed image) provides the basis for semantic classification at any desired ‘Top-X’ accuracy level (
                              <a id="xref-fig-5-2" class="xref-fig" href="#F5">
                               Fig 5
                              </a>
                              ). Specifically, the classification is marked ‘correct’ when the ground truth category obtains the best similarity score among all 1000+ classes (‘Top-1’), or when it is among the top 5 most similar classes (‘Top-5’). The location of the ground-truth class within the sorted list of 1000+ classes further provides a ”rank score” for evaluating our classification accuracy (see
                              <a id="xref-table-wrap-2-1" class="xref-table" href="#T2">
                               Table 2
                              </a>
                              ).
                             </p>
                             <div id="T1" class="table pos-float">
                              <div class="table-inline table-callout-links">
                               <div class="callout">
                                <span>
                                 View this table:
                                </span>
                                <ul class="callout-links">
                                 <li class="view-inline first">
                                  <a href="##" class="table-expand-inline" data-table-url="/highwire/markup/2462466/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&table-expand-inline=1" data-icon-position="" data-hide-link-title="0">
                                   View inline
                                  </a>
                                 </li>
                                 <li class="view-popup">
                                  <a href="/highwire/markup/2462466/expansion?width=1000&height=500&iframe=true&postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed" class="colorbox colorbox-load table-expand-popup" rel="gallery-fragment-tables" data-icon-position="" data-hide-link-title="0">
                                   View popup
                                  </a>
                                 </li>
                                 <li class="download-ppt last">
                                  <a href="/highwire/powerpoint/2462466" class="highwire-figure-link highwire-figure-link-ppt" data-icon-position="" data-hide-link-title="0">
                                   Download powerpoint
                                  </a>
                                 </li>
                                </ul>
                               </div>
                              </div>
                              <div class="table-caption">
                               <span class="table-label">
                                Table 1.
                               </span>
                               <span class="caption-title">
                                Summary of fMRI datasets used in analyses.
                               </span>
                               <p id="p-34" class="first-child">
                                Repeat count refers to the number of fMRI recordings per presented stimulus. Voxel count refers to approximated number of voxels used in analysis.
                               </p>
                               <div class="sb-div caption-clear">
                               </div>
                              </div>
                             </div>
                             <div id="T2" class="table pos-float">
                              <div class="table-inline table-callout-links">
                               <div class="callout">
                                <span>
                                 View this table:
                                </span>
                                <ul class="callout-links">
                                 <li class="view-inline first">
                                  <a href="##" class="table-expand-inline" data-table-url="/highwire/markup/2462473/expansion?postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed&table-expand-inline=1" data-icon-position="" data-hide-link-title="0">
                                   View inline
                                  </a>
                                 </li>
                                 <li class="view-popup">
                                  <a href="/highwire/markup/2462473/expansion?width=1000&height=500&iframe=true&postprocessors=highwire_tables%2Chighwire_reclass%2Chighwire_figures%2Chighwire_math%2Chighwire_inline_linked_media%2Chighwire_embed" class="colorbox colorbox-load table-expand-popup" rel="gallery-fragment-tables" data-icon-position="" data-hide-link-title="0">
                                   View popup
                                  </a>
                                 </li>
                                 <li class="download-ppt last">
                                  <a href="/highwire/powerpoint/2462473" class="highwire-figure-link highwire-figure-link-ppt" data-icon-position="" data-hide-link-title="0">
                                   Download powerpoint
                                  </a>
                                 </li>
                                </ul>
                               </div>
                              </div>
                              <div class="table-caption">
                               <span class="table-label">
                                Table 2.
                               </span>
                               <span class="caption-title">
                                Self-supervision allows classification to rich and novel semantic categories.
                               </span>
                               <p id="p-35" class="first-child">
                                Median rank of the ground truth class among 1030 class representatives (Lower is better). Significant differences between the two methods are marked with asterisks (Mann-Whitney test, N = 50, p < .05). Adding self-supervision leads to significant improvement in classification rank for the four (out of five) subjects with the highest fMRI median noise-ceiling. SD and SE are standard deviation and error, respectively.
                               </p>
                               <div class="sb-div caption-clear">
                               </div>
                              </div>
                             </div>
                             <p id="p-36">
                              This classification procedure greatly benefits from our self-supervised perceptual approach, which enables to train on additional unpaired images from arbitrarily many novel semantic categories (
                              <a id="xref-fig-1-10" class="xref-fig" href="#F1">
                               Fig 1b
                              </a>
                              ). This allows to adapt the Decoder to a much richer (practically unlimited) semantic coverage in a
                              <strong>
                               completely category-free way
                              </strong>
                              , namely without any explicit semantic supervision during training. The key component which induces semantically meaningful decoding is the Perceptual Similarity metric, which involves
                              <span class="underline">
                               higher-level
                              </span>
                              ”semantic” criteria. This type of
                              <span class="underline">
                               non-specific
                              </span>
                              semantic supervision enables our method to generalize well to new never-before-seen semantic classes – classes which are neither contained in the paired training data, nor in the unpaired external images used at train time.
                             </p>
                            </div>
                           </div>
                           <div class="section" id="sec-5">
                            <h2 class="">
                             3 Methods
                            </h2>
                            <div id="sec-6" class="subsection">
                             <h3>
                              3.1 Self-supervised Encoder/Decoder training
                             </h3>
                             <p id="p-37">
                              The Encoder (E) is trained first on predicting fMRI responses from input images. Once trained, the encoder is fixed, and the Decoder (D) is trained on reconstructing images from fMRI responses. The motivation for training the Encoder and Decoder in separate phases (with a fixed Encoder during Decoder training) is to ensure that the Encoder’s output does not diverge from predicting the fMRI responses by the unsupervised training objectives 1d. Additionally, we start by supervised training of the Encoder in order to allow it to converge at the first phase, and then serve as strong guidance for the more severely ill-posed decoding task [
                              <a id="xref-ref-9-1" class="xref-bibr" href="#ref-9">
                               9
                              </a>
                              ], which is the focus of the next phase. We next describe each phase in more detail.
                             </p>
                             <div id="sec-7" class="subsection">
                              <h4>
                               3.1.1 Encoder supervised training (Phase I)
                              </h4>
                              <p id="p-38">
                               The supervised training of the Encoder is illustrated in
                               <a id="xref-fig-3-5" class="xref-fig" href="#F3">
                                Fig 3a
                               </a>
                               . Let
                               <span class="inline-formula" id="inline-formula-1">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-1.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-1.gif"/>
                                 </noscript>
                                </span>
                               </span>
                               denote the encoded fMRI response from image, s, by Encoder E. We define an fMRI loss by a convex combination of mean square error and cosine proximity with respect to the ground truth fMRI, r. The
                               <strong>
                                fMRI loss
                               </strong>
                               is defined as:
                               <span class="disp-formula" id="disp-formula-1">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-8.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-8.gif"/>
                                 </noscript>
                                </span>
                               </span>
                               where α is a hyperparameter set empirically (α = 0.9). We use this loss for training the Encoder E.
                              </p>
                              <p id="p-39">
                               Notably, in the considered fMRI datasets, the subjects who participated in the experiments were instructed to fixate at the center of the images. Nevertheless, eye movements were not recorded during the scans, thus the fixation performance is not known. To account for the center-fixation uncertainty, we introduced small random shifts (+/− a few pixels) of the input images during Encoder training [
                               <a id="xref-ref-13-2" class="xref-bibr" href="#ref-13">
                                13
                               </a>
                               ]. This resulted in a substantial improvement in the Encoder performance and subsequently in the image reconstruction quality. Upon completion of Encoder training, we transition to training the Decoder together with the fixed Encoder.
                              </p>
                             </div>
                             <div id="sec-8" class="subsection">
                              <h4>
                               3.1.2 Decoder training (Phase II)
                              </h4>
                              <p id="p-40">
                               The training loss of our Decoder consists of two main losses illustrated in
                               <a id="xref-fig-3-6" class="xref-fig" href="#F3">
                                Fig 3b
                               </a>
                               :
                               <span class="disp-formula" id="disp-formula-2">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-9.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-9.gif"/>
                                 </noscript>
                                </span>
                               </span>
                               ℒ
                               <sup>
                                D
                               </sup>
                               is a supervised loss on training pairs of image-fMRI. ℒ
                               <sup>
                                ED
                               </sup>
                               (Encoder-Decoder) is an unsupervised loss on unpaired images (without corresponding fMRI responses). Both components of the loss are normalized to have the same order of magnitude (all in the range [0, 1], with equal weights), to guarantee that the total loss is not dominated by any individual component. We found our reconstruction results to be relatively insensitive to the exact balancing between the two-loss components. We next detail each component of the loss.
                              </p>
                              <p id="p-41">
                               ℒ
                               <sup>
                                D
                               </sup>
                               <strong>
                                : Decoder Supervised Training
                               </strong>
                               is illustrated in
                               <a id="xref-fig-1-11" class="xref-fig" href="#F1">
                                Fig 1b
                               </a>
                               . Given {fMRI, Image} training pairs {(r, s)}, the supervised loss ℒ
                               <sup>
                                D
                               </sup>
                               is imposed on the decoded image,
                               <span class="inline-formula" id="inline-formula-2">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-2.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-2.gif"/>
                                 </noscript>
                                </span>
                               </span>
                               , and is defined via the image reconstruction objective, ℒ
                               <sub>
                                s
                               </sub>
                               , as
                               <span class="disp-formula" id="disp-formula-3">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-10.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-10.gif"/>
                                 </noscript>
                                </span>
                               </span>
                              </p>
                              <p id="p-42">
                               ℒ
                               <sub>
                                s
                               </sub>
                               consists of losses on image RGB values, ℒ
                               <sub>
                                RGB
                               </sub>
                               , as well as losses on Deep Image Features extracted from the image using a pretrained VGG16 network [
                               <a id="xref-ref-47-2" class="xref-bibr" href="#ref-47">
                                47
                               </a>
                               ] (a deep network that was trained for the task of object recognition from images). Using features of pretrained deep neural network classifiers was recently introduced to the task of reconstructing images from fMRI responses in [
                               <a id="xref-ref-12-4" class="xref-bibr" href="#ref-12">
                                12
                               </a>
                               ,
                               <a id="xref-ref-13-3" class="xref-bibr" href="#ref-13">
                                13
                               </a>
                               ], although our implementation is somewhat different (for a comparison of the approaches, see the Discussion section). We denote the deep features extracted from an image, s, by φ (s), on which we apply a
                               <strong>
                                Perceptual Similarity
                               </strong>
                               criterion, ℒ
                               <sub>
                                perceptual
                               </sub>
                               . This last component gave a significant performance leap. Unlike our preliminary work [
                               <a id="xref-ref-45-7" class="xref-bibr" href="#ref-45">
                                45
                               </a>
                               ], where we imposed only a Mean-Square-Error loss on the low level features alone (hence failed to capture or exploit any ”semantic” appearance or interpretation), here we impose Perceptual similarity [
                               <a id="xref-ref-46-3" class="xref-bibr" href="#ref-46">
                                46
                               </a>
                               ] using the outputs of all the five feature-extractor blocks of VGG (from low to high VGG layers, i.e., lower-to-higher “semantic” levels), denoted as
                               <span class="inline-formula" id="inline-formula-3">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-3.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-3.gif"/>
                                 </noscript>
                                </span>
                               </span>
                               for a particular block b. This metric is implemented by cosine similarity between channel-normalized ground-truth and predicted features at each block output. The complete criterion is then a sum of the block-wise contributions. The Image loss for a reconstructed image
                               <span class="inline-formula" id="inline-formula-4">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-4.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-4.gif"/>
                                 </noscript>
                                </span>
                               </span>
                               thus reads:
                               <span class="disp-formula" id="disp-formula-4">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-11.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-11.gif"/>
                                 </noscript>
                                </span>
                               </span>
                               where:
                               <span class="disp-formula" id="disp-formula-5">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-12.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-12.gif"/>
                                 </noscript>
                                </span>
                               </span>
                              </p>
                              <p id="p-43">
                               The last term,
                               <span class="inline-formula" id="inline-formula-5">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-5.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-5.gif"/>
                                 </noscript>
                                </span>
                               </span>
                               , corresponds to total variation (TV) regularization of the reconstructed (decoded) image,
                               <span class="inline-formula" id="inline-formula-6">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-6.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/inline-graphic-6.gif"/>
                                 </noscript>
                                </span>
                               </span>
                               . The Image loss in
                               <a id="xref-disp-formula-4-1" class="xref-disp-formula" href="#disp-formula-4">
                                Eq. 3
                               </a>
                               is also used for the self-supervised Encoder-Decoder training on
                               <span class="underline">
                                unpaired images
                               </span>
                               (images without fMRI), as explained next.
                              </p>
                              <p id="p-44">
                               ℒ
                               <sup>
                                ED
                               </sup>
                               <strong>
                                : Encoder-Decoder training on unpaired Natural Images
                               </strong>
                               is illustrated in
                               <a id="xref-fig-1-12" class="xref-fig" href="#F1">
                                Fig 1d
                               </a>
                               . This objective enables to train on any desired unpaired image (images for which no fMRI was ever recorded), well beyond the 1200 or 1750 images included in the fMRI dataset (‘fMRI on ImageNet’ or ‘vim-1’ respectively). In particular, we used ~50K additional natural images from ImageNet’s 1000-class data [
                               <a id="xref-ref-44-2" class="xref-bibr" href="#ref-44">
                                44
                               </a>
                               ] (excluding the test classes). This allows adaptation to the statistics of many more novel semantic categories, thus learning the common higher-level feature representation of various novel classes. To train on images without corresponding fMRI responses, we map images to themselves through our Encoder-Decoder transformation,
                               <span class="disp-formula" id="disp-formula-6">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-13.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-13.gif"/>
                                 </noscript>
                                </span>
                               </span>
                              </p>
                              <p id="p-45">
                               The unsupervised component ℒ
                               <sup>
                                ED
                               </sup>
                               of the loss in
                               <a id="xref-disp-formula-2-1" class="xref-disp-formula" href="#disp-formula-2">
                                Eq. 2
                               </a>
                               on unpaired images, s, reads:
                               <span class="disp-formula" id="disp-formula-7">
                                <span class="highwire-responsive-lazyload">
                                 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class="highwire-embed lazyload" alt="Embedded Image" data-src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-14.gif"/>
                                 <noscript>
                                  <img class="highwire-embed" alt="Embedded Image" src="https://www.biorxiv.org/sites/default/files/highwire/biorxiv/early/2022/03/22/2020.09.06.284794/embed/graphic-14.gif"/>
                                 </noscript>
                                </span>
                               </span>
                               where ℒ
                               <sub>
                                s
                               </sub>
                               is the Image loss defined in
                               <a id="xref-disp-formula-4-2" class="xref-disp-formula" href="#disp-formula-4">
                                Eq 3
                               </a>
                               . In other words, ℒ
                               <sup>
                                ED
                               </sup>
                               imposes
                               <span class="underline">
                                cycle-consistency
                               </span>
                               on any natural image, but at a
                               <span class="underline">
                                perceptual level
                               </span>
                               (not only at the pixel level). Note that this high-level perceptual consistency does
                               <span class="underline">
                                not
                               </span>
                               require using any class labels in the training.
                              </p>
                             </div>
                            </div>
                            <div id="sec-9" class="subsection">
                             <h3>
                              3.2 Deep Architecture
                             </h3>
                             <p id="p-46">
                              We focused on 112×112 RGB or grayscale image reconstruction (depending on the dataset), although our method works well also on other resolutions.
                             </p>
                             <p id="p-47">
                              <strong>
                               Architectures
                              </strong>
                              of the Encoder and the Decoder are illustrated in
                              <a id="xref-fig-6-1" class="xref-fig" href="#F6">
                               Fig 6
                              </a>
                              . The Encoder comprises four parallel branches of representation, built on top of features extracted from blocks 1-4 of VGG19. This enables to benefit from the hierarchy of “semantic” levels of the pretrained VGG network. The outputs of the four resulting branches (with their various resolutions) are then fed into branch-specific learned convolutional modules, which are designed to reduce the representation’s dimensions to more compact representations of 28×28×32 or 14×14×32 (Height×Width×ConvolutionChannels). These modules consist of batch normalization, 3×3 convolution with 32 channels, ReLU, ×2 subsampling, and batch normalization. The first branch is preceded by an additional ×2 maximum pooling while the fourth branch is not subsampled. Inspired by the feature-weighted receptive field [
                              <a id="xref-ref-48-1" class="xref-bibr" href="#ref-48">
                               48
                              </a>
                              ] for complex feature spaces, we designed a locally-connected layer which acts on the spatial and channel dimensions separately. This induced separability enables a dramatic decrease in the number of parameters required to regress the voxel activations. In this spatial-feature locally-connected layer, for each spatial coordinate we stack along the channel dimension the values of the immediate 9 neighboring coordinates. This results in tensors of shapes 26×26×288 for branches 1-3 and 12×12×288 for branch 4 (after eliminating the boundaries), and effectively constrains spatial association among adjacent pixels within 3×3 patches. Next, the tensors are multiplied by voxel-specific spatial maps, which learn the voxels’ receptive fields. This outputs space-reduced tensors of shape 288×N
                              <sub>
                               v
                              </sub>
                              , where N
                              <sub>
                               v
                              </sub>
                              is the number of voxels. To encourage the smoothness of the spatial maps, we penalize for the within-map total variation. The next layer is a cross-channel locally-connected layer, which weighs the contribution of each feature/channel per voxel and outputs a vector of size N
                              <sub>
                               v
                              </sub>
                              . This vector is produced by every branch separately. Finally, the outputs of the 4 branches are concatenated along a new dimension and followed by a locally-connected layer (4×N
                              <sub>
                               v
                              </sub>
                              parameters) that weighs the contribution from each branch and output the final fMRI activations. We initialized all weights using Glorot normal initializer, except for the last layer, which was 1-initialized (and restricted to remain non-negative).
                             </p>
                             <div id="F6" class="fig pos-float type-fig odd">
                              <div class="highwire-figure">
                               <div class="fig-inline-img-wrapper">
                                <div class="fig-inline-img">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F6.large.jpg?width=800&height=600&carousel=1" title="Encoder & Decoder Architectures. BN, GN, US, and ReLU stand for batch normalization, group normalization, up-sampling, and rectified linear unit, respectively. We designed a custom space-feature locally-connected layer (see text)." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-1196661220" data-figure-caption='<div class="highwire-markup"><span xmlns="http://www.w3.org/1999/xhtml" class="caption-title">Encoder & Decoder Architectures.</span> BN, GN, US, and ReLU stand for batch normalization, group normalization, up-sampling, and rectified linear unit, respectively. We designed a custom space-feature locally-connected layer (see text).</div>' data-icon-position="" data-hide-link-title="0">
                                  <span class="hw-responsive-img">
                                   <img class="highwire-fragment fragment-image lazyload" alt="Figure 6." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F6.medium.gif" width="440" height="312"/>
                                   <noscript>
                                    <img class="highwire-fragment fragment-image" alt="Figure 6." src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F6.medium.gif" width="440" height="312"/>
                                   </noscript>
                                  </span>
                                 </a>
                                </div>
                               </div>
                               <ul class="highwire-figure-links inline">
                                <li class="download-fig first">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F6.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Figure 6." data-icon-position="" data-hide-link-title="0">
                                  Download figure
                                 </a>
                                </li>
                                <li class="new-tab last">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F6.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">
                                  Open in new tab
                                 </a>
                                </li>
                               </ul>
                              </div>
                              <div class="fig-caption">
                               <span class="fig-label">
                                Figure 6.
                               </span>
                               <span class="caption-title">
                                Encoder & Decoder Architectures.
                               </span>
                               <p id="p-48" class="first-child">
                                BN, GN, US, and ReLU stand for batch normalization, group normalization, up-sampling, and rectified linear unit, respectively. We designed a custom space-feature locally-connected layer (see text).
                               </p>
                               <div class="sb-div caption-clear">
                               </div>
                              </div>
                             </div>
                             <p id="p-49">
                              The Decoder architecture uses a locally-connected layer to transform and reshape the input vector-form fMRI input into 64 feature maps with spatial resolution 14×14. This representation is then followed by three blocks, and each consists of: (i) ×2 up-sampling, (ii) 5×5 convolution with unity stride, 64 channels, and ReLU activation, and (iii) group normalization (16 groups). To yield the output image we finally performed an additional convolution, similar to the preceding ones, but with three channels to represent colors, and a sigmoid activation to keep the output values in the 0-1 range. We used Glorot-normal [
                              <a id="xref-ref-49-1" class="xref-bibr" href="#ref-49">
                               49
                              </a>
                              ] to initialize the weights.
                             </p>
                            </div>
                            <div id="sec-10" class="subsection">
                             <h3>
                              3.3 Experimental datasets
                             </h3>
                             <p id="p-50">
                              We tested our self-supervised approach on two publicly available (and very different) benchmark fMRI datasets that we summarize in
                              <a id="xref-table-wrap-1-1" class="xref-table" href="#T1">
                               Table 1
                              </a>
                              . These datasets provide elicited fMRI recordings of human subjects paired with their corresponding underlying natural images. In both datasets, subjects were instructed to fixate at the center of the images. The same architectures and hyperparameters were used for both datasets. In ‘fMRI on ImageNet’ dataset [
                              <a id="xref-ref-43-7" class="xref-bibr" href="#ref-43">
                               43
                              </a>
                              ], which was used also for image classification, 1250 images were drawn from 200 selected ImageNet categories. 150 categories (classes) were used as training data (8 images per category – altogether 1200 training images). The 50 remaining image categories were designated as the novel test categories, represented by 50 test images (1 image from each test category) to be recovered from their fMRI recordings (“test-fMRI”). Importantly, we averaged across same-stimulus repeated fMRI recordings, when available (
                              <a id="xref-table-wrap-1-2" class="xref-table" href="#T1">
                               Table 1
                              </a>
                              ). This gave rise to a higher signal-to-noise ratio fMRI samples, and thus to best reconstruction and classification performance (see Supplementary-Material for single-trial reconstruction, identification, and classification results). This further enabled direct comparison with previous works [
                              <a id="xref-ref-12-5" class="xref-bibr" href="#ref-12">
                               12
                              </a>
                              ,
                              <a id="xref-ref-13-4" class="xref-bibr" href="#ref-13">
                               13
                              </a>
                              ,
                              <a id="xref-ref-36-5" class="xref-bibr" href="#ref-36">
                               36
                              </a>
                              ].
                             </p>
                             <div id="sec-11" class="subsection">
                              <h4>
                               External (unpaired) images database
                              </h4>
                              <p id="p-51">
                               For unsupervised training on unpaired images (Encoder-Decoder objective,
                               <a id="xref-fig-1-13" class="xref-fig" href="#F1">
                                Fig 1d
                               </a>
                               ) we used additional 49K natural images from 980 classes of ImageNet (”ILSVRC”) train-data [
                               <a id="xref-ref-44-3" class="xref-bibr" href="#ref-44">
                                44
                               </a>
                               ]. We verified that the images and categories in our additional unpaired external dataset do not overlap with the test-images and test-categories in the ‘fMRI on ImageNet’ dataset (the inference target). Since the 50 test classes of ‘fMRI on ImageNet’ [
                               <a id="xref-ref-43-8" class="xref-bibr" href="#ref-43">
                                43
                               </a>
                               ] partially overlap with the 1000 original ILSVRC classes, we particularly discarded the 20 overlapping classes. At test-time, we tested classification against 1030 classes (980+50).
                              </p>
                             </div>
                            </div>
                           </div>
                           <div class="section" id="sec-12">
                            <h2 class="">
                             4 Results
                            </h2>
                            <p id="p-52">
                             To test the feasibility of our approach we experimented with two publicly available benchmark fMRI datasets: (i) fMRI on ImageNet [
                             <a id="xref-ref-43-9" class="xref-bibr" href="#ref-43">
                              43
                             </a>
                             ], and (ii) vim-1 [
                             <a id="xref-ref-26-3" class="xref-bibr" href="#ref-26">
                              26
                             </a>
                             ].
                            </p>
                            <div id="sec-13" class="subsection">
                             <h3>
                              4.1 Image Reconstruction from fMRI
                             </h3>
                             <p id="p-53">
                              <a id="xref-fig-2-4" class="xref-fig" href="#F2">
                               Fig 2
                              </a>
                              shows our results with the proposed method, including the combined supervised and self-supervised training with perceptual criteria. These results (in red frames – 3rd column) are contrasted with the results obtainable when using supervised training only (e.g., the 1200 paired training examples of the ‘fMRI on ImageNet’ dataset – 2nd column), as well as when not using perceptual criteria (4th column). All the displayed images were reconstructed from the dataset’s test-fMRI cohort (fMRI of new images from never-before-seen semantic categories). The red-framed images show many faithfully-reconstructed shapes, textures, and colors, which depict recognizable scenes and objects. In contrast, using the supervised objective alone led to reconstructions that were considerably less recognizable (2nd column). Similarly, excluding the perceptual criteria as well as the extended Encoder architecture led to reconstructions that were less perceptually understandable (4th column). The reconstructions of the entire test cohort (50 images in the ‘fMRI on ImageNet’ dataset) as well as an assessment of the separated contribution of the perceptual criteria and the extended Encoder architecture can be found in the Supplementary-Material.
                             </p>
                             <p id="p-54">
                              To verify that our method can successfully be applied to different subjects,
                              <a id="xref-fig-7-1" class="xref-fig" href="#F7">
                               Fig 7
                              </a>
                              shows the reconstructions for all five subjects in the ‘fMRI on ImageNet’ dataset. Note that using fMRI data of different subjects give rise to varying quality of reconstruction as driven by the varied SNR in the subjects’ fMRI data (which is substantially affected by the subject’s ability to maintain fixation at the center pixel in all repeated scans of the same image). Nevertheless, clear and common identifying markers of the ground truth image appear across all subjects. The red frame indicates the results for the best subject (Subject 3 in the dataset) which is the subject of focus in the remaining parts of this paper (unless mentioned otherwise).
                             </p>
                             <div id="F7" class="fig pos-float type-fig odd">
                              <div class="highwire-figure">
                               <div class="fig-inline-img-wrapper">
                                <div class="fig-inline-img">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F7.large.jpg?width=800&height=600&carousel=1" title="Reconstructions for all five subjects in ‘fMRI on ImageNet’ [43]. Reconstructed images when using the full method, which includes training on unpaired data (1d,e). Reconstruction quality varies across subjects, depending on noise-ceiling/SNR of subjects’ data (voxel median noise-ceiling for subjects 1-5: 0.56, 0.57, 0.73, 0.68, 0.58). Subject 3 (in the dataset), which is framed above in red, is the subject of focus in the remaining parts of this paper unless remarked otherwise." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-1196661220" data-figure-caption='<div class="highwire-markup"><span xmlns="http://www.w3.org/1999/xhtml" class="caption-title">Reconstructions for all five subjects in ‘fMRI on ImageNet’ [43].</span> Reconstructed images when using the full method, which includes training on unpaired data (1d,e). Reconstruction quality varies across subjects, depending on noise-ceiling/SNR of subjects’ data (voxel median noise-ceiling for subjects 1-5: 0.56, 0.57, 0.73, 0.68, 0.58). Subject 3 (in the dataset), which is framed above in red, is the subject of focus in the remaining parts of this paper unless remarked otherwise.</div>' data-icon-position="" data-hide-link-title="0">
                                  <span class="hw-responsive-img">
                                   <img class="highwire-fragment fragment-image lazyload" alt="Figure 7." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F7.medium.gif" width="440" height="297"/>
                                   <noscript>
                                    <img class="highwire-fragment fragment-image" alt="Figure 7." src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F7.medium.gif" width="440" height="297"/>
                                   </noscript>
                                  </span>
                                 </a>
                                </div>
                               </div>
                               <ul class="highwire-figure-links inline">
                                <li class="download-fig first">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F7.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Figure 7." data-icon-position="" data-hide-link-title="0">
                                  Download figure
                                 </a>
                                </li>
                                <li class="new-tab last">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F7.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">
                                  Open in new tab
                                 </a>
                                </li>
                               </ul>
                              </div>
                              <div class="fig-caption">
                               <span class="fig-label">
                                Figure 7.
                               </span>
                               <span class="caption-title">
                                Reconstructions for all five subjects in ‘fMRI on ImageNet’ [
                                <a id="xref-ref-43-10" class="xref-bibr" href="#ref-43">
                                 43
                                </a>
                                ].
                               </span>
                               <p id="p-55" class="first-child">
                                Reconstructed images when using the full method, which includes training on unpaired data (1d,e). Reconstruction quality varies across subjects, depending on noise-ceiling/SNR of subjects’ data (voxel median noise-ceiling for subjects 1-5: 0.56, 0.57, 0.73, 0.68, 0.58). Subject 3 (in the dataset), which is framed above in red, is the subject of focus in the remaining parts of this paper unless remarked otherwise.
                               </p>
                               <div class="sb-div caption-clear">
                               </div>
                              </div>
                             </div>
                             <div id="sec-14" class="subsection">
                              <h4>
                               4.1.1 Comparison with state-of-the-art image reconstruction methods
                              </h4>
                              <p id="p-56">
                               We compared our reconstruction results against the two leading methods: Shen et al. [
                               <a id="xref-ref-12-6" class="xref-bibr" href="#ref-12">
                                12
                               </a>
                               ]) and St-Yves et al. [
                               <a id="xref-ref-36-6" class="xref-bibr" href="#ref-36">
                                36
                               </a>
                               ] – each on its relevant dataset.
                               <a id="xref-fig-8-3" class="xref-fig" href="#F8">
                                Fig 8a,b
                               </a>
                               compares the results of our method with those two methods (both of which are deep-learning GAN-based methods). Visual comparison of [
                               <a id="xref-ref-12-7" class="xref-bibr" href="#ref-12">
                                12
                               </a>
                               ,
                               <a id="xref-ref-36-7" class="xref-bibr" href="#ref-36">
                                36
                               </a>
                               ] with our method (
                               <a id="xref-fig-8-4" class="xref-fig" href="#F8">
                                Fig 8a,b
                               </a>
                               ) highlights that despite their natural-like visual appearance enforced by the GAN, the reconstructed images of [
                               <a id="xref-ref-12-8" class="xref-bibr" href="#ref-12">
                                12
                               </a>
                               ,
                               <a id="xref-ref-36-8" class="xref-bibr" href="#ref-36">
                                36
                               </a>
                               ] are often not faithful to the underlying ground truth image. We further report quantitative comparisons, both by image-metric-based evaluation and by human visual evaluation for the top-SNR subject from each dataset (Subject 3 in ‘fMRI on ImageNet’ dataset [
                               <a id="xref-ref-43-11" class="xref-bibr" href="#ref-43">
                                43
                               </a>
                               ]; Subject 1 in ‘vim-1‘ dataset [
                               <a id="xref-ref-26-4" class="xref-bibr" href="#ref-26">
                                26
                               </a>
                               ]). Our quantitative evaluations are based on an n-way identification task [
                               <a id="xref-ref-12-9" class="xref-bibr" href="#ref-12">
                                12
                               </a>
                               ,
                               <a id="xref-ref-31-2" class="xref-bibr" href="#ref-31">
                                31
                               </a>
                               ,
                               <a id="xref-ref-33-1" class="xref-bibr" href="#ref-33">
                                33
                               </a>
                               ,
                               <a id="xref-ref-37-1" class="xref-bibr" href="#ref-37">
                                37
                               </a>
                               ]. Namely, each reconstructed image is compared against n candidate images (the ground truth image, and (n − 1) other randomly selected images), and the goal is to identify its ground truth. We considered two identification methods under this task:
                              </p>
                              <div id="F8" class="fig pos-float type-fig odd">
                               <div class="highwire-figure">
                                <div class="fig-inline-img-wrapper">
                                 <div class="fig-inline-img">
                                  <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F8.large.jpg?width=800&height=600&carousel=1" title="Comparison of image-reconstruction with state-of-the-art methods. (a), (b) Visual comparison with [12, 36] – each compared on its relevant dataset. Our method reconstructs shapes, details and global layout in images better than the leading methods. (c), (d) Quantitative comparisons of identification accuracy (per method) in n-way identification task according to Perceptual Similarity metric (see text for details). (e), (f) n-way identification responses of human raters via Mechanical Turk. Our self-supervised approach significantly outperforms all baseline methods on two datasets and across n-way difficulty levels by both types of experiments – image-metric-based and behavioral human-based (Wilcoxon, N = 50, 120 for panels (c), (d); Mann-Whitney, N = 45 for panels (e), (f)). 95% Confidence Intervals by bootstrap shown on charts." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-1196661220" data-figure-caption='<div class="highwire-markup"><div xmlns="http://www.w3.org/1999/xhtml"><span class="caption-title">Comparison of image-reconstruction with state-of-the-art methods.</span> <strong>(a), (b)</strong> Visual comparison with [12, 36] – each compared on its relevant dataset. Our method reconstructs shapes, details and global layout in images better than the leading methods. <strong>(c), (d)</strong> Quantitative comparisons of identification accuracy (per method) in n-way identification task according to Perceptual Similarity metric (see text for details). <strong>(e), (f)</strong> n-way identification responses of human raters via Mechanical Turk. Our self-supervised approach significantly outperforms all baseline methods on two datasets and across n-way difficulty levels by both types of experiments – image-metric-based and behavioral human-based (Wilcoxon, N = 50, 120 for panels <strong>(c), (d)</strong>; Mann-Whitney, N = 45 for panels <strong>(e), (f)</strong>). 95% Confidence Intervals by bootstrap shown on charts.</div></div>' data-icon-position="" data-hide-link-title="0">
                                   <span class="hw-responsive-img">
                                    <img class="highwire-fragment fragment-image lazyload" alt="Figure 8." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F8.medium.gif" width="296" height="440"/>
                                    <noscript>
                                     <img class="highwire-fragment fragment-image" alt="Figure 8." src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F8.medium.gif" width="296" height="440"/>
                                    </noscript>
                                   </span>
                                  </a>
                                 </div>
                                </div>
                                <ul class="highwire-figure-links inline">
                                 <li class="download-fig first">
                                  <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F8.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Figure 8." data-icon-position="" data-hide-link-title="0">
                                   Download figure
                                  </a>
                                 </li>
                                 <li class="new-tab last">
                                  <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F8.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">
                                   Open in new tab
                                  </a>
                                 </li>
                                </ul>
                               </div>
                               <div class="fig-caption">
                                <span class="fig-label">
                                 Figure 8.
                                </span>
                                <span class="caption-title">
                                 Comparison of image-reconstruction with state-of-the-art methods.
                                </span>
                                <p id="p-57" class="first-child">
                                 <strong>
                                  (a), (b)
                                 </strong>
                                 Visual comparison with [
                                 <a id="xref-ref-12-10" class="xref-bibr" href="#ref-12">
                                  12
                                 </a>
                                 ,
                                 <a id="xref-ref-36-9" class="xref-bibr" href="#ref-36">
                                  36
                                 </a>
                                 ] – each compared on its relevant dataset. Our method reconstructs shapes, details and global layout in images better than the leading methods.
                                 <strong>
                                  (c), (d)
                                 </strong>
                                 Quantitative comparisons of identification accuracy (per method) in n-way identification task according to Perceptual Similarity metric (see text for details).
                                 <strong>
                                  (e), (f)
                                 </strong>
                                 n-way identification responses of human raters via Mechanical Turk. Our self-supervised approach significantly outperforms all baseline methods on two datasets and across n-way difficulty levels by both types of experiments – image-metric-based and behavioral human-based (Wilcoxon, N = 50, 120 for panels
                                 <strong>
                                  (c), (d)
                                 </strong>
                                 ; Mann-Whitney, N = 45 for panels
                                 <strong>
                                  (e), (f)
                                 </strong>
                                 ). 95% Confidence Intervals by bootstrap shown on charts.
                                </p>
                                <div class="sb-div caption-clear">
                                </div>
                               </div>
                              </div>
                              <div id="sec-15" class="subsection">
                               <h5>
                                (i) Image-metric-based identification performed using the Perceptual Similarity metric [
                                <a id="xref-ref-46-4" class="xref-bibr" href="#ref-46">
                                 46
                                </a>
                                ] (between the reconstructed image and the candidate image)
                               </h5>
                               <p id="p-58">
                                Each reconstructed image is compared against n images, and the nearest-neighbor candidate image under this metric was determined to be the identified ‘correct’ image. Panels 8c,d show the correct-identification rate (for each method separately) for n-way identification tasks for n = 2, 5, 10, 50. We evaluate our method and two variants of the method provided by [
                                <a id="xref-ref-12-11" class="xref-bibr" href="#ref-12">
                                 12
                                </a>
                                ] on the ‘fMRI on ImageNet’ benchmark dataset (
                                <a id="xref-fig-8-5" class="xref-fig" href="#F8">
                                 Fig 8c
                                </a>
                                ). In 2-way identification task our method scored an accuracy of 98.5% (SEM
                                <sup>
                                 <a id="xref-fn-5-1" class="xref-fn" href="#fn-5">
                                  1
                                 </a>
                                </sup>
                                = 0.6%, N = 50). Our accuracy remains quite high for all n, outperforming both variants of [
                                <a id="xref-ref-12-12" class="xref-bibr" href="#ref-12">
                                 12
                                </a>
                                ] by a margin of 8-46% across all task difficulty levels (n = 2, 5, 10, 50). We repeated the analysis for ‘vim-1’ fMRI dataset (
                                <a id="xref-fig-8-6" class="xref-fig" href="#F8">
                                 Fig 8d
                                </a>
                                ), where our method scored an accuracy of 87.5% (SEM = 1.7%, N = 120) (2-way task), outperforming the method from [
                                <a id="xref-ref-36-10" class="xref-bibr" href="#ref-36">
                                 36
                                </a>
                                ] by a large margin of 28-40% across the same difficulty levels. Particularly in the challenging 50-way task our method achieved striking leaps: outperforming the baseline by more than x2 prediction accuracy in ‘fMRI on ImageNet’, and more than x10 better prediction accuracy in ‘vim-1’. Importantly, the statistical power of these findings generalizes beyond the specific 50 test examples (image-fMRI) in ‘fMRI on ImageNet’, or the 120 test examples in ‘vim-1’ (Wilcoxon test, N = 50, 120, p < 10
                                <sup>
                                 −5
                                </sup>
                                ).
                               </p>
                              </div>
                              <div id="sec-16" class="subsection">
                               <h5>
                                (ii) Human-based identification
                               </h5>
                               <p id="p-59">
                                Panels 8e,f show reconstruction evaluation results when repeating the same quantitative comparison approach, but this time outsourcing the n-way identification task (n = 2, 5, 10) to random human raters. We used Mechanical Turk to launch surveys to new 45 raters for each evaluated method. Our method scored 90.4% (SEM = 0.6%, N = 45) and 85.0% (SEM = 1.5%, N = 45) in a 2-way identification task on ‘fMRI on ImageNet’ and ‘vim-1’ respectively; Scaling the task difficulty up to 10-way, our method scored 65.0% (SEM = 3.0%, N = 45) and 44.3% (SEM = 2.5%, N = 45). Overall our method significantly outperformed the previous methods, on both datasets, and across difficulty levels by a margin of at least 8.7% (Mann-Whitney test, N = 45, p < .001).
                               </p>
                               <p id="p-60">
                                Notably, the identification accuracy of each reconstructed image when using the image-metric for evaluation, mostly depended on the reconstruction quality of the specific image, and was robust to randomizing the selection of the (non-ground-truth) candidate images. Furthermore, the choice of candidate images in the human-based evaluation was fixed across the raters and the methods we compared. Therefore, while the two types of evaluations (image-metric and human-based) consider a seemingly similar n-way identification task, they are not directly comparable. Additionally, note that they suggest different statistical generalization insights – generalization beyond the specific set of test examples, when using the image-metric (Wilcoxon test, N = 50, 120, p < 10
                                <sup>
                                 −5
                                </sup>
                                ), and generalization beyond the specific pool of human raters, in the human-based evaluations (Mann-Whitney test, N = 45, p < .001). Overall, our method significantly outperforms state-of-the-art methods by a large margin in both image-metric-based and human-based evaluations.
                               </p>
                              </div>
                             </div>
                            </div>
                            <div id="sec-17" class="subsection">
                             <h3>
                              4.2 Decoding rich novel semantic categories of reconstructed images
                             </h3>
                             <p id="p-61">
                              The benefit of our self-supervised approach extends beyond the task of image reconstruction. It introduces significant gains in the task of semantic classification of the reconstructed images to their novel semantic categories (i.e., categories/classes never seen during training – classes neither represented in the ‘paired’ training set, nor in the ‘unpaired’ external images). For the classification task, we consider the ‘fMRI on ImageNet’ dataset [
                              <a id="xref-ref-43-12" class="xref-bibr" href="#ref-43">
                               43
                              </a>
                              ], whose train-set contains 1200 images from 150 ImageNet classes. Its test-set contains 50 images from disjoint ImageNet classes. The unpaired external images used for the self-supervised training of our network, are drawn from 980 ImageNet classes. Notably, the unpaired images have no fMRI recordings whatsoever. The 50 test classes (to be recovered from the 50 test-fMRIs), are neither included in the 150 ’paired’ train-classes, nor in the 980 classes of the ’unpaired’ external images. These are totally novel classes (details in
                              <a id="xref-sec-10-1" class="xref-sec" href="#sec-10">
                               Sec 3.3
                              </a>
                              ). When checking the classification results of the 50 test fMRI, we test the classification of their reconstructed images against the gallery of 1030 ImageNet classes
                              <sup>
                               <a id="xref-fn-6-1" class="xref-fn" href="#fn-6">
                                2
                               </a>
                              </sup>
                              . As mentioned earlier (
                              <a id="xref-sec-4-1" class="xref-sec" href="#sec-4">
                               Sec 2.2
                              </a>
                              ), such classification requires no explicit class-labels in the training (see
                              <a id="xref-fig-4-6" class="xref-fig" href="#F4">
                               Fig 4
                              </a>
                              ). It is achieved by comparing the Deep-Features of the reconstructed image, with the Deep-Feature class-representative vector – one representative vector for each of the 1030 classes in our gallery (or any other gallery of novel classes).
                             </p>
                             <p id="p-62">
                              <a id="xref-fig-5-3" class="xref-fig" href="#F5">
                               Fig 5a
                              </a>
                              exemplifies novel-class classification results by our method. We present each reconstructed image alongside the five top predicted (‘Top-5’) classes among the 1030 classes. For visualization purpose only, each of the Top-5 classes are visually exemplified by the nearest-neighbor image (most similar Deep-Features) among
                              <span class="underline">
                               100 randomly selected images
                              </span>
                              from that class label. The first five rows show correct-classification cases at Top-5 accuracy level. In these cases our method successfully includes the ground truth class among the nearest five classes (marked by a red frame). Interestingly, many of the non-ground-truth classes which are assigned among the Top-5, are also reasonable, frequently representing semantic and visual content, which is reminiscent of the ground truth class as well. For example, the ‘housefly’ is found similar also to other flies and comparable shape insects, and the ‘beer mug’ is also found similar to other types of mugs. The bottom rows show incorrect-classification cases, where the ground truth class is not found among the Top-5 classes. Nonetheless, even in these allegedly “failure” cases, many of the Top-5 classes (and even Top-1) are considerably relevant both semantically and visually. For example, the reconstructed ‘video’ image was associated with the ‘television’ class; the ‘duck’ was wrongly classified as an ‘ostrich’ ; the ‘leopard’ was wrongly classified as a ’lion’; the ‘minaret’ was associated with several other tower-like classes, etc.
                             </p>
                             <p id="p-63">
                              We evaluate the performance of our classification results using two types of quantitative evaluations: (i) the “Classification Rank” – the average rank of the correct class among all classes (
                              <a id="xref-table-wrap-2-2" class="xref-table" href="#T2">
                               Table 2
                              </a>
                              ), and (ii) the more familiar “n-way classification” accuracy (evaluated for n = 50; 100; 500; 1000,
                              <a id="xref-fig-5-4" class="xref-fig" href="#F5">
                               Fig 5b
                              </a>
                              ). These 2 types of numerical evaluation are detailed below.
                             </p>
                             <div id="sec-18" class="subsection">
                              <h4>
                               (i) Classification Rank
                              </h4>
                              <p id="p-64">
                               As explained in
                               <a id="xref-sec-4-2" class="xref-sec" href="#sec-4">
                                Sec 2.2
                               </a>
                               , the location of the ground-truth class within the list of 1030 classes (sorted according to our similarity-based classification measure - see
                               <a id="xref-sec-4-3" class="xref-sec" href="#sec-4">
                                Sec 2.2
                               </a>
                               ), provides a “rank score” for evaluating our classification accuracy per image (where rank=1 out of 1030 means perfect score). Moreover, this rank criterion further allows to asses the generalization power of our method beyond the limited test-set (the 50 test images and their 50 specific test classes available in the benchmark dataset).
                               <a id="xref-table-wrap-2-3" class="xref-table" href="#T2">
                                Table 2
                               </a>
                               summarizes novel-class classification rank results for all five subjects in ‘fMRI on ImageNet’. To demonstrate the power of our self-supervised approach, we compare its classification performance with a baseline of the purely supervised approach. This baseline uses reconstructions that were produced by a Decoder trained using the scarce paired data alone (
                               <a id="xref-fig-1-14" class="xref-fig" href="#F1">
                                Fig 1b
                               </a>
                               ). This comparison shows a leap improvement in median classification rank in favor of our self-supervised approach in all five subjects. Notably, for Subjects 2-5 (excluding Subjects 1 who has the lowest median noise-ceiling), the advantage of our self-supervised approach generalizes beyond the specifically chosen 50 test images and classes of the considered dataset.
                              </p>
                             </div>
                             <div id="sec-19" class="subsection">
                              <h4>
                               (ii) n-way Classification
                              </h4>
                              <p id="p-65">
                               In addition to the Ranking-score within the 1030 gallery classes (for each test-fMRI), we present another alternative way of evaluating the classification results – through classification accuracy in n-way classification experiments (for n = 50, 100, 500, 1000), using our automated Deep-Features class-similarity criterion.
                               <a id="xref-fig-5-5" class="xref-fig" href="#F5">
                                Fig 5b
                               </a>
                               shows Top-1 classification accuracy across a range of classification task difficulties (shown for Subject 3; the remaining subjects can be found in the Supplementary-Material). The tasks differ in the number of candidate classes (n-way) from which prediction is made (i.e., the percent of cases that the Top-1 predicted class out of n class labels is indeed the ground-truth class). Our full method scores 29.9% Top-1 accuracy (SEM = 0.3%, N = 25000) in 50-way classification task. Even when scaling to 1000-way (as in ImageNet classification), our method scores 12.1% Top-1 accuracy (SEM = 0.2%, N = 25000), which exceeds chance level accuracy by more than 100-fold. Contrasting this performance with the baseline of the supervised approach shows a striking leap improvement in classification-accuracy in favor of our self-supervised approach: between x2 and x3 accuracy improvement, in all the n-way experiments (n = 50, 100, 500, 1000).
                              </p>
                              <p id="p-66">
                               We further performed an ablation study of the Perceptual Similarity for the task of semantic classification.
                               <a id="xref-fig-5-6" class="xref-fig" href="#F5">
                                Fig 5c
                               </a>
                               shows 1000-way Top-5 classification accuracy by our self-supervised method, where the reconstructions used are produced by ablated versions of the Perceptual Similarity [
                               <a id="xref-ref-46-5" class="xref-bibr" href="#ref-46">
                                46
                               </a>
                               ]. Specifically, we limit the Perceptual Similarity criterion, which is used in Decoder training, to a varying range of Deep VGG layers, starting from using only the outputs of the first block (low ”semantic” features) of VGG16, and up to aggregating outputs from all five blocks (high ”semantic” features) of the pretrained network as in the full method. We find that the classification accuracy of the reconstructed images shows an increasing trend with the number of higher-level features, which are used as reconstruction criteria. This highlights the significance of the Perceptual Similarity reconstruction criterion, which includes higher-level features, for semantic classification. Note that the increasing trend appears to a various degree for different subjects, depending on experiment noise and subject-specific noise-ceiling (e.g., Subject 1 having the lowest noise-ceiling); Nevertheless, the trend is well illustrated by their cross-subject mean accuracy. Furthermore, we find that our classification approach, which is applied onto the reconstructed images, outperforms a direct-fMRI classification approach, where the test-fMRI samples are directly classified (see Supplementary-Material).
                              </p>
                              <p id="p-67">
                               Our classification approach is inspired by [
                               <a id="xref-ref-43-13" class="xref-bibr" href="#ref-43">
                                43
                               </a>
                               ]. Both methods use deep-feature embeddings to search for the nearest class centroid in a gallery of novel classes (our embedding is extracted from the reconstructed image (
                               <a id="xref-fig-4-7" class="xref-fig" href="#F4">
                                Fig 4b
                               </a>
                               ), whereas that of [
                               <a id="xref-ref-43-14" class="xref-bibr" href="#ref-43">
                                43
                               </a>
                               ] employs an intermediate deep image-embedding decoded from fMRI). Notably, [
                               <a id="xref-ref-43-15" class="xref-bibr" href="#ref-43">
                                43
                               </a>
                               ] presented the classification of novel categories in a 2-way task (i.e., discriminating between the correct category and a single random category). Here we scale up this classification task to 1000-way (i.e., finding the correct category among 1000 rich categories). To the best of our knowledge, we are the first to demonstrate such large-scale semantic classification capabilities from fMRI data.
                              </p>
                             </div>
                            </div>
                            <div id="sec-20" class="subsection">
                             <h3>
                              4.3 Predominance of early visual areas in reconstruction
                             </h3>
                             <p id="p-68">
                              To reconstruct the images of ‘fMRI on ImageNet’, we considered 4600 visual-cortex voxels provided and labeled in [
                              <a id="xref-ref-43-16" class="xref-bibr" href="#ref-43">
                               43
                              </a>
                              ]. We studied the contribution of different visual areas to our reconstruction performance. To this end, we selected subsets of voxels according to their marked brain areas, and restricted the training of our Encoder/Decoder to those voxels.
                              <a id="xref-fig-9-1" class="xref-fig" href="#F9">
                               Fig 9
                              </a>
                              shows reconstruction results when using voxels only from the following visual areas: (i) V1 (870 voxels), (ii) V1-V3, which refer to as Lower Visual Cortex (LVC, 2300 voxels), (iii) Fusiform Face Area (FFA), Parahippocampal Place Area (PPA), and Lateral Occipital Cortex (LOC), which we refer to as Higher Visual Cortex (HVC, 2150 voxels), or (iv) Full Visual Cortex (VC = LVC + V4 + HVC, 4600 voxels). These results show that the early visual areas, particularly V1-V3 (LVC), contain most of the information recoverable by our method. Considering voxels only from HVC leads to substantial degradation in performance despite comprising approximately half of the complete visual cortex voxels. Nevertheless, the higher visual areas clearly add semantic interpretability to the reconstructed images (which is evident when comparing the reconstructions from the Full VC, to those from LVC only).
                             </p>
                             <div id="F9" class="fig pos-float type-fig odd">
                              <div class="highwire-figure">
                               <div class="fig-inline-img-wrapper">
                                <div class="fig-inline-img">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F9.large.jpg?width=800&height=600&carousel=1" title="Decoding quality is dominated by early visual areas. Columns show reconstructions using our method with fMRI data from various ROIs in the visual cortex including: • Primary Visual Cortex – V1 • Lower Visual Cortex – V1-V3 • Higher Visual Cortex – Fusiform Face Area (FFA), Parahippocampal Place Area (PPA), Lateral Occipital Cortex (LOC) • Full Visual Cortex – LVC + V4 + HVC (in red frame)." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-1196661220" data-figure-caption='<div class="highwire-markup"><div xmlns="http://www.w3.org/1999/xhtml"><span class="caption-title">Decoding quality is dominated by early visual areas.</span> Columns show reconstructions using our method with fMRI data from various ROIs in the visual cortex including: • <strong>Primary Visual Cortex</strong> – V1 • <strong>Lower Visual Cortex</strong> – V1-V3 • <strong>Higher Visual Cortex</strong> – Fusiform Face Area (FFA), Parahippocampal Place Area (PPA), Lateral Occipital Cortex (LOC) • <strong>Full Visual Cortex</strong> – LVC + V4 + HVC (in red frame).</div></div>' data-icon-position="" data-hide-link-title="0">
                                  <span class="hw-responsive-img">
                                   <img class="highwire-fragment fragment-image lazyload" alt="Figure 9." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F9.medium.gif" width="440" height="263"/>
                                   <noscript>
                                    <img class="highwire-fragment fragment-image" alt="Figure 9." src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F9.medium.gif" width="440" height="263"/>
                                   </noscript>
                                  </span>
                                 </a>
                                </div>
                               </div>
                               <ul class="highwire-figure-links inline">
                                <li class="download-fig first">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F9.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Figure 9." data-icon-position="" data-hide-link-title="0">
                                  Download figure
                                 </a>
                                </li>
                                <li class="new-tab last">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F9.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">
                                  Open in new tab
                                 </a>
                                </li>
                               </ul>
                              </div>
                              <div class="fig-caption">
                               <span class="fig-label">
                                Figure 9.
                               </span>
                               <span class="caption-title">
                                Decoding quality is dominated by early visual areas.
                               </span>
                               <p id="p-69" class="first-child">
                                Columns show reconstructions using our method with fMRI data from various ROIs in the visual cortex including: •
                                <strong>
                                 Primary Visual Cortex
                                </strong>
                                – V1 •
                                <strong>
                                 Lower Visual Cortex
                                </strong>
                                – V1-V3 •
                                <strong>
                                 Higher Visual Cortex
                                </strong>
                                – Fusiform Face Area (FFA), Parahippocampal Place Area (PPA), Lateral Occipital Cortex (LOC) •
                                <strong>
                                 Full Visual Cortex
                                </strong>
                                – LVC + V4 + HVC (in red frame).
                               </p>
                               <div class="sb-div caption-clear">
                               </div>
                              </div>
                             </div>
                             <p id="p-70">
                              Importantly, we found that removing any single visual area from our dataset, including V1, does not degrade the results significantly, suggesting the existence of information redundancy across visual areas. The results are strongly affected only when several regions, specifically the entire early visual cortex, are discarded. Furthermore adding V4 to either LVC or HVC did not change the results significantly.
                             </p>
                             <p id="p-71">
                              We studied two extensions for the ROI-specific reconstruction results: (i) We controlled for the ROI-voxel count bias by sampling an equal number of voxels from each ROI. This control did not introduce major changes in the cross-ROI comparison or to the conclusion of early-visual-areas predominance; (ii) We analyzed the ROI-specific contribution also in semantic classification. Our results were consistent with those from the ROI-specific reconstruction experiments, showing predominance of early visual areas also in semantic classification. These results can be found in the Supplementary-Material.
                             </p>
                            </div>
                            <div id="sec-21" class="subsection">
                             <h3>
                              4.4 Emergent biologically consistent population receptive fields
                             </h3>
                             <p id="p-72">
                              The human visual system is characterized by the well-known primate retinotopic organization [
                              <a id="xref-ref-50-1" class="xref-bibr" href="#ref-50">
                               50
                              </a>
                              –
                              <a id="xref-ref-58-1" class="xref-bibr" href="#ref-58">
                               58
                              </a>
                              ]. Retinotopy maps reflect the spatial tuning of cortical hypercolumns or of their aggregation into Population Receptive Field (pRF) as in the case of voxel data. Here, we sought to visualize receptive field of voxels as captured by our Encoder & Decoder. To this end we considered two approaches: (i) Interpreting model weights that associate each voxel with spatial locations as a heatmap, and (ii) A gradient-based approach on the Encoder’s input space. While this approach is applicable to the Encoder only, it is favorable over the former approach because it provides a higher resolution receptive field map (i.e., 112×112, following Encoder’s input dimensions vs. 26×26 or 14×14, following the layer’s architecture in the Encoder/Decoder, see details in
                              <a id="xref-sec-23-1" class="xref-sec" href="#sec-23">
                               Sec 6
                              </a>
                              ). Since both methods gave rise to well aligned receptive fields (per voxel, see Supplementary-Material), we henceforth focused our analysis on the receptive fields recovered for the trained Encoder using the gradient-based approach.
                             </p>
                             <p id="p-73">
                              <a id="xref-fig-10-1" class="xref-fig" href="#F10">
                               Fig 10a
                              </a>
                              shows receptive fields for several selected voxels, which indicates their
                              <span class="underline">
                               spatial locality
                              </span>
                              within the image. Next, we estimated the pRFs eccentricity and polar angle for each voxel. Lastly, we plot these data on the subject-specific cortical surface.
                              <a id="xref-fig-10-2" class="xref-fig" href="#F10">
                               Fig 10b,c
                              </a>
                              shows the resulting tuning maps revealing the expected retinotopic organization. This includes the emergence of horizontal and vertical meridians and their transitions, contra-laterality and up-down inversion, and fovea-periphery gradual transition. We emphasize that this organization automatically emerged on its own from our training method involving natural images and fMRI. No biological atlas or other retinotopic prior was imposed. These results support the biological consistency of our model’s predictions.
                             </p>
                             <div id="F10" class="fig pos-float type-fig odd">
                              <div class="highwire-figure">
                               <div class="fig-inline-img-wrapper">
                                <div class="fig-inline-img">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F10.large.jpg?width=800&height=600&carousel=1" title="Our models capture biologically consistent voxel tuning properties. (a) Receptive field of five selected voxels with high SNR from early visual cortex, which indicates their spatial locality in the image. Panels (b)-(e) show single subject data on the corresponding subject-specific cortical surface. (b) Polar angle. (c) Eccentricity tuning, measured by degree of visual angle (DVA). (d) Noise-corrected prediction accuracy. (e) Prediction accuracy (non-scaled Pearson correlation). For simplicity we show the data on either left or right hemisphere. Voxel noise-ceiling is coded by transparency level (alpha channel) in all cortical maps." class="highwire-fragment fragment-images colorbox-load" rel="gallery-fragment-images-1196661220" data-figure-caption='<div class="highwire-markup"><div xmlns="http://www.w3.org/1999/xhtml"><span class="caption-title">Our models capture biologically consistent voxel tuning properties.</span> <strong>(a)</strong> Receptive field of five selected voxels with high SNR from early visual cortex, which indicates their spatial locality in the image. Panels <strong>(b)-(e)</strong> show single subject data on the corresponding subject-specific cortical surface. <strong>(b)</strong> Polar angle. <strong>(c)</strong> Eccentricity tuning, measured by degree of visual angle (DVA). <strong>(d)</strong> Noise-corrected prediction accuracy. <strong>(e)</strong> Prediction accuracy (non-scaled Pearson correlation). For simplicity we show the data on either left or right hemisphere. Voxel noise-ceiling is coded by transparency level (alpha channel) in all cortical maps.</div></div>' data-icon-position="" data-hide-link-title="0">
                                  <span class="hw-responsive-img">
                                   <img class="highwire-fragment fragment-image lazyload" alt="Figure 10." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F10.medium.gif" width="440" height="364"/>
                                   <noscript>
                                    <img class="highwire-fragment fragment-image" alt="Figure 10." src="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F10.medium.gif" width="440" height="364"/>
                                   </noscript>
                                  </span>
                                 </a>
                                </div>
                               </div>
                               <ul class="highwire-figure-links inline">
                                <li class="download-fig first">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F10.large.jpg?download=true" class="highwire-figure-link highwire-figure-link-download" title="Download Figure 10." data-icon-position="" data-hide-link-title="0">
                                  Download figure
                                 </a>
                                </li>
                                <li class="new-tab last">
                                 <a href="https://www.biorxiv.org/content/biorxiv/early/2022/03/22/2020.09.06.284794/F10.large.jpg" class="highwire-figure-link highwire-figure-link-newtab" target="_blank" data-icon-position="" data-hide-link-title="0">
                                  Open in new tab
                                 </a>
                                </li>
                               </ul>
                              </div>
                              <div class="fig-caption">
                               <span class="fig-label">
                                Figure 10.
                               </span>
                               <span class="caption-title">
                                Our models capture biologically consistent voxel tuning properties.
                               </span>
                               <p id="p-74" class="first-child">
                                <strong>
                                 (a)
                                </strong>
                                Receptive field of five selected voxels with high SNR from early visual cortex, which indicates their spatial locality in the image. Panels
                                <strong>
                                 (b)-(e)
                                </strong>
                                show single subject data on the corresponding subject-specific cortical surface.
                                <strong>
                                 (b)
                                </strong>
                                Polar angle.
                                <strong>
                                 (c)
                                </strong>
                                Eccentricity tuning, measured by degree of visual angle (DVA).
                                <strong>
                                 (d)
                                </strong>
                                Noise-corrected prediction accuracy.
                                <strong>
                                 (e)
                                </strong>
                                Prediction accuracy (non-scaled Pearson correlation). For simplicity we show the data on either left or right hemisphere. Voxel noise-ceiling is coded by transparency level (alpha channel) in all cortical maps.
                               </p>
                               <div class="sb-div caption-clear">
                               </div>
                              </div>
                             </div>
                             <p id="p-75">
                              We sought to analyze the prediction accuracy achieved by of our models.
                              <a id="xref-fig-10-3" class="xref-fig" href="#F10">
                               Fig 10d,e
                              </a>
                              show the prediction accuracy distribution (Pearson correlation) of the modeled voxels when normalized by voxel noise-ceiling (
                              <a id="xref-fig-10-4" class="xref-fig" href="#F10">
                               Fig 10d
                              </a>
                              ) and when not normalized (
                              <a id="xref-fig-10-5" class="xref-fig" href="#F10">
                               Fig 10e
                              </a>
                              ). The prediction noise-ceiling is used to provide an estimate of the best possible prediction accuracy obtainable given infinite data [
                              <a id="xref-ref-23-2" class="xref-bibr" href="#ref-23">
                               23
                              </a>
                              ]. These panels show high prediction accuracy in LVC, and low in HVC. Furthermore, they show that throughout the visual cortex, our model markedly saturates the noise-ceiling of the given data. This indicates the sufficient expressive power of our multi-layer encoding model, enabling it to capture the given data complexity. A comparison of the model’s prediction accuracy with a single-layer Encoder architecture from [
                              <a id="xref-ref-45-8" class="xref-bibr" href="#ref-45">
                               45
                              </a>
                              ] can be found in the Supplementary-Material.
                             </p>
                            </div>
                           </div>
                           <div class="section" id="sec-22">
                            <h2 class="">
                             5 Discussion
                            </h2>
                            <p id="p-76">
                             We introduced a novel approach for self-supervised training of image decoders on external, image-only datasets. We show that our method gives rise to both state-of-the-art results in image-reconstruction as well as large scale semantic categorization from fMRI data. To date, the performance in the task of natural image reconstruction and semantic categorization from human fMRI recordings is limited by the characteristics of fMRI datasets. In the typical case, the paired training data are scarce, and represent a narrow semantic coverage.
                            </p>
                            <p id="p-77">
                             Our self-supervised training on tens of thousands of additional unpaired images from wide coverage, combined with high-level Perceptual Similarity constrains, adapts the decoding model to the statistics of natural images and novel categories. Our framework enables substantial improvement in both image reconstruction quality
                             <span class="underline">
                              and
                             </span>
                             classification capabilities compared to methods that rely only on the scarce paired training data. This self-supervised perceptual approach leads to state-of-the-art image reconstructions from fMRI, of unprecedented quality, as supported by image-metric-based evaluations, as well as extensive human-based evaluations. We accomplish this for two substantially different fMRI datasets using a single method (with the same hyperparameters).
                            </p>
                            <p id="p-78">
                             Our self-supervised training on tens of thousands of unpaired external images further leads to unprecedented capabilities in the semantic classification of fMRI data (and moreover, of classes never encountered during training). We consider the challenging 1000-way semantic classification task, and demonstrate a striking leap improvement (more than 2x) in classification performance when applying our self-supervised approach over a purely supervised approach. To the best of our knowledge, we are the first to demonstrate such large-scale semantic classification capabilities (1000-way) from fMRI data.
                            </p>
                            <p id="p-79">
                             The improved reconstruction quality achieved when minimizing deep image representation error rather than low-level pixel-based one (
                             <a id="xref-fig-2-5" class="xref-fig" href="#F2">
                              Fig 2
                             </a>
                             ) supports reports in earlier works that employed similar approaches [
                             <a id="xref-ref-12-13" class="xref-bibr" href="#ref-12">
                              12
                             </a>
                             ,
                             <a id="xref-ref-13-5" class="xref-bibr" href="#ref-13">
                              13
                             </a>
                             ]. In [
                             <a id="xref-ref-12-14" class="xref-bibr" href="#ref-12">
                              12
                             </a>
                             ], Perceptual Similarity was used to drive image reconstruction by iterative optimization at test time. Here, however, the Decoder itself is trained to minimize the higher-level reconstruction error, allowing a non-iterative feed-forward reconstruction at test-time. In this respect, our approach is more compatible with [
                             <a id="xref-ref-13-6" class="xref-bibr" href="#ref-13">
                              13
                             </a>
                             ], albeit our reconstruction loss is based on the complete Perceptual Similarity metric [
                             <a id="xref-ref-46-6" class="xref-bibr" href="#ref-46">
                              46
                             </a>
                             ], defined via features extracted at multiple intermediate layers of a pretrained object classification network rather than only its almost last layer. Furthermore, this work goes beyond [
                             <a id="xref-ref-12-15" class="xref-bibr" href="#ref-12">
                              12
                             </a>
                             ,
                             <a id="xref-ref-13-7" class="xref-bibr" href="#ref-13">
                              13
                             </a>
                             ] in two aspects: (i) Our Decoder benefited from applying Perceptual Similarity to substantially more data with no fMRI recording. The combined effect of this type of reconstruction criterion with the additional unpaired data is very effective, as evident in
                             <a id="xref-fig-2-6" class="xref-fig" href="#F2">
                              Fig 2
                             </a>
                             ; (ii) We show that applying Perceptual Similarity not only results in an improved reconstruction quality, but also gives rise to a higher classification accuracy of novel ImageNet classes (classes that were not represented in any of the training data). An important question for future research is whether a Natural Image Prior, such as the Deep Generator Network employed by [
                             <a id="xref-ref-12-16" class="xref-bibr" href="#ref-12">
                              12
                             </a>
                             ] can be combined with the self-supervised approach in a way that benefits from both the self-supervision on additional unpaired data and the imposed narrower Natural Image search space.
                            </p>
                            <p id="p-80">
                             Our ablation studies indicate that reconstruction quality is dominated by data originating from Lower Visual Cortex (V1-V3). The extended architecture of the Encoder, which incorporates high-level deep features, was designed to improve information-harnessing from the Higher Visual Cortex (HVC) as well. Indeed prediction accuracy maps show that the noise-ceiling is saturated throughout the visual cortex, including in higher visual areas. These findings suggest a reasonable representation of the HVC by our model. Nevertheless, the SNR of the data arising from these areas renders them weaker contributors to overall reconstruction quality.
                            </p>
                            <p id="p-81">
                             We provide evidence for the retinotopic organization implicitly learned (on its own) by our image-to-fMRI Encoder. This suggests that our models are biologically meaningful, as opposed to tailored and overfit to a limited dataset. Note that while we show data for the Encoder, we verified in our experiments that model voxels in the Decoder and the Encoder indeed agree (while not explicitly forced to do so).
                            </p>
                            <p id="p-82">
                             The proposed method currently focuses on data from individual subjects. A natural extension of the present work is to combine information across multiple subjects (see [
                             <a id="xref-ref-59-1" class="xref-bibr" href="#ref-59">
                              59
                             </a>
                             ]). This is part of our future work.
                            </p>
                           </div>
                           <div class="section" id="sec-23">
                            <h2 class="">
                             6 Additional Technical Details
                            </h2>
                            <div id="sec-24" class="subsection">
                             <h3>
                              Hyperparameters
                             </h3>
                             <p id="p-83">
                              We trained the Encoder using Adam optimizer for 50 epochs with an initial learning rate of 1e-3, with a 90% learning rate drop using milestones (20, 30, and 35 epochs). During Decoder training with supervised and unsupervised objectives, each training batch contained 16 pairs (supervised training), and 16 unpaired natural images (randomly sampled from the external image database – images without fMRI). We trained the Decoder for 150 epochs using Adam optimizer with an initial learning rate of 1e-3, and 80% learning rate drop after every 30 epochs.
                             </p>
                            </div>
                            <div id="sec-25" class="subsection">
                             <h3>
                              Runtime
                             </h3>
                             <p id="p-84">
                              Our system completes the two-stage training within approximately 1.5 hours using a single Tesla V100 GPU. Once trained, the inference itself (decoding of a new fMRI) is performed in a few milliseconds per image.
                             </p>
                            </div>
                            <div id="sec-26" class="subsection">
                             <h3>
                              Behavioral experiments
                             </h3>
                             <p id="p-85">
                              The participants in the Mechanical Turk behavioral experiments gave their online informed consent to be recorded, and were granted financial incentives for every completed survey. The research protocol was reviewed and approved by the Bioethics and Embryonic Stem Cell Research Oversight (ESCRO) Committee at the Weizmann Institute of Science. In order to assure the validity of the behavioral data (e.g. bot observers, fatigue along the survey), we screened subjects according to their score in interleaved sanity check experiments. The sanity check experiments comprised adding to the actual experiments also 10% unexpected trivial identification tasks of mildly degraded versions of the ground truth images, instead of the reconstructed images. We further discarded subjects with MTurk success-score (reputation) lower than 97%. Each survey consisted of 50 or 20 trials corresponding to the number of test-images comparison in ‘fMRI on ImageNet’ [
                              <a id="xref-ref-43-17" class="xref-bibr" href="#ref-43">
                               43
                              </a>
                              ] or ‘vim-1’ [
                              <a id="xref-ref-26-5" class="xref-bibr" href="#ref-26">
                               26
                              </a>
                              ]
                              <sup>
                               <a id="xref-fn-7-1" class="xref-fn" href="#fn-7">
                                3
                               </a>
                              </sup>
                              , all of which were reconstructed using a single particular method. In each trial subjects were presented with a reconstructed image and n candidate images, the ground-truth image and n 1 distractor images, and were prompted ”Which image at the bottom row is most similar to the image at the top row?”. To assure task difficulty agreement across subjects and reconstruction methods the set of distractor images was randomly selected for each test-image, but remained fixed across surveys; Our results were insensitive to their re-selection.
                             </p>
                            </div>
                            <div id="sec-27" class="subsection">
                             <h3>
                              Semantic category decoding
                             </h3>
                             <p id="p-86">
                              We defined the feature vector underlying the class representatives to be the outputs of block 4 in AlexNet, and used Pearson correlation as the distance metric for ranking class representatives. Our experiments showed that using this intermediate representation level as the embedding of choice yields optimal results for classification.
                             </p>
                            </div>
                            <div id="sec-28" class="subsection">
                             <h3>
                              Voxel receptive field visualization and estimation
                             </h3>
                             <p id="p-87">
                              We analyzed the receptive field of our encoding model in terms of simulated retinotopy as previously proposed in [
                              <a id="xref-ref-39-3" class="xref-bibr" href="#ref-39">
                               39
                              </a>
                              ,
                              <a id="xref-ref-60-1" class="xref-bibr" href="#ref-60">
                               60
                              </a>
                              ]. To generate analogous spatial tuning maps for our modeled voxels, we estimated the voxel spatial tuning captured by our models. We start by visualizing each voxel’s receptive field (pRF) using the trained Encoder. Our primary approach to this end was gradient-based [
                              <a id="xref-ref-61-1" class="xref-bibr" href="#ref-61">
                               61
                              </a>
                              ,
                              <a id="xref-ref-62-1" class="xref-bibr" href="#ref-62">
                               62
                              </a>
                              ]: Given a random input image, we compute the gradient of a particular voxel with respect to this input. This allows to visualize the image which would drive the maximum change in activity at the target voxel. To produce a heat-map, the values within the resulting gradient-image are squared, averaged across the color-channels, and normalized. Next, we define the pRF center as the center of mass of the preprocessed map. The preprocessing was designed to minimize noise effects. It included map smoothing with a Gaussian kernel, σ = 3, followed by raising the map values to the power of 10 (emphasizing the areas with higher values). About 15% of the voxels had pRF maps which were not confined spatially around a center of mass, and were thus discarded in subsequent analysis. The remaining 85% voxels were considered in the retinotopy maps. In a secondary approach, we recovered voxel receptive field maps for both the Encoder and the Decoder from the learned voxel weights. For the Encoder, we directly visualized the spatial map weights of the space-feature locally-connected layer. For the Decoder, we reshaped the input fully-connected layer to square spatial dimensions and 64 channels and computed the mean square weights across channels to yield a heat map.
                             </p>
                            </div>
                            <div id="sec-29" class="subsection">
                             <h3>
                              Cortical surface visualization
                             </h3>
                             <p id="p-88">
                              We reconstructed and flattened the individual cortical surfaces using FreeSurfer, and plotted the data using a custom extension of PySurfer.
                             </p>
                            </div>
                            <div id="sec-30" class="subsection">
                             <h3>
                              Noise-Ceiling
                             </h3>
                             <p id="p-89">
                              We estimated the fMRI prediction Noise-Ceiling by half-split over the test data repeats following [
                              <a id="xref-ref-23-3" class="xref-bibr" href="#ref-23">
                               23
                              </a>
                              ].
                             </p>
                            </div>
                            <div id="sec-31" class="subsection">
                             <h3>
                              Statistics
                             </h3>
                             <p id="p-90">
                              We used Wilcoxon signed-rank (paired) test (two-tailed) for significance testing in the image-metric-based multi-image identification experiments. For the rank-classification experiments, and the (unpaired) behavioral experiments we used Mann-Whitney rank test.
                             </p>
                            </div>
                           </div>
                           <div class="section" id="sec-33">
                            <h2 class="">
                             CRediT authorship contribution statement
                            </h2>
                            <p id="p-91">
                             <strong>
                              Guy Gaziv:
                             </strong>
                             Writing - Original Draft, Conceptualization, Methodology, Investigation, Formal analysis, Visualization.
                             <strong>
                              Roman Beliy:
                             </strong>
                             Conceptualization, Methodology, Investigation, Software.
                             <strong>
                              Niv Granot:
                             </strong>
                             Conceptualization, Methodology, Investigation, Software (added the Perceptual Similarity to the framework).
                             <strong>
                              Assaf Hoogi:
                             </strong>
                             Resources, Investigation (human-based experiments).
                             <strong>
                              Francesca Strappini:
                             </strong>
                             Resources, Visualization (fMRI preprocessing and brain visualization).
                             <strong>
                              Tal Golan:
                             </strong>
                             Resources, Visualization, Formal analysis (fMRI preprocessing, brain visualization, advised on statistical testing).
                             <strong>
                              Michal Irani:
                             </strong>
                             Conceptualization, Methodology, Investigation, Supervision, Funding acquisition.
                             <strong>
                              All authors:
                             </strong>
                             Writing - Review & Editing.
                            </p>
                           </div>
                           <div class="section" id="sec-34">
                            <h2 class="">
                             Data and code availability statements
                            </h2>
                            <p id="p-92">
                             We used publicly available fMRI datasets as well as image datasets as follows: fMRI on ImageNet, vim-1, and ImageNet. We will make our code publicly available upon publication.
                            </p>
                           </div>
                           <div class="section ack" id="ack-1">
                            <h2 class="">
                             Acknowledgments
                            </h2>
                            <p id="p-93">
                             This project has received funding from the European Research Council (
                             <strong>
                              ERC
                             </strong>
                             ) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 788535).
                            </p>
                           </div>
                           <div class="section fn-group" id="fn-group-1">
                            <h2>
                             Footnotes
                            </h2>
                            <ul>
                             <li class="fn-update fn-group-summary-of-updates" id="fn-2">
                              <p id="p-5">
                               Updated version of the manuscript that was now accepted to a Journal.
                              </p>
                             </li>
                             <li class="fn-dataset fn-group-external-links" id="fn-3">
                              <p id="p-6">
                               <a href="https://github.com/WeizmannVision/SelfSuperReconst">
                                https://github.com/WeizmannVision/SelfSuperReconst
                               </a>
                              </p>
                             </li>
                             <li class="fn-dataset fn-group-external-links" id="fn-4">
                              <p id="p-7">
                               <a href="http://www.wisdom.weizmann.ac.il/~vision/SSReconstnClass">
                                http://www.wisdom.weizmann.ac.il/~vision/SSReconstnClass
                               </a>
                              </p>
                             </li>
                             <li class="fn" id="fn-5">
                              <p id="p-94">
                               <a class="rev-xref" href="#xref-fn-5-1">
                                ↵
                               </a>
                               <span class="fn-label">
                                <sup>
                                 1
                                </sup>
                               </span>
                               Standard Error of the Mean.
                              </p>
                             </li>
                             <li class="fn" id="fn-6">
                              <p id="p-95">
                               <a class="rev-xref" href="#xref-fn-6-1">
                                ↵
                               </a>
                               <span class="fn-label">
                                <sup>
                                 2
                                </sup>
                               </span>
                               ImageNet consists of
                               <span class="underline">
                                15K
                               </span>
                               semantic classes, from which only 1000 classes participate in the ImageNet classification challenge (ILSVRC). In ‘fMRI on ImageNet’ [
                               <a id="xref-ref-43-18" class="xref-bibr" href="#ref-43">
                                43
                               </a>
                               ] which we use, only 20 out of the 50 test classes are included among the ILSVRC classes. The remaining 30 classes are taken from the larger collection of 15K ImageNet classes. Since our gallery is based on the 1000 ILSVRC classes, at train-time we omit the test classes, resulting in 980 train-classes (= 1000 20). At classification test-time, we add the 50 test-labels to the gallery, resulting in 1030 class labels (= 980 + 50).
                              </p>
                             </li>
                             <li class="fn" id="fn-7">
                              <p id="p-96">
                               <a class="rev-xref" href="#xref-fn-7-1">
                                ↵
                               </a>
                               <span class="fn-label">
                                <sup>
                                 3
                                </sup>
                               </span>
                               ‘vim-1’ originally contains 120 test-images, however in the behavioral evaluation we considered only the subset of 20 images that were defined in [
                               <a id="xref-ref-36-11" class="xref-bibr" href="#ref-36">
                                36
                               </a>
                               ] as test-images
                              </p>
                             </li>
                            </ul>
                           </div>
                           <div class="section ref-list" id="ref-list-1">
                            <h2 class="">
                             References
                            </h2>
                            <ol class="cit-list ref-use-labels">
                             <li>
                              <span class="ref-label">
                               1.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-1-1" title="View reference 1. in text" id="ref-1">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.1" data-doi="10.1016/j.neuroimage.2006.06.062">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Thirion
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   E.
                                  </span>
                                  <span class="cit-name-surname">
                                   Duchesnay
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   E.
                                  </span>
                                  <span class="cit-name-surname">
                                   Hubbard
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Dubois
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.-B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Poline
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Lebihan
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Dehaene
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Inverse retinotopy: Inferring the visual content of images from brain activation patterns
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  33
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  1104
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  1116
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  12
                                 </span>
                                 <span class="cit-pub-date">
                                  2006
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.stitle%253DNeuroimage%26rft.aulast%253DThirion%26rft.auinit1%253DB.%26rft.volume%253D33%26rft.issue%253D4%26rft.spage%253D1104%26rft.epage%253D1116%26rft.atitle%253DInverse%2Bretinotopy%253A%2Binferring%2Bthe%2Bvisual%2Bcontent%2Bof%2Bimages%2Bfrom%2Bbrain%2Bactivation%2Bpatterns.%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.neuroimage.2006.06.062%26rft_id%253Dinfo%253Apmid%252F17029988%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.neuroimage.2006.06.062&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=17029988&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000242262900009&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               2.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-2-1" title="View reference 2. in text" id="ref-2">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.2" data-doi="10.1016/j.neuron.2008.11.004">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Miyawaki
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Uchida
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   O.
                                  </span>
                                  <span class="cit-name-surname">
                                   Yamashita
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.-a.
                                  </span>
                                  <span class="cit-name-surname">
                                   Sato
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Morito
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   H. C.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tanabe
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   N.
                                  </span>
                                  <span class="cit-name-surname">
                                   Sadato
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kamitani
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Neuron
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  60
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  915
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  929
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  12
                                 </span>
                                 <span class="cit-pub-date">
                                  2008
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuron%26rft.volume%253D60%26rft.spage%253D915%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.neuron.2008.11.004%26rft_id%253Dinfo%253Apmid%252F19081384%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.neuron.2008.11.004&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=19081384&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000261746700020&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               3.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-3-1" title="View reference 3. in text" id="ref-3">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.3" data-doi="10.1038/nrn1931">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.-D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Haynes
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   Rees
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Decoding mental states from brain activity in humans
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Nature Reviews Neuroscience
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  7
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  523
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  534
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  7
                                 </span>
                                 <span class="cit-pub-date">
                                  2006
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNature%2Breviews.%2BNeuroscience%26rft.stitle%253DNat%2BRev%2BNeurosci%26rft.aulast%253DHaynes%26rft.auinit1%253DJ.%2BD.%26rft.volume%253D7%26rft.issue%253D7%26rft.spage%253D523%26rft.epage%253D534%26rft.atitle%253DDecoding%2Bmental%2Bstates%2Bfrom%2Bbrain%2Bactivity%2Bin%2Bhumans.%26rft_id%253Dinfo%253Adoi%252F10.1038%252Fnrn1931%26rft_id%253Dinfo%253Apmid%252F16791142%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1038/nrn1931&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=16791142&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000238476600013&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               4.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.4" data-doi="10.1038/nn1444">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kamitani
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   F.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tong
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Decoding the visual and subjective contents of the human brain
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Nature Neuroscience
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  8
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  679
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  685
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  5
                                 </span>
                                 <span class="cit-pub-date">
                                  2005
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNature%2Bneuroscience%26rft.stitle%253DNat%2BNeurosci%26rft.aulast%253DKamitani%26rft.auinit1%253DY.%26rft.volume%253D8%26rft.issue%253D5%26rft.spage%253D679%26rft.epage%253D685%26rft.atitle%253DDecoding%2Bthe%2Bvisual%2Band%2Bsubjective%2Bcontents%2Bof%2Bthe%2Bhuman%2Bbrain.%26rft_id%253Dinfo%253Adoi%252F10.1038%252Fnn1444%26rft_id%253Dinfo%253Apmid%252F15852014%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1038/nn1444&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=15852014&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000228895100030&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               5.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.5">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kamitani
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   F.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tong
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Decoding seen and attended motion directions from activity in the human visual cortex
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Current biology : CB
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  16
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  1096
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  1102
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  6
                                 </span>
                                 <span class="cit-pub-date">
                                  2006
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DCurrent%2Bbiology%2B%253A%2BCB%26rft.volume%253D16%26rft.spage%253D1096%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               6.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-6-1" title="View reference 6. in text" id="ref-6">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.6" data-doi="10.1126/science.1063736">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. V.
                                  </span>
                                  <span class="cit-name-surname">
                                   Haxby
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. I.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gobbini
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Furey
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Ishai
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Schouten
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   P.
                                  </span>
                                  <span class="cit-name-surname">
                                   Pietrini
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Distributed and overlapping representations of faces and objects in ventral temporal cortex
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Science
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  293
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  2425
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  2430
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  9
                                 </span>
                                 <span class="cit-pub-date">
                                  2001
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DScience%26rft.stitle%253DScience%26rft.aulast%253DHaxby%26rft.auinit1%253DJ.%2BV.%26rft.volume%253D293%26rft.issue%253D5539%26rft.spage%253D2425%26rft.epage%253D2430%26rft.atitle%253DDistributed%2Band%2BOverlapping%2BRepresentations%2Bof%2BFaces%2Band%2BObjects%2Bin%2BVentral%2BTemporal%2BCortex%26rft_id%253Dinfo%253Adoi%252F10.1126%252Fscience.1063736%26rft_id%253Dinfo%253Apmid%252F11577229%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Mzoic2NpIjtzOjU6InJlc2lkIjtzOjEzOiIyOTMvNTUzOS8yNDI1IjtzOjQ6ImF0b20iO3M6NDg6Ii9iaW9yeGl2L2Vhcmx5LzIwMjIvMDMvMjIvMjAyMC4wOS4wNi4yODQ3OTQuYXRvbSI7fXM6ODoiZnJhZ21lbnQiO3M6MDoiIjt9" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink">
                                 <span>
                                  <span class="cit-reflinks-abstract">
                                   Abstract
                                  </span>
                                  <span class="cit-sep cit-reflinks-variant-name-sep">
                                   /
                                  </span>
                                  <span class="cit-reflinks-full-text">
                                   <span class="free-full-text">
                                    FREE
                                   </span>
                                   Full Text
                                  </span>
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               7.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.7" data-doi="10.1016/S1053-8119(03)00049-1">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   D. D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Cox
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Savoy
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Functional magnetic resonance imaging (fMRI) ”brain reading”: detecting and classifying distributed patterns of fMRI activity in human visual cortex
                                 </span>
                                 .,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  19
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  261
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  70
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  6
                                 </span>
                                 <span class="cit-pub-date">
                                  2003
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.stitle%253DNeuroimage%26rft.aulast%253DCox%26rft.auinit1%253DD.%2BD.%26rft.volume%253D19%26rft.issue%253D2%2BPt%2B1%26rft.spage%253D261%26rft.epage%253D270%26rft.atitle%253DFunctional%2Bmagnetic%2Bresonance%2Bimaging%2B%2528fMRI%2529%2B%2526quot%253Bbrain%2Breading%2526quot%253B%253A%2Bdetecting%2Band%2Bclassifying%2Bdistributed%2Bpatterns%2Bof%2BfMRI%2Bactivity%2Bin%2Bhuman%2Bvisual%2Bcortex.%26rft_id%253Dinfo%253Adoi%252F10.1016%252FS1053-8119%252803%252900049-1%26rft_id%253Dinfo%253Apmid%252F12814577%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/S1053-8119(03)00049-1&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=12814577&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000183568900006&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               8.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.8" data-doi="10.1162/08989290051137549">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   O’Craven
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   N.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kanwisher
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Mental imagery of faces and places activates corresponding stiimulus-specific brain regions
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Journal of cognitive neuroscience
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  12
                                 </span>
                                 , no.
                                 <span class="cit-issue">
                                  6
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  1013
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  1023
                                 </span>
                                 ,
                                 <span class="cit-pub-date">
                                  2000
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DJournal%2Bof%2BCognitive%2BNeuroscience%26rft.stitle%253DJ.%2BCogn.%2BNeurosci.%26rft.aulast%253DO%2527Craven%26rft.auinit1%253DK.%2BM.%26rft.volume%253D12%26rft.issue%253D6%26rft.spage%253D1013%26rft.epage%253D1023%26rft.atitle%253DMental%2BImagery%2Bof%2BFaces%2Band%2BPlaces%2BActivates%2BCorresponding%2BStimulus-Specific%2BBrain%2BRegions%26rft_id%253Dinfo%253Adoi%252F10.1162%252F08989290051137549%26rft_id%253Dinfo%253Apmid%252F11177421%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1162/08989290051137549&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=11177421&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000166295200009&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               9.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-9-1" title="View reference 9. in text" id="ref-9">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.9" data-doi="10.1016/j.neuroimage.2010.07.073">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Naselaris
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K. N.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kay
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Nishimoto
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gallant
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Encoding and decoding in fMRI
                                 </span>
                                 .,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  56
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  400
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  10
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  5
                                 </span>
                                 <span class="cit-pub-date">
                                  2011
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D56%26rft.spage%253D400%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.neuroimage.2010.07.073%26rft_id%253Dinfo%253Apmid%252F20691790%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.neuroimage.2010.07.073&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=20691790&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000290081900003&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               10.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-10-1" title="View reference 10. in text" id="ref-10">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.10" data-doi="10.1016/j.neuroimage.2017.08.005">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. N.
                                  </span>
                                  <span class="cit-name-surname">
                                   Hebart
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   C. I.
                                  </span>
                                  <span class="cit-name-surname">
                                   Baker
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Deconstructing multivariate decoding for the study of brain function
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  180
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  4
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  18
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  10
                                 </span>
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D180%26rft.spage%253D4%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.neuroimage.2017.08.005%26rft_id%253Dinfo%253Apmid%252F28782682%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.neuroimage.2017.08.005&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=28782682&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               11.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-11-1" title="View reference 11. in text" id="ref-11">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.11" data-doi="10.1016/j.neuroimage.2014.10.018">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Naselaris
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   C. A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Olman
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   D. E.
                                  </span>
                                  <span class="cit-name-surname">
                                   Stansbury
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Ugurbil
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gallant
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  105
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  215
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  228
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  1
                                 </span>
                                 <span class="cit-pub-date">
                                  2015
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D105%26rft.spage%253D215%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.neuroimage.2014.10.018%26rft_id%253Dinfo%253Apmid%252F25451480%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.neuroimage.2014.10.018&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=25451480&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               12.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-12-1" title="View reference 12. in text" id="ref-12">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.12">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   Shen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Horikawa
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Majima
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kamitani
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Deep image reconstruction from human brain activity
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  PLOS Computational Biology
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  15
                                 </span>
                                 , p.
                                 <span class="cit-fpage">
                                  e1006633
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  1
                                 </span>
                                 <span class="cit-pub-date">
                                  2019
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DPLOS%2BComputational%2BBiology%26rft.volume%253D15%26rft.spage%253D1006633e%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               13.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-13-1" title="View reference 13. in text" id="ref-13">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.13">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   Shen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Dwivedi
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Majima
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Horikawa
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kamitani
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  End-to-end deep image reconstruction from human brain activity
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Frontiers in Computational Neuroscience
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  13
                                 </span>
                                 , p.
                                 <span class="cit-fpage">
                                  21
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  4
                                 </span>
                                 <span class="cit-pub-date">
                                  2019
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DFrontiers%2Bin%2BComputational%2BNeuroscience%26rft.volume%253D13%26rft.spage%253D21%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               14.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.14" data-doi="10.1093/cercor/bhr106">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Cichy
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Heinzle
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.-D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Haynes
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Imagery and Perception Share Cortical Representations of Content and Location
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Cerebral Cortex
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  22
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  372
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  380
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  2
                                 </span>
                                 <span class="cit-pub-date">
                                  2012
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DCerebral%2BCortex%26rft_id%253Dinfo%253Adoi%252F10.1093%252Fcercor%252Fbhr106%26rft_id%253Dinfo%253Apmid%252F21666128%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1093/cercor/bhr106&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=21666128&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000299124400012&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               15.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-15-1" title="View reference 15. in text" id="ref-15">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.15" data-doi="10.1016/j.tics.2015.08.003">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Pearson
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Naselaris
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   E. A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Holmes
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kosslyn
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Mental Imagery: Functional Mechanisms and Clinical Applications
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Trends in Cognitive Sciences
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  19
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  590
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  602
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  10
                                 </span>
                                 <span class="cit-pub-date">
                                  2015
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DTrends%2Bin%2BCognitive%2BSciences%26rft.volume%253D19%26rft.spage%253D590%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.tics.2015.08.003%26rft_id%253Dinfo%253Apmid%252F26412097%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.tics.2015.08.003&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=26412097&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               16.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-16-1" title="View reference 16. in text" id="ref-16">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.16">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Horikawa
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tamaki
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Miyawaki
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kamitani
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Neural Decoding of Visual Imagery During Sleep
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Science
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  340
                                 </span>
                                 , no.
                                 <span class="cit-issue">
                                  6132
                                 </span>
                                 ,
                                 <span class="cit-pub-date">
                                  2013
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               17.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-17-1" title="View reference 17. in text" id="ref-17">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.17">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Horikawa
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kamitani
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Hierarchical Neural Representation of Dreamed Objects Revealed by Brain Decoding with Deep Neural Network Features
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Frontiers in Computational Neuroscience
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  11
                                 </span>
                                 ,
                                 <span class="cit-pub-date">
                                  2017
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               18.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-18-1" title="View reference 18. in text" id="ref-18">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.18">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tripathy
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Z. E.
                                  </span>
                                  <span class="cit-name-surname">
                                   Markow
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Fishell
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Sherafati
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Burns-Yocum
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Schroeder
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Svoboda
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Eggebrecht
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Anastasio
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Schlaggar
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. P.
                                  </span>
                                  <span class="cit-name-surname">
                                   Culver
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Decoding visual information from high-density diffuse optical tomography neuroimaging data
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  226
                                 </span>
                                 , p.
                                 <span class="cit-fpage">
                                  117516
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  2
                                 </span>
                                 <span class="cit-pub-date">
                                  2021
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D226%26rft.spage%253D117516%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               19.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.19" data-doi="10.1152/jn.00493.2017">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Milekovic
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Sarma
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Bacher
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Simeral
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Saab
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   C.
                                  </span>
                                  <span class="cit-name-surname">
                                   Pandarinath
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Sorice
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   C.
                                  </span>
                                  <span class="cit-name-surname">
                                   Blabe
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   E. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Oakley
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K. R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tringale
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   E.
                                  </span>
                                  <span class="cit-name-surname">
                                   Eskandar
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S. S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Cash
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Henderson
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K. V.
                                  </span>
                                  <span class="cit-name-surname">
                                   Shenoy
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. P.
                                  </span>
                                  <span class="cit-name-surname">
                                   Donoghue
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L. R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Hochberg
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Stable long-term BCI-enabled communication in ALS and locked-in syndrome using LFP signals
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Journal of neurophysiology
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  120
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  343
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  360
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  7
                                 </span>
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DJournal%2Bof%2Bneurophysiology%26rft.volume%253D120%26rft.spage%253D343%26rft_id%253Dinfo%253Adoi%252F10.1152%252Fjn.00493.2017%26rft_id%253Dinfo%253Apmid%252F29694279%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1152/jn.00493.2017&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=29694279&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               20.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-20-1" title="View reference 20. in text" id="ref-20">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.20" data-doi="10.1002/ana.23656">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Naci
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Monti
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Cruse
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kübler
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Sorger
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Goebel
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kotchoubey
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Owen
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Brain–computer interfaces for communication with nonresponsive patients
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Annals of Neurology
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  72
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  312
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  323
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  9
                                 </span>
                                 <span class="cit-pub-date">
                                  2012
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DAnnals%2Bof%2Bneurology%26rft.stitle%253DAnn%2BNeurol%26rft.aulast%253DNaci%26rft.auinit1%253DL.%26rft.volume%253D72%26rft.issue%253D3%26rft.spage%253D312%26rft.epage%253D323%26rft.atitle%253DBrain-computer%2Binterfaces%2Bfor%2Bcommunication%2Bwith%2Bnonresponsive%2Bpatients.%26rft_id%253Dinfo%253Adoi%252F10.1002%252Fana.23656%26rft_id%253Dinfo%253Apmid%252F23034907%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1002/ana.23656&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=23034907&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               21.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-21-1" title="View reference 21. in text" id="ref-21">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.21" data-doi="10.1056/NEJMoa0905370">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Monti
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Vanhaudenhuyse
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Coleman
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Boly
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Pickard
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tshibanda
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Owen
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Laureys
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Willful Modulation of Brain Activity in Disorders of Consciousness
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  New England Journal of Medicine
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  362
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  579
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  589
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  2
                                 </span>
                                 <span class="cit-pub-date">
                                  2010
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNew%2BEngland%2BJournal%2Bof%2BMedicine%26rft_id%253Dinfo%253Adoi%252F10.1056%252FNEJMoa0905370%26rft_id%253Dinfo%253Apmid%252F20130250%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1056/NEJMoa0905370&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=20130250&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000274571200006&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               22.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-22-1" title="View reference 22. in text" id="ref-22">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.22" data-doi="10.1126/science.1130197">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Owen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Coleman
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Boly
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Davis
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Laureys
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Pickard
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Detecting awareness in the vegetative state
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Science
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  313
                                 </span>
                                 , p.
                                 <span class="cit-fpage">
                                  1402
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  9
                                 </span>
                                 <span class="cit-pub-date">
                                  2006
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DScience%26rft.stitle%253DScience%26rft.aulast%253DOwen%26rft.auinit1%253DA.%2BM.%26rft.volume%253D313%26rft.issue%253D5792%26rft.spage%253D1402%26rft.epage%253D1402%26rft.atitle%253DDetecting%2BAwareness%2Bin%2Bthe%2BVegetative%2BState%26rft_id%253Dinfo%253Adoi%252F10.1126%252Fscience.1130197%26rft_id%253Dinfo%253Apmid%252F16959998%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Mzoic2NpIjtzOjU6InJlc2lkIjtzOjEzOiIzMTMvNTc5Mi8xNDAyIjtzOjQ6ImF0b20iO3M6NDg6Ii9iaW9yeGl2L2Vhcmx5LzIwMjIvMDMvMjIvMjAyMC4wOS4wNi4yODQ3OTQuYXRvbSI7fXM6ODoiZnJhZ21lbnQiO3M6MDoiIjt9" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink">
                                 <span>
                                  <span class="cit-reflinks-abstract">
                                   Abstract
                                  </span>
                                  <span class="cit-sep cit-reflinks-variant-name-sep">
                                   /
                                  </span>
                                  <span class="cit-reflinks-full-text">
                                   <span class="free-full-text">
                                    FREE
                                   </span>
                                   Full Text
                                  </span>
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               23.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-23-1" title="View reference 23. in text" id="ref-23">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.23">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Lage-Castellanos
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   Valente
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   E.
                                  </span>
                                  <span class="cit-name-surname">
                                   Formisano
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   F.
                                  </span>
                                  <span class="cit-name-surname">
                                   De Martino
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Methods for computing the maximum performance of computational models of fMRI responses
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  PLOS Computational Biology
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  15
                                 </span>
                                 , p.
                                 <span class="cit-fpage">
                                  e1006397
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  3
                                 </span>
                                 <span class="cit-pub-date">
                                  2019
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DPLOS%2BComputational%2BBiology%26rft.volume%253D15%26rft.spage%253De1006397%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               24.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.24">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G. H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Glover
                                  </span>
                                 </span>
                                 , “
                                 <abbr class="cit-jnl-abbrev">
                                  Overview of functional magnetic resonance imaging
                                 </abbr>
                                 ,”
                                 <span class="cit-month">
                                  4
                                 </span>
                                 <span class="cit-pub-date">
                                  2011
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               25.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-25-1" title="View reference 25. in text" id="ref-25">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.25" data-doi="10.1016/j.neuroimage.2013.04.127">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. F.
                                  </span>
                                  <span class="cit-name-surname">
                                   Glasser
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S. N.
                                  </span>
                                  <span class="cit-name-surname">
                                   Sotiropoulos
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wilson
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T. S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Coalson
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Fischl
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Andersson
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Xu
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Jbabdi
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Webster
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Polimeni
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   D. C.
                                  </span>
                                  <span class="cit-name-surname">
                                   Van Essen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Jenkinson
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth cit-collab">
                                  WU-Minn HCP Consortium
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  The minimal preprocessing pipelines for the Human Connectome Project
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  80
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  105
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  124
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  10
                                 </span>
                                 <span class="cit-pub-date">
                                  2013
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D80%26rft.spage%253D105%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.neuroimage.2013.04.127%26rft_id%253Dinfo%253Apmid%252F23668970%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.neuroimage.2013.04.127&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=23668970&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000322416000011&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               26.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-26-1" title="View reference 26. in text" id="ref-26">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.26" data-doi="10.1038/nature06713">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K. N.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kay
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Naselaris
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R. J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Prenger
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gallant
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Identifying natural images from human brain activity
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Nature
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  452
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  352
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  355
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  3
                                 </span>
                                 <span class="cit-pub-date">
                                  2008
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNature%26rft.stitle%253DNature%26rft.aulast%253DKay%26rft.auinit1%253DK.%2BN.%26rft.volume%253D452%26rft.issue%253D7185%26rft.spage%253D352%26rft.epage%253D355%26rft.atitle%253DIdentifying%2Bnatural%2Bimages%2Bfrom%2Bhuman%2Bbrain%2Bactivity.%26rft_id%253Dinfo%253Adoi%252F10.1038%252Fnature06713%26rft_id%253Dinfo%253Apmid%252F18322462%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1038/nature06713&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=18322462&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000254117400047&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               27.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.27" data-doi="10.1016/j.neuron.2009.09.006">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Naselaris
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R. J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Prenger
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K. N.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kay
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Oliver
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gallant
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Bayesian Reconstruction of Natural Images from Human Brain Activity
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Neuron
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  63
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  902
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  915
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  9
                                 </span>
                                 <span class="cit-pub-date">
                                  2009
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuron%26rft.volume%253D63%26rft.spage%253D902%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.neuron.2009.09.006%26rft_id%253Dinfo%253Apmid%252F19778517%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.neuron.2009.09.006&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=19778517&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000270569700021&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               28.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-28-1" title="View reference 28. in text" id="ref-28">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.28">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Nishimoto
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Vu
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Naselaris
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Benjamini
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Yu
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gallant
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Reconstructing visual experiences from brain activity evoked by natural movies
                                 </span>
                                 .,”
                                 <abbr class="cit-jnl-abbrev">
                                  Current biology : CB
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  21
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  1641
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  6
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  10
                                 </span>
                                 <span class="cit-pub-date">
                                  2011
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DCurrent%2Bbiology%2B%253A%2BCB%26rft.volume%253D21%26rft.spage%253D1641%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               29.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-29-1" title="View reference 29. in text" id="ref-29">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.29" data-doi="10.1093/cercor/bhx268">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Shi
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zhang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.-H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Lu
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Cao
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Z.
                                  </span>
                                  <span class="cit-name-surname">
                                   Liu
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Cerebral Cortex
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  28
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  4136
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  4160
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  12
                                 </span>
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DCerebral%2BCortex%26rft.volume%253D28%26rft.spage%253D4136%26rft_id%253Dinfo%253Adoi%252F10.1093%252Fcercor%252Fbhx268%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1093/cercor/bhx268&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               30.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.30" data-doi="10.1523/JNEUROSCI.5023-14.2015">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   U.
                                  </span>
                                  <span class="cit-name-surname">
                                   Guclu
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. A. J.
                                  </span>
                                  <span class="cit-name-surname">
                                   van Gerven
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   U.
                                  </span>
                                  <span class="cit-name-surname">
                                   Güçlü
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. A. J.
                                  </span>
                                  <span class="cit-name-surname">
                                   van Gerven
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Journal of Neuroscience
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  35
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  10005
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  10014
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  7
                                 </span>
                                 <span class="cit-pub-date">
                                  2015
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DJournal%2Bof%2BNeuroscience%26rft.stitle%253DJ.%2BNeurosci.%26rft.aulast%253DGuclu%26rft.auinit1%253DU.%26rft.volume%253D35%26rft.issue%253D27%26rft.spage%253D10005%26rft.epage%253D10014%26rft.atitle%253DDeep%2BNeural%2BNetworks%2BReveal%2Ba%2BGradient%2Bin%2Bthe%2BComplexity%2Bof%2BNeural%2BRepresentations%2Bacross%2Bthe%2BVentral%2BStream%26rft_id%253Dinfo%253Adoi%252F10.1523%252FJNEUROSCI.5023-14.2015%26rft_id%253Dinfo%253Apmid%252F26157000%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Njoiam5ldXJvIjtzOjU6InJlc2lkIjtzOjExOiIzNS8yNy8xMDAwNSI7czo0OiJhdG9tIjtzOjQ4OiIvYmlvcnhpdi9lYXJseS8yMDIyLzAzLzIyLzIwMjAuMDkuMDYuMjg0Nzk0LmF0b20iO31zOjg6ImZyYWdtZW50IjtzOjA6IiI7fQ==" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink">
                                 <span>
                                  <span class="cit-reflinks-abstract">
                                   Abstract
                                  </span>
                                  <span class="cit-sep cit-reflinks-variant-name-sep">
                                   /
                                  </span>
                                  <span class="cit-reflinks-full-text">
                                   <span class="free-full-text">
                                    FREE
                                   </span>
                                   Full Text
                                  </span>
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               31.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-31-1" title="View reference 31. in text" id="ref-31">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.31">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   C.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zhang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Qiao
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tong
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zeng
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Yan
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Constraint-Free Natural Image Reconstruction From fMRI Signals Based on Convolutional Neural Network
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Frontiers in Human Neuroscience
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  12
                                 </span>
                                 , p.
                                 <span class="cit-fpage">
                                  242
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  6
                                 </span>
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DFrontiers%2Bin%2BHuman%2BNeuroscience%26rft.volume%253D12%26rft.spage%253D242%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               32.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-32-1" title="View reference 32. in text" id="ref-32">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.32" data-doi="10.1016/j.neuroimage.2019.05.039">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Han
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Shi
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.-H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Lu
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zhang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Fu
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Z.
                                  </span>
                                  <span class="cit-name-surname">
                                   Liu
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Variational autoencoder: An unsupervised model for encoding and decoding fMRI activity in visual cortex
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  198
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  125
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  136
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  9
                                 </span>
                                 <span class="cit-pub-date">
                                  2019
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D198%26rft.spage%253D125%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.neuroimage.2019.05.039%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.neuroimage.2019.05.039&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               33.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-33-1" title="View reference 33. in text" id="ref-33">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.33">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Z.
                                  </span>
                                  <span class="cit-name-surname">
                                   Ren
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Li
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   X.
                                  </span>
                                  <span class="cit-name-surname">
                                   Xue
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   X.
                                  </span>
                                  <span class="cit-name-surname">
                                   Li
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   F.
                                  </span>
                                  <span class="cit-name-surname">
                                   Yang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Z.
                                  </span>
                                  <span class="cit-name-surname">
                                   Jiao
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   X.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gao
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  228
                                 </span>
                                 , p.
                                 <span class="cit-fpage">
                                  117602
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  3
                                 </span>
                                 <span class="cit-pub-date">
                                  2021
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D228%26rft.spage%253D117602%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               34.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-34-1" title="View reference 34. in text" id="ref-34">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.34">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Mozafari
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Reddy
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Vanrullen
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  tech. rep
                                 </abbr>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               35.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-35-1" title="View reference 35. in text" id="ref-35">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.35">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Qiao
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Chen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   C.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zhang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tong
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Yan
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  BigGAN-based Bayesian Reconstruction of Natural Images from Human Brain Activity
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Neuroscience
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  444
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  92
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  105
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  7
                                 </span>
                                 <span class="cit-pub-date">
                                  2020
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroscience%26rft.volume%253D444%26rft.spage%253D92%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               36.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-36-1" title="View reference 36. in text" id="ref-36">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-book" id="cit-2020.09.06.284794v2.36">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   St-Yves
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Naselaris
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-chapter-title">
                                  Generative Adversarial Networks Conditioned on Brain Activity Reconstruct Seen Images
                                 </span>
                                 ,” in
                                 <span class="cit-source">
                                  Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  1054
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  1061
                                 </span>
                                 ,
                                 <span class="cit-publ-name">
                                  Institute of Electrical and Electronics Engineers Inc
                                 </span>
                                 .,
                                 <span class="cit-month">
                                  1
                                 </span>
                                 <span class="cit-pub-date">
                                  2019
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               37.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-37-1" title="View reference 37. in text" id="ref-37">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.37">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Seeliger
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   U.
                                  </span>
                                  <span class="cit-name-surname">
                                   Güçlü
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Ambrogioni
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Güçlütürk
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. A.
                                  </span>
                                  <span class="cit-name-surname">
                                   van Gerven
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Generative adversarial networks for reconstructing natural images from brain activity
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  181
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  775
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  785
                                 </span>
                                 ,
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D181%26rft.spage%253D775%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               38.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-38-1" title="View reference 38. in text" id="ref-38">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.38">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Lin
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Li
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wang
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Jiao
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  DCNN-GAN: Reconstructing Realistic Image from fMRI
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  tech. rep.
                                 </abbr>
                                 ,
                                 <span class="cit-pub-date">
                                  2019
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               39.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-39-1" title="View reference 39. in text" id="ref-39">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.39" data-doi="10.1016/j.neuroimage.2016.10.001">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Eickenberg
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gramfort
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   Varoquaux
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Thirion
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Seeing it all: Convolutional network layers map the function of the human visual system
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  152
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  184
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  194
                                 </span>
                                 ,
                                 <span class="cit-pub-date">
                                  2017
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D152%26rft.spage%253D184%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.neuroimage.2016.10.001%26rft_id%253Dinfo%253Apmid%252F27777172%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.neuroimage.2016.10.001&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=27777172&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               40.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.40" data-doi="10.1523/JNEUROSCI.0983-13.2013">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Konkle
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Caramazza
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Tripartite organization of the ventral stream by animacy and object size
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Journal of Neuroscience
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  33
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  10235
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  10242
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  6
                                 </span>
                                 <span class="cit-pub-date">
                                  2013
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DJournal%2Bof%2BNeuroscience%26rft.stitle%253DJ.%2BNeurosci.%26rft.aulast%253DKonkle%26rft.auinit1%253DT.%26rft.volume%253D33%26rft.issue%253D25%26rft.spage%253D10235%26rft.epage%253D10242%26rft.atitle%253DTripartite%2BOrganization%2Bof%2Bthe%2BVentral%2BStream%2Bby%2BAnimacy%2Band%2BObject%2BSize%26rft_id%253Dinfo%253Adoi%252F10.1523%252FJNEUROSCI.0983-13.2013%26rft_id%253Dinfo%253Apmid%252F23785139%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Njoiam5ldXJvIjtzOjU6InJlc2lkIjtzOjExOiIzMy8yNS8xMDIzNSI7czo0OiJhdG9tIjtzOjQ4OiIvYmlvcnhpdi9lYXJseS8yMDIyLzAzLzIyLzIwMjAuMDkuMDYuMjg0Nzk0LmF0b20iO31zOjg6ImZyYWdtZW50IjtzOjA6IiI7fQ==" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink">
                                 <span>
                                  <span class="cit-reflinks-abstract">
                                   Abstract
                                  </span>
                                  <span class="cit-sep cit-reflinks-variant-name-sep">
                                   /
                                  </span>
                                  <span class="cit-reflinks-full-text">
                                   <span class="free-full-text">
                                    FREE
                                   </span>
                                   Full Text
                                  </span>
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               41.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-41-1" title="View reference 41. in text" id="ref-41">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.41">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Shi
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   W.
                                  </span>
                                  <span class="cit-name-surname">
                                   Chen
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Z.
                                  </span>
                                  <span class="cit-name-surname">
                                   Liu
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Deep Residual Network Predicts Cortical Representation and Organization of Visual Features for Rapid Categorization
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Scientific Reports
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  8
                                 </span>
                                 , p.
                                 <span class="cit-fpage">
                                  3752
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  12
                                 </span>
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DScientific%2BReports%26rft.volume%253D8%26rft.spage%253D3752%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               42.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-42-1" title="View reference 42. in text" id="ref-42">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.42">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Qiao
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Chen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   C.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zhang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zeng
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tong
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Yan
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Category decoding of visual stimuli from human brain activity using a bidirectional recurrent neural network to simulate bidirectional information flows in human visual cortices
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Frontiers in Neuroscience
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  13
                                 </span>
                                 , no.
                                 <span class="cit-issue">
                                  JUL
                                 </span>
                                 ,
                                 <span class="cit-pub-date">
                                  2019
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               43.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-43-1" title="View reference 43. in text" id="ref-43">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.43">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Horikawa
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kamitani
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Generic decoding of seen and imagined objects using hierarchical visual features
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Nature Communications
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  8
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  1
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  15
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  5
                                 </span>
                                 <span class="cit-pub-date">
                                  2017
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNature%2BCommunications%26rft.volume%253D8%26rft.spage%253D1%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               44.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-44-1" title="View reference 44. in text" id="ref-44">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.44">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Deng
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   W.
                                  </span>
                                  <span class="cit-name-surname">
                                   Dong
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Socher
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   L.-J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Li
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Kai
                                  </span>
                                  <span class="cit-name-surname">
                                   Li
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Li
                                  </span>
                                  <span class="cit-name-surname">
                                   Fei-Fei
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  ImageNet: A large-scale hierarchical image database
                                 </span>
                                 ,” in
                                 <abbr class="cit-jnl-abbrev">
                                  2009 IEEE Conference on Computer Vision and Pattern Recognition
                                 </abbr>
                                 , pp.
                                 <span class="cit-fpage">
                                  248
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  255
                                 </span>
                                 , IEEE,
                                 <span class="cit-month">
                                  6
                                 </span>
                                 <span class="cit-pub-date">
                                  2009
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               45.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-45-1" title="View reference 45. in text" id="ref-45">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.45">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Beliy
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gaziv
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Hoogi
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   F.
                                  </span>
                                  <span class="cit-name-surname">
                                   Strappini
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Golan
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Irani
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI
                                 </span>
                                 ,” in
                                 <abbr class="cit-jnl-abbrev">
                                  Advances in Neural Information Processing Systems
                                 </abbr>
                                 ,
                                 <span class="cit-pub-date">
                                  2019
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               46.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-46-1" title="View reference 46. in text" id="ref-46">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.46">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zhang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   P.
                                  </span>
                                  <span class="cit-name-surname">
                                   Isola
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Efros
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   E.
                                  </span>
                                  <span class="cit-name-surname">
                                   Shechtman
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   O.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wang
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  The Unreasonable Effectiveness of Deep Features as a Perceptual Metric
                                 </span>
                                 ,” in
                                 <abbr class="cit-jnl-abbrev">
                                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
                                 </abbr>
                                 ,
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               47.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-47-1" title="View reference 47. in text" id="ref-47">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.47">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Simonyan
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zisserman
                                  </span>
                                 </span>
                                 , “
                                 <abbr class="cit-jnl-abbrev">
                                  Very Deep Convolutional Networks for Large-Scale Image Recognition
                                 </abbr>
                                 ,”
                                 <span class="cit-month">
                                  9
                                 </span>
                                 <span class="cit-pub-date">
                                  2014
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               48.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-48-1" title="View reference 48. in text" id="ref-48">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.48">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   St-Yves
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Naselaris
                                  </span>
                                 </span>
                                 , “
                                 <abbr class="cit-jnl-abbrev">
                                  The feature-weighted receptive field: an interpretable encoding model for complex feature spaces
                                 </abbr>
                                 ,”
                                 <span class="cit-pub-date">
                                  2017
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               49.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-49-1" title="View reference 49. in text" id="ref-49">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.49">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   X.
                                  </span>
                                  <span class="cit-name-surname">
                                   Glorot
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Bengio
                                  </span>
                                 </span>
                                 , “
                                 <abbr class="cit-jnl-abbrev">
                                  Understanding the difficulty of training deep feedforward neural networks
                                 </abbr>
                                 ,”
                                 <span class="cit-month">
                                  3
                                 </span>
                                 <span class="cit-pub-date">
                                  2010
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               50.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-50-1" title="View reference 50. in text" id="ref-50">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.50">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   St-Yves
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Naselaris
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  The feature-weighted receptive field: an interpretable encoding model for complex feature spaces
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  180
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  188
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  202
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  10
                                 </span>
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D180%26rft.spage%253D188%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               51.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.51" data-doi="10.1152/jn.1957.20.4.408">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   V. B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Mountcastle
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Modality and topographic properties of single neurons of cat’s somatic sensory cortex
                                 </span>
                                 .,”
                                 <abbr class="cit-jnl-abbrev">
                                  Journal of neurophysiology
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  20
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  408
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  34
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  7
                                 </span>
                                 <span class="cit-pub-date">
                                  1957
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DJournal%2Bof%2BNeurophysiology%26rft.stitle%253DJ.%2BNeurophysiol.%26rft.aulast%253DMountcastle%26rft.auinit1%253DV.%2BB.%26rft.volume%253D20%26rft.issue%253D4%26rft.spage%253D408%26rft.epage%253D434%26rft.atitle%253DMODALITY%2BAND%2BTOPOGRAPHIC%2BPROPERTIES%2BOF%2BSINGLE%2BNEURONS%2BOF%2BCAT%2527S%2BSOMATIC%2BSENSORY%2BCORTEX%26rft_id%253Dinfo%253Adoi%252F10.1152%252Fjn.1957.20.4.408%26rft_id%253Dinfo%253Apmid%252F13439410%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1152/jn.1957.20.4.408&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=13439410&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=A1957WA11500006&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               52.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.52" data-doi="10.1113/jphysiol.2009.171082">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   D. Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Ts’o
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zarella
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   Burkitt
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Whither the hypercolumn?
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  The Journal of physiology
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  587
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  2791
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  805
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  6
                                 </span>
                                 <span class="cit-pub-date">
                                  2009
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DThe%2BJournal%2Bof%2BPhysiology%26rft.stitle%253DJ.%2BPhysiol.%26rft.aulast%253DTs%2527o%26rft.auinit1%253DD.%2BY.%26rft.volume%253D587%26rft.issue%253D12%26rft.spage%253D2791%26rft.epage%253D2805%26rft.atitle%253DWhither%2Bthe%2Bhypercolumn%253F%26rft_id%253Dinfo%253Adoi%252F10.1113%252Fjphysiol.2009.171082%26rft_id%253Dinfo%253Apmid%252F19525564%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1113/jphysiol.2009.171082&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=19525564&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               53.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.53" data-doi="10.1038/353429a0">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T.
                                  </span>
                                  <span class="cit-name-surname">
                                   Bonhoeffer
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Grinvald
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Iso-orientation domains in cat visual cortex are arranged in pinwheel-like patterns
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Nature
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  353
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  429
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  431
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  10
                                 </span>
                                 <span class="cit-pub-date">
                                  1991
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNature%26rft.stitle%253DNature%26rft.aulast%253DBonhoeffer%26rft.auinit1%253DT.%26rft.volume%253D353%26rft.issue%253D6343%26rft.spage%253D429%26rft.epage%253D431%26rft.atitle%253DIso-orientation%2Bdomains%2Bin%2Bcat%2Bvisual%2Bcortex%2Bare%2Barranged%2Bin%2Bpinwheel-like%2Bpatterns.%26rft_id%253Dinfo%253Adoi%252F10.1038%252F353429a0%26rft_id%253Dinfo%253Apmid%252F1896085%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1038/353429a0&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=1896085&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=A1991GH60600056&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               54.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.54" data-doi="10.1093/cercor/12.10.1005">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Dow
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Orientation and Color Columns in Monkey Visual Cortex
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Cerebral Cortex
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  12
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  1005
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  1015
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  10
                                 </span>
                                 <span class="cit-pub-date">
                                  2002
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DCerebral%2BCortex%26rft.stitle%253DCereb%2BCortex%26rft.aulast%253DDow%26rft.auinit1%253DB.%2BM.%26rft.volume%253D12%26rft.issue%253D10%26rft.spage%253D1005%26rft.epage%253D1015%26rft.atitle%253DOrientation%2Band%2BColor%2BColumns%2Bin%2BMonkey%2BVisual%2BCortex%26rft_id%253Dinfo%253Adoi%252F10.1093%252Fcercor%252F12.10.1005%26rft_id%253Dinfo%253Apmid%252F12217963%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1093/cercor/12.10.1005&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=12217963&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=000178000600001&link_type=ISI" class="cit-ref-sprinkles cit-ref-sprinkles-newisilink cit-ref-sprinkles-webofscience">
                                 <span>
                                  Web of Science
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               55.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.55" data-doi="10.1016/j.tics.2015.03.009">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B. A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wandell
                                  </span>
                                 </span>
                                 and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Winawer
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Computational neuroimaging and population receptive fields
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Trends in Cognitive Sciences
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  19
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  349
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  357
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  6
                                 </span>
                                 <span class="cit-pub-date">
                                  2015
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DTrends%2Bin%2BCognitive%2BSciences%26rft.volume%253D19%26rft.spage%253D349%26rft_id%253Dinfo%253Adoi%252F10.1016%252Fj.tics.2015.03.009%26rft_id%253Dinfo%253Apmid%252F25850730%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=10.1016/j.tics.2015.03.009&link_type=DOI" class="cit-ref-sprinkles cit-ref-sprinkles-doi cit-ref-sprinkles-crossref">
                                 <span>
                                  CrossRef
                                 </span>
                                </a>
                                <a href="/lookup/external-ref?access_num=25850730&link_type=MED&atom=%2Fbiorxiv%2Fearly%2F2022%2F03%2F22%2F2020.09.06.284794.atom" class="cit-ref-sprinkles cit-ref-sprinkles-medline">
                                 <span>
                                  PubMed
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               56.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.56">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gomez
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   V.
                                  </span>
                                  <span class="cit-name-surname">
                                   Natu
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Jeska
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Barnett
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Grill-Spector
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Development differentially sculpts receptive fields across early and high-level human visual cortex
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Nature Communications
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  9
                                 </span>
                                 , p.
                                 <span class="cit-fpage">
                                  788
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  12
                                 </span>
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNature%2BCommunications%26rft.volume%253D9%26rft.spage%253D788%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               57.
                              </span>
                              <div class="cit ref-cit ref-journal no-rev-xref" id="cit-2020.09.06.284794v2.57" data-doi="10.1126/science.6291152">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R. B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tootell
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Silverman
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   E.
                                  </span>
                                  <span class="cit-name-surname">
                                   Switkes
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   De Valois
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Deoxyglucose analysis of retinotopic organization in primate striate cortex
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Science
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  218
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  900
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  901
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  11
                                 </span>
                                 <span class="cit-pub-date">
                                  1982
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DScience%26rft.stitle%253DScience%26rft.aulast%253DJanowsky%26rft.auinit1%253DA%26rft.volume%253D218%26rft.issue%253D4575%26rft.spage%253D900%26rft.epage%253D901%26rft.atitle%253DRole%2Bof%2Bserotonergic%2Binput%2Bin%2Bthe%2Bregulation%2Bof%2Bthe%2Bbeta-adrenergic%2Breceptor-coupled%2Badenylate%2Bcyclase%2Bsystem%26rft_id%253Dinfo%253Adoi%252F10.1126%252Fscience.6291152%26rft_id%253Dinfo%253Apmid%252F6291152%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Mzoic2NpIjtzOjU6InJlc2lkIjtzOjEyOiIyMTgvNDU3NS85MDAiO3M6NDoiYXRvbSI7czo0ODoiL2Jpb3J4aXYvZWFybHkvMjAyMi8wMy8yMi8yMDIwLjA5LjA2LjI4NDc5NC5hdG9tIjt9czo4OiJmcmFnbWVudCI7czowOiIiO30=" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink">
                                 <span>
                                  <span class="cit-reflinks-abstract">
                                   Abstract
                                  </span>
                                  <span class="cit-sep cit-reflinks-variant-name-sep">
                                   /
                                  </span>
                                  <span class="cit-reflinks-full-text">
                                   <span class="free-full-text">
                                    FREE
                                   </span>
                                   Full Text
                                  </span>
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               58.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-58-1" title="View reference 58. in text" id="ref-58">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.58" data-doi="10.1126/science.7754376">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M. I.
                                  </span>
                                  <span class="cit-name-surname">
                                   Sereno
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Dale
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Reppas
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K. K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Kwong
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. W.
                                  </span>
                                  <span class="cit-name-surname">
                                   Belliveau
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   T. J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Brady
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   B. R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Rosen
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R. B.
                                  </span>
                                  <span class="cit-name-surname">
                                   Tootell
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Science
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  268
                                 </span>
                                 , no.
                                 <span class="cit-issue">
                                  5212
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  889
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  893
                                 </span>
                                 ,
                                 <span class="cit-pub-date">
                                  1995
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DScience%26rft.stitle%253DScience%26rft.aulast%253DSereno%26rft.auinit1%253DM.%26rft.volume%253D268%26rft.issue%253D5212%26rft.spage%253D889%26rft.epage%253D893%26rft.atitle%253DBorders%2Bof%2Bmultiple%2Bvisual%2Bareas%2Bin%2Bhumans%2Brevealed%2Bby%2Bfunctional%2Bmagnetic%2Bresonance%2Bimaging%26rft_id%253Dinfo%253Adoi%252F10.1126%252Fscience.7754376%26rft_id%253Dinfo%253Apmid%252F7754376%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                                <a href="/lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Mzoic2NpIjtzOjU6InJlc2lkIjtzOjEyOiIyNjgvNTIxMi84ODkiO3M6NDoiYXRvbSI7czo0ODoiL2Jpb3J4aXYvZWFybHkvMjAyMi8wMy8yMi8yMDIwLjA5LjA2LjI4NDc5NC5hdG9tIjt9czo4OiJmcmFnbWVudCI7czowOiIiO30=" class="cit-ref-sprinkles cit-ref-sprinkles-ijlink">
                                 <span>
                                  <span class="cit-reflinks-abstract">
                                   Abstract
                                  </span>
                                  <span class="cit-sep cit-reflinks-variant-name-sep">
                                   /
                                  </span>
                                  <span class="cit-reflinks-full-text">
                                   <span class="free-full-text">
                                    FREE
                                   </span>
                                   Full Text
                                  </span>
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               59.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-59-1" title="View reference 59. in text" id="ref-59">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.59">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Shi
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   W.
                                  </span>
                                  <span class="cit-name-surname">
                                   Chen
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Z.
                                  </span>
                                  <span class="cit-name-surname">
                                   Liu
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Transferring and generalizing deep-learning-based neural encoding models across subjects
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  NeuroImage
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  176
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  152
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  163
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  8
                                 </span>
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNeuroImage%26rft.volume%253D176%26rft.spage%253D152%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               60.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-60-1" title="View reference 60. in text" id="ref-60">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.60">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   H.
                                  </span>
                                  <span class="cit-name-surname">
                                   Wen
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Han
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J.
                                  </span>
                                  <span class="cit-name-surname">
                                   Shi
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Y.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zhang
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   E.
                                  </span>
                                  <span class="cit-name-surname">
                                   Culurciello
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   Z.
                                  </span>
                                  <span class="cit-name-surname">
                                   Liu
                                  </span>
                                 </span>
                                 , “
                                 <abbr class="cit-jnl-abbrev">
                                  Deep Predictive Coding Network for Object Recognition
                                 </abbr>
                                 ,”
                                 <span class="cit-pub-date">
                                  2018
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               61.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-61-1" title="View reference 61. in text" id="ref-61">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.61">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   K.
                                  </span>
                                  <span class="cit-name-surname">
                                   Simonyan
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Vedaldi
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A.
                                  </span>
                                  <span class="cit-name-surname">
                                   Zisserman
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  tech. rep
                                 </abbr>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                               </div>
                              </div>
                             </li>
                             <li>
                              <span class="ref-label">
                               62.
                              </span>
                              <a class="rev-xref-ref" href="#xref-ref-62-1" title="View reference 62. in text" id="ref-62">
                               ↵
                              </a>
                              <div class="cit ref-cit ref-journal" id="cit-2020.09.06.284794v2.62">
                               <div class="cit-metadata">
                                <cite>
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Grossman
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   G.
                                  </span>
                                  <span class="cit-name-surname">
                                   Gaziv
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   E. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Yeagle
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Harel
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   P.
                                  </span>
                                  <span class="cit-name-surname">
                                   Mégevand
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   D. M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Groppe
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   S.
                                  </span>
                                  <span class="cit-name-surname">
                                   Khuvis
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   J. L.
                                  </span>
                                  <span class="cit-name-surname">
                                   Herrero
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   M.
                                  </span>
                                  <span class="cit-name-surname">
                                   Irani
                                  </span>
                                 </span>
                                 ,
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   A. D.
                                  </span>
                                  <span class="cit-name-surname">
                                   Mehta
                                  </span>
                                 </span>
                                 , and
                                 <span class="cit-auth">
                                  <span class="cit-name-given-names">
                                   R.
                                  </span>
                                  <span class="cit-name-surname">
                                   Malach
                                  </span>
                                 </span>
                                 , “
                                 <span class="cit-article-title">
                                  Convergent evolution of face spaces across human face-selective neuronal groups and deep convolutional networks
                                 </span>
                                 ,”
                                 <abbr class="cit-jnl-abbrev">
                                  Nature Communications
                                 </abbr>
                                 , vol.
                                 <span class="cit-vol">
                                  10
                                 </span>
                                 , pp.
                                 <span class="cit-fpage">
                                  1
                                 </span>
                                 –
                                 <span class="cit-lpage">
                                  13
                                 </span>
                                 ,
                                 <span class="cit-month">
                                  12
                                 </span>
                                 <span class="cit-pub-date">
                                  2019
                                 </span>
                                 .
                                </cite>
                               </div>
                               <div class="cit-extra">
                                <a href="{openurl}?query=rft.jtitle%253DNature%2BCommunications%26rft.volume%253D10%26rft.spage%253D1%26rft.genre%253Darticle%26rft_val_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Ajournal%26ctx_ver%253DZ39.88-2004%26url_ver%253DZ39.88-2004%26url_ctx_fmt%253Dinfo%253Aofi%252Ffmt%253Akev%253Amtx%253Actx" class="cit-ref-sprinkles cit-ref-sprinkles-openurl cit-ref-sprinkles-open-url">
                                 <span>
                                  OpenUrl
                                 </span>
                                </a>
                               </div>
                              </div>
                             </li>
                            </ol>
                           </div>
                           <span class="highwire-journal-article-marker-end">
                           </span>
                          </div>
                          <span class="related-urls">
                          </span>
                         </div>
                        </div>
                       </div>
                      </div>
                     </div>
                    </div>
                   </div>
                  </div>
                 </div>
                </div>
               </div>
               <div class="panel-separator">
               </div>
               <div class="panel-pane pane-disqus-comment">
                <div class="pane-content">
                 <div id="disqus_thread">
                  <noscript>
                   <p>
                    <a href="http://biorxivstage.disqus.com/?url=https%3A%2F%2Fwww.biorxiv.org%2Fcontent%2F10.1101%2F2020.09.06.284794v2" class="" data-icon-position="" data-hide-link-title="0">
                     View the discussion thread.
                    </a>
                   </p>
                  </noscript>
                 </div>
                </div>
               </div>
               <div class="panel-separator">
               </div>
               <div class="panel-pane pane-highwire-back-to-top">
                <div class="pane-content">
                 <a href="#page" class="back-to-top" data-icon-position="" data-hide-link-title="0">
                  <span class="icon-chevron-up">
                  </span>
                  Back to top
                 </a>
                </div>
               </div>
              </div>
             </div>
            </div>
            <div class="sidebar-right-wrapper grid-10 omega">
             <div class="panel-panel panel-region-sidebar-right">
              <div class="inside">
               <div class="panel-pane pane-highwire-node-pager">
                <div class="pane-content">
                 <div class="pager highwire-pager pager-mini clearfix highwire-node-pager highwire-article-pager">
                  <span class="pager-prev">
                   <a href="/content/10.1101/2021.03.31.437822v2" title="Simultaneous suppression of ribosome biogenesis and Tor activation by TRIM-NHL proteins promotes terminal differentiation" rel="prev" class="pager-link-prev link-icon">
                    <span class="icon-circle-arrow-left">
                    </span>
                    <span class="title">
                     Previous
                    </span>
                   </a>
                  </span>
                  <span class="pager-next">
                   <a href="/content/10.1101/2021.07.06.451353v2" title="Receptor-binding domain recombinant protein on alum-CpG induces broad protection against SARS-CoV-2 variants of concern" rel="next" class="pager-link-next link-icon-right link-icon">
                    <span class="title">
                     Next
                    </span>
                    <span class="icon-circle-arrow-right">
                    </span>
                   </a>
                  </span>
                 </div>
                </div>
               </div>
               <div class="panel-separator">
               </div>
               <div class="panel-pane pane-custom pane-1">
                <div class="pane-content">
                 Posted March 22, 2022.
                </div>
               </div>
               <div class="panel-separator">
               </div>
               <div class="panel-pane pane-panels-mini pane-biorxiv-art-tools">
                <div class="pane-content">
                 <div id="mini-panel-biorxiv_art_tools" class="highwire-2col-stacked panel-display">
                  <div class="panel-row-wrapper clearfix">
                   <div class="content-left-wrapper content-column">
                    <div class="panel-panel panel-region-content-left">
                     <div class="inside">
                      <div class="panel-pane pane-highwire-variant-link">
                       <div class="pane-content">
                        <a href="/content/10.1101/2020.09.06.284794v2.full.pdf" target="_self" class="article-dl-pdf-link link-icon">
                         <span class="icon-external-link-sign">
                         </span>
                         <span class="title">
                          Download PDF
                         </span>
                        </a>
                       </div>
                      </div>
                      <div class="panel-separator">
                      </div>
                      <div class="panel-pane pane-biorxiv-supplementary-fragment">
                       <div class="pane-content">
                        <p>
                         <a href="/content/10.1101/2020.09.06.284794v2.supplementary-material">
                          <i class="icon-file">
                          </i>
                          Supplementary Material
                         </a>
                        </p>
                       </div>
                      </div>
                      <div class="panel-separator">
                      </div>
                      <div class="panel-pane pane-highwire-variant-link">
                       <div class="pane-content">
                        <a href="/content/early/2022/03/22/2020.09.06.284794.external-links" target="_self" class="link-icon">
                         <span class="icon-file">
                         </span>
                         <span class="title">
                          Data/Code
                         </span>
                        </a>
                       </div>
                      </div>
                     </div>
                    </div>
                   </div>
                   <div class="content-right-wrapper content-column">
                    <div class="panel-panel panel-region-content-right">
                     <div class="inside">
                      <div class="panel-pane pane-minipanel-dialog-link pane-biorxiv-art-email">
                       <div class="pane-content">
                        <div class="minipanel-dialog-wrapper">
                         <div class="minipanel-dialog-link-link">
                          <a href="/" oncontextmenu="javascript: return false;" class="minipanel-dialog-link-trigger" title="Email this Article" data-icon-position="" data-hide-link-title="0">
                           <i class="icon-envelope">
                           </i>
                           Email
                          </a>
                         </div>
                         <div class="minipanel-dialog-link-mini" style="display:none">
                          <div class="panel-display panel-1col clearfix" id="mini-panel-biorxiv_art_email">
                           <div class="panel-panel panel-col">
                            <div>
                             <div class="panel-pane pane-block pane-forward-form pane-forward">
                              <div class="pane-content">
                               <form action="/content/10.1101/2020.09.06.284794v2.full" method="post" id="forward-form" accept-charset="UTF-8">
                                <div>
                                 <div id="edit-instructions" class="form-item form-item-label-before form-type-item">
                                  <p>
                                   Thank you for your interest in spreading the word about bioRxiv.
                                  </p>
                                  <p>
                                   NOTE: Your email address is requested solely to identify you as the sender of this article.
                                  </p>
                                 </div>
                                 <div class="form-item form-item-label-before form-type-textfield form-item-email">
                                  <label for="edit-email">
                                   Your Email
                                   <span class="form-required" title="This field is required.">
                                    *
                                   </span>
                                  </label>
                                  <input type="text" id="edit-email" name="email" value="" size="58" maxlength="256" class="form-text required"/>
                                 </div>
                                 <div class="form-item form-item-label-before form-type-textfield form-item-name">
                                  <label for="edit-name">
                                   Your Name
                                   <span class="form-required" title="This field is required.">
                                    *
                                   </span>
                                  </label>
                                  <input type="text" id="edit-name" name="name" value="" size="58" maxlength="256" class="form-text required"/>
                                 </div>
                                 <div class="form-item form-item-label-before form-type-textarea form-item-recipients">
                                  <label for="edit-recipients">
                                   Send To
                                   <span class="form-required" title="This field is required.">
                                    *
                                   </span>
                                  </label>
                                  <div class="form-textarea-wrapper resizable">
                                   <textarea id="edit-recipients" name="recipients" cols="50" rows="5" class="form-textarea required"></textarea>
                                  </div>
                                  <div class="description">
                                   Enter multiple addresses on separate lines or separate them with commas.
                                  </div>
                                 </div>
                                 <div id="edit-page" class="form-item form-item-label-before form-type-item">
                                  <label for="edit-page">
                                   You are going to email the following
                                  </label>
                                  <a href="/content/10.1101/2020.09.06.284794v2" class="active" data-icon-position="" data-hide-link-title="0">
                                   Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity
                                  </a>
                                 </div>
                                 <div id="edit-subject" class="form-item form-item-label-before form-type-item">
                                  <label for="edit-subject">
                                   Message Subject
                                  </label>
                                  (Your Name) has forwarded a page to you from bioRxiv
                                 </div>
                                 <div id="edit-body" class="form-item form-item-label-before form-type-item">
                                  <label for="edit-body">
                                   Message Body
                                  </label>
                                  (Your Name) thought you would like to see this page from the bioRxiv website.
                                 </div>
                                 <div class="form-item form-item-label-before form-type-textarea form-item-message">
                                  <label for="edit-message--2">
                                   Your Personal Message
                                  </label>
                                  <div class="form-textarea-wrapper resizable">
                                   <textarea id="edit-message--2" name="message" cols="50" rows="10" class="form-textarea"></textarea>
                                  </div>
                                 </div>
                                 <input type="hidden" name="path" value="node/2459645"/>
                                 <input type="hidden" name="path_cid" value=""/>
                                 <input type="hidden" name="forward_footer" value=" "/>
                                 <input type="hidden" name="form_build_id" value="form-j6T6ymOnmK1BcJ-VWNrI68Io518Qo-sH_jcphmymPc8"/>
                                 <input type="hidden" name="form_id" value="forward_form"/>
                                 <fieldset class="captcha form-wrapper">
                                  <legend>
                                   <span class="fieldset-legend">
                                    CAPTCHA
                                   </span>
                                  </legend>
                                  <div class="fieldset-wrapper">
                                   <div class="fieldset-description">
                                    This question is for testing whether or not you are a human visitor and to prevent automated spam submissions.
                                   </div>
                                   <input type="hidden" name="captcha_sid" value="484478487"/>
                                   <input type="hidden" name="captcha_token" value="0286fba98ea2bf878b0f86e33f5668a0"/>
                                   <input type="hidden" name="captcha_response" value="Google no captcha"/>
                                   <div class="g-recaptcha" data-sitekey="6LfnJVIUAAAAAE-bUOMg0MJGki4lqSvDmhJp19fN" data-theme="light" data-type="image">
                                   </div>
                                  </div>
                                 </fieldset>
                                 <div class="form-actions form-wrapper" id="edit-actions">
                                  <input type="submit" id="edit-submit" name="op" value="Send Message" class="form-submit"/>
                                 </div>
                                </div>
                               </form>
                              </div>
                             </div>
                            </div>
                           </div>
                          </div>
                         </div>
                        </div>
                       </div>
                      </div>
                      <div class="panel-separator">
                      </div>
                      <div class="panel-pane pane-highwire-share-link highwire_clipboard_link_ajax" id="shareit">
                       <div class="pane-content">
                        <a href="/" class="link-icon">
                         <span class="icon-share-alt">
                         </span>
                         <span class="title">
                          Share
                         </span>
                        </a>
                       </div>
                      </div>
                      <div class="panel-separator">
                      </div>
                      <div class="panel-pane pane-panels-mini pane-biorxiv-share highwire_clipboard_form_ajax_shareit">
                       <div class="pane-content">
                        <div class="panel-display omega-12-onecol" id="mini-panel-biorxiv_share">
                         <div class="panel-panel grid-12 panel-region-preface">
                          <div class="inside">
                           <div class="panel-pane pane-highwire-article-citation">
                            <div class="pane-content">
                             <div class="highwire-article-citation highwire-citation-type-highwire-article node2459645--3" data-node-nid="2459645" id="node-2459645--42065413011" data-pisa="biorxiv;2020.09.06.284794v2" data-pisa-master="biorxiv;2020.09.06.284794" data-seqnum="2459645" data-apath="/biorxiv/early/2022/03/22/2020.09.06.284794.atom">
                              <div class="highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-pap-list clearfix">
                               <div class="highwire-cite-title">
                                <div class="highwire-cite-title">
                                 Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity
                                </div>
                               </div>
                               <div class="highwire-cite-authors">
                                <span class="highwire-citation-authors">
                                 <span class="highwire-citation-author first" data-delta="0">
                                  <span class="nlm-given-names">
                                   Guy
                                  </span>
                                  <span class="nlm-surname">
                                   Gaziv
                                  </span>
                                 </span>
                                 ,
                                 <span class="highwire-citation-author" data-delta="1">
                                  <span class="nlm-given-names">
                                   Roman
                                  </span>
                                  <span class="nlm-surname">
                                   Beliy
                                  </span>
                                 </span>
                                 ,
                                 <span class="highwire-citation-author" data-delta="2">
                                  <span class="nlm-given-names">
                                   Niv
                                  </span>
                                  <span class="nlm-surname">
                                   Granot
                                  </span>
                                 </span>
                                 ,
                                 <span class="highwire-citation-author" data-delta="3">
                                  <span class="nlm-given-names">
                                   Assaf
                                  </span>
                                  <span class="nlm-surname">
                                   Hoogi
                                  </span>
                                 </span>
                                 ,
                                 <span class="highwire-citation-author" data-delta="4">
                                  <span class="nlm-given-names">
                                   Francesca
                                  </span>
                                  <span class="nlm-surname">
                                   Strappini
                                  </span>
                                 </span>
                                 ,
                                 <span class="highwire-citation-author" data-delta="5">
                                  <span class="nlm-given-names">
                                   Tal
                                  </span>
                                  <span class="nlm-surname">
                                   Golan
                                  </span>
                                 </span>
                                 ,
                                 <span class="highwire-citation-author" data-delta="6">
                                  <span class="nlm-given-names">
                                   Michal
                                  </span>
                                  <span class="nlm-surname">
                                   Irani
                                  </span>
                                 </span>
                                </span>
                               </div>
                               <div class="highwire-cite-metadata">
                                <span class="highwire-cite-metadata-journal highwire-cite-metadata">
                                 bioRxiv
                                </span>
                                <span class="highwire-cite-metadata-pages highwire-cite-metadata">
                                 2020.09.06.284794;
                                </span>
                                <span class="highwire-cite-metadata-doi highwire-cite-metadata">
                                 <span class="doi_label">
                                  doi:
                                 </span>
                                 https://doi.org/10.1101/2020.09.06.284794
                                </span>
                               </div>
                              </div>
                             </div>
                            </div>
                           </div>
                          </div>
                         </div>
                         <div class="panel-panel grid-12 panel-region-content">
                          <div class="inside">
                           <div class="panel-pane pane-highwire-article-clipboard-copy">
                            <div class="pane-content">
                             <div class="clipboard-copy">
                              <span class="label-url">
                               <label for="dynamic">
                                Share This Article:
                               </label>
                              </span>
                              <span class="input-text-url">
                               <input type="text" id="dynamic" value="https://www.biorxiv.org/content/10.1101/2020.09.06.284794v2" size="50"/>
                              </span>
                              <span class="copy-button button">
                               <button id="copy-dynamic" class="clipboardjs-button" data-clipboard-target="#dynamic" data-clipboard-alert-style="tooltip" data-clipboard-alert-text="Copied!">
                                Copy
                               </button>
                              </span>
                             </div>
                            </div>
                           </div>
                          </div>
                         </div>
                         <div class="panel-panel grid-12 panel-region-postscript">
                          <div class="inside">
                           <div class="panel-pane pane-service-links">
                            <div class="pane-content">
                             <div class="service-links">
                              <a href="http://digg.com/submit?phase=2&url=https%3A//www.biorxiv.org/content/10.1101/2020.09.06.284794v2&title=Self-Supervised%20Natural%20Image%20Reconstruction%20and%20Large-Scale%20Semantic%20Classification%20from%20Brain%20Activity" id="digg" title="Digg this post on digg.com" class="service-links-digg" rel="nofollow" data-icon-position="" data-hide-link-title="0">
                               <img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/digg.png" alt="Digg logo"/>
                              </a>
                              <a href="http://reddit.com/submit?url=https%3A//www.biorxiv.org/content/10.1101/2020.09.06.284794v2&title=Self-Supervised%20Natural%20Image%20Reconstruction%20and%20Large-Scale%20Semantic%20Classification%20from%20Brain%20Activity" id="reddit" title="Submit this post on reddit.com" class="service-links-reddit" rel="nofollow" data-icon-position="" data-hide-link-title="0">
                               <img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/reddit.png" alt="Reddit logo"/>
                              </a>
                              <a href="http://twitter.com/share?url=https%3A//www.biorxiv.org/content/10.1101/2020.09.06.284794v2&text=Self-Supervised%20Natural%20Image%20Reconstruction%20and%20Large-Scale%20Semantic%20Classification%20from%20Brain%20Activity" id="twitter" title="Share this on Twitter" class="service-links-twitter" rel="nofollow" data-icon-position="" data-hide-link-title="0">
                               <img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/twitter.png" alt="Twitter logo"/>
                              </a>
                              <a href="http://www.facebook.com/sharer.php?u=https%3A//www.biorxiv.org/content/10.1101/2020.09.06.284794v2&t=Self-Supervised%20Natural%20Image%20Reconstruction%20and%20Large-Scale%20Semantic%20Classification%20from%20Brain%20Activity" id="facebook" title="Share on Facebook" class="service-links-facebook" rel="nofollow" data-icon-position="" data-hide-link-title="0">
                               <img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/fb-blue.png" alt="Facebook logo"/>
                              </a>
                              <a href="http://www.google.com/bookmarks/mark?op=add&bkmk=https%3A//www.biorxiv.org/content/10.1101/2020.09.06.284794v2&title=Self-Supervised%20Natural%20Image%20Reconstruction%20and%20Large-Scale%20Semantic%20Classification%20from%20Brain%20Activity" id="google" title="Bookmark this post on Google" class="service-links-google" rel="nofollow" data-icon-position="" data-hide-link-title="0">
                               <img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/google-32.png" alt="Google logo"/>
                              </a>
                              <a href="http://www.linkedin.com/shareArticle?mini=true&url=https%3A//www.biorxiv.org/content/10.1101/2020.09.06.284794v2&title=Self-Supervised%20Natural%20Image%20Reconstruction%20and%20Large-Scale%20Semantic%20Classification%20from%20Brain%20Activity&summary=&source=bioRxiv" id="linkedin" title="Publish this post to LinkedIn" class="service-links-linkedin" rel="nofollow" data-icon-position="" data-hide-link-title="0">
                               <img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/linkedin-32px.png" alt="LinkedIn logo"/>
                              </a>
                              <a href="http://www.mendeley.com/import/?url=https%3A//www.biorxiv.org/content/10.1101/2020.09.06.284794v2&title=Self-Supervised%20Natural%20Image%20Reconstruction%20and%20Large-Scale%20Semantic%20Classification%20from%20Brain%20Activity" id="mendeley" title="Share on Mendeley" class="service-links-mendeley" rel="nofollow" data-icon-position="" data-hide-link-title="0">
                               <img src="https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/mendeley.png" alt="Mendeley logo"/>
                              </a>
                             </div>
                            </div>
                           </div>
                          </div>
                         </div>
                        </div>
                       </div>
                      </div>
                      <div class="panel-separator">
                      </div>
                      <div class="panel-pane pane-minipanel-dialog-link pane-biorxiv-cite-tool">
                       <div class="pane-content">
                        <div class="minipanel-dialog-wrapper">
                         <div class="minipanel-dialog-link-link">
                          <a href="/" oncontextmenu="javascript: return false;" class="minipanel-dialog-link-trigger link-icon" title="Citation Tools">
                           <span class="icon-globe">
                           </span>
                           <span class="title">
                            Citation Tools
                           </span>
                          </a>
                         </div>
                         <div class="minipanel-dialog-link-mini" style="display:none">
                          <div class="panel-display panel-1col clearfix" id="mini-panel-biorxiv_cite_tool">
                           <div class="panel-panel panel-col">
                            <div>
                             <div class="panel-pane pane-highwire-citation-export">
                              <div class="pane-content">
                               <div class="highwire-citation-export">
                                <div class="highwire-citation-info">
                                 <div class="highwire-article-citation highwire-citation-type-highwire-article cite-tool-node2459645--5" data-node-nid="2459645" id="citation-node-2459645--61896293986" data-pisa="biorxiv;2020.09.06.284794v2" data-pisa-master="biorxiv;2020.09.06.284794" data-seqnum="2459645" data-apath="/biorxiv/early/2022/03/22/2020.09.06.284794.atom">
                                  <div class="highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-pap-list clearfix">
                                   <div class="highwire-cite-title">
                                    <div class="highwire-cite-title">
                                     Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity
                                    </div>
                                   </div>
                                   <div class="highwire-cite-authors">
                                    <span class="highwire-citation-authors">
                                     <span class="highwire-citation-author first" data-delta="0">
                                      <span class="nlm-given-names">
                                       Guy
                                      </span>
                                      <span class="nlm-surname">
                                       Gaziv
                                      </span>
                                     </span>
                                     ,
                                     <span class="highwire-citation-author" data-delta="1">
                                      <span class="nlm-given-names">
                                       Roman
                                      </span>
                                      <span class="nlm-surname">
                                       Beliy
                                      </span>
                                     </span>
                                     ,
                                     <span class="highwire-citation-author" data-delta="2">
                                      <span class="nlm-given-names">
                                       Niv
                                      </span>
                                      <span class="nlm-surname">
                                       Granot
                                      </span>
                                     </span>
                                     ,
                                     <span class="highwire-citation-author" data-delta="3">
                                      <span class="nlm-given-names">
                                       Assaf
                                      </span>
                                      <span class="nlm-surname">
                                       Hoogi
                                      </span>
                                     </span>
                                     ,
                                     <span class="highwire-citation-author" data-delta="4">
                                      <span class="nlm-given-names">
                                       Francesca
                                      </span>
                                      <span class="nlm-surname">
                                       Strappini
                                      </span>
                                     </span>
                                     ,
                                     <span class="highwire-citation-author" data-delta="5">
                                      <span class="nlm-given-names">
                                       Tal
                                      </span>
                                      <span class="nlm-surname">
                                       Golan
                                      </span>
                                     </span>
                                     ,
                                     <span class="highwire-citation-author" data-delta="6">
                                      <span class="nlm-given-names">
                                       Michal
                                      </span>
                                      <span class="nlm-surname">
                                       Irani
                                      </span>
                                     </span>
                                    </span>
                                   </div>
                                   <div class="highwire-cite-metadata">
                                    <span class="highwire-cite-metadata-journal highwire-cite-metadata">
                                     bioRxiv
                                    </span>
                                    <span class="highwire-cite-metadata-pages highwire-cite-metadata">
                                     2020.09.06.284794;
                                    </span>
                                    <span class="highwire-cite-metadata-doi highwire-cite-metadata">
                                     <span class="doi_label">
                                      doi:
                                     </span>
                                     https://doi.org/10.1101/2020.09.06.284794
                                    </span>
                                   </div>
                                  </div>
                                 </div>
                                </div>
                                <div class="highwire-citation-formats">
                                 <h2>
                                  Citation Manager Formats
                                 </h2>
                                 <div class="highwire-citation-formats-links">
                                  <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.spage&rft.epage&rft.atitle=Self-Supervised%20Natural%20Image%20Reconstruction%20and%20Large-Scale%20Semantic%20Classification%20from%20Brain%20Activity&rft.volume&rft.issue&rft.date=2022-01-01%2000%3A00%3A00&rft.stitle&rft.jtitle=bioRxiv&rft.au=Gaziv%2C+Guy&rft.au=Beliy%2C+Roman&rft.au=Granot%2C+Niv&rft.au=Hoogi%2C+Assaf&rft.au=Strappini%2C+Francesca&rft.au=Golan%2C+Tal&rft.au=Irani%2C+Michal">
                                  </span>
                                  <ul class="hw-citation-links inline button button-alt button-grid clearfix">
                                   <li class="bibtext first">
                                    <a href="/highwire/citation/2459645/bibtext" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     BibTeX
                                    </a>
                                   </li>
                                   <li class="bookends">
                                    <a href="/highwire/citation/2459645/bookends" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     Bookends
                                    </a>
                                   </li>
                                   <li class="easybib">
                                    <a href="/highwire/citation/2459645/easybib" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     EasyBib
                                    </a>
                                   </li>
                                   <li class="endnote-tagged">
                                    <a href="/highwire/citation/2459645/endnote-tagged" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     EndNote (tagged)
                                    </a>
                                   </li>
                                   <li class="endnote-8-xml">
                                    <a href="/highwire/citation/2459645/endnote-8-xml" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     EndNote 8 (xml)
                                    </a>
                                   </li>
                                   <li class="medlars">
                                    <a href="/highwire/citation/2459645/medlars" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     Medlars
                                    </a>
                                   </li>
                                   <li class="mendeley">
                                    <a href="/highwire/citation/2459645/mendeley" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     Mendeley
                                    </a>
                                   </li>
                                   <li class="papers">
                                    <a href="/highwire/citation/2459645/papers" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     Papers
                                    </a>
                                   </li>
                                   <li class="refworks-tagged">
                                    <a href="/highwire/citation/2459645/refworks-tagged" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     RefWorks Tagged
                                    </a>
                                   </li>
                                   <li class="reference-manager">
                                    <a href="/highwire/citation/2459645/reference-manager" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     Ref Manager
                                    </a>
                                   </li>
                                   <li class="ris">
                                    <a href="/highwire/citation/2459645/ris" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     RIS
                                    </a>
                                   </li>
                                   <li class="zotero last">
                                    <a href="/highwire/citation/2459645/zotero" class="hw-download-citation-link" data-icon-position="" data-hide-link-title="0">
                                     Zotero
                                    </a>
                                   </li>
                                  </ul>
                                 </div>
                                </div>
                               </div>
                              </div>
                             </div>
                            </div>
                           </div>
                          </div>
                         </div>
                        </div>
                       </div>
                      </div>
                     </div>
                    </div>
                   </div>
                  </div>
                 </div>
                </div>
               </div>
               <div class="panel-separator">
               </div>
               <div class="panel-pane pane-service-links">
                <div class="pane-content">
                 <div class="service-links">
                  <div class="item-list">
                   <ul>
                    <li class="first">
                     <a href="http://twitter.com/share?url=https%3A//www.biorxiv.org/content/10.1101/2020.09.06.284794v2&count=horizontal&via=&text=Self-Supervised%20Natural%20Image%20Reconstruction%20and%20Large-Scale%20Semantic%20Classification%20from%20Brain%20Activity&counturl=https%3A//www.biorxiv.org/content/10.1101/2020.09.06.284794v2" class="twitter-share-button service-links-twitter-widget" id="twitter_widget" title="Tweet This" rel="nofollow" data-icon-position="" data-hide-link-title="0">
                      <span class="element-invisible">
                       Tweet Widget
                      </span>
                     </a>
                    </li>
                    <li>
                     <a href="http://www.facebook.com/plugins/like.php?href=https%3A//www.biorxiv.org/content/10.1101/2020.09.06.284794v2&layout=button_count&show_faces=false&action=like&colorscheme=light&width=100&height=21&font=&locale=" id="facebook_like" title="I Like it" class="service-links-facebook-like" rel="nofollow" data-icon-position="" data-hide-link-title="0">
                      <span class="element-invisible">
                       Facebook Like
                      </span>
                     </a>
                    </li>
                    <li class="last">
                     <a href="https://www.biorxiv.org/content/10.1101/2020.09.06.284794v2" id="google_plus_one" title="Plus it" class="service-links-google-plus-one" rel="nofollow" data-icon-position="" data-hide-link-title="0">
                      <span class="element-invisible">
                       Google Plus One
                      </span>
                     </a>
                    </li>
                   </ul>
                  </div>
                 </div>
                </div>
               </div>
               <div class="panel-separator">
               </div>
               <div class="panel-pane pane-highwire-article-collections">
                <h2 class="pane-title">
                 Subject Area
                </h2>
                <div class="pane-content">
                 <div class="highwire-list-wrapper highwire-article-collections">
                  <div class="highwire-list">
                   <ul class="highwire-article-collection-term-list">
                    <li class="first last odd">
                     <span class="highwire-article-collection-term">
                      <a href="/collection/neuroscience" class="highlight" data-icon-position="" data-hide-link-title="0">
                       Neuroscience
                       <i class="icon-caret-right">
                       </i>
                      </a>
                     </span>
                    </li>
                   </ul>
                  </div>
                 </div>
                </div>
               </div>
               <div class="panel-separator">
               </div>
               <div class="panel-pane pane-panels-mini pane-biorxiv-subject-collections block-style-col2">
                <div class="pane-content">
                 <div class="panel-flexible panels-flexible-new clearfix" id="mini-panel-biorxiv_subject_collections">
                  <div class="panel-flexible-inside panels-flexible-new-inside">
                   <div class="panels-flexible-region panels-flexible-region-new-center panels-flexible-region-first panels-flexible-region-last">
                    <div class="inside panels-flexible-region-inside panels-flexible-region-new-center-inside panels-flexible-region-inside-first panels-flexible-region-inside-last">
                     <div class="panel-pane pane-snippet">
                      <div class="pane-content">
                       <div class="snippet biorxiv-subject-areas-table-title" id="biorxiv-subject-areas-table-title">
                        <div class="snippet-content">
                         <b>
                          Subject Areas
                         </b>
                        </div>
                       </div>
                      </div>
                     </div>
                     <div class="panel-separator">
                     </div>
                     <div class="panel-pane pane-snippet">
                      <div class="pane-content">
                       <div class="snippet biorxiv-subject-areas-view-papers" id="biorxiv-subject-areas-view-papers">
                        <div class="snippet-content">
                         <a href="/content/early/recent">
                          <strong>
                           All Articles
                          </strong>
                         </a>
                        </div>
                       </div>
                      </div>
                     </div>
                     <div class="panel-separator">
                     </div>
                     <div class="panel-pane pane-highwire-subject-collections">
                      <div class="pane-content">
                       <ul id="collection" class="collection highwire-list-expand">
                        <li class="outer collection depth-2 child first">
                         <div class="data-wrapper">
                          <a href="/collection/animal-behavior-and-cognition" class="" data-icon-position="" data-hide-link-title="0">
                           Animal Behavior and Cognition
                          </a>
                          <span class="article-count">
                           (4123)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/biochemistry" class="" data-icon-position="" data-hide-link-title="0">
                           Biochemistry
                          </a>
                          <span class="article-count">
                           (8837)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/bioengineering" class="" data-icon-position="" data-hide-link-title="0">
                           Bioengineering
                          </a>
                          <span class="article-count">
                           (6539)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/bioinformatics" class="" data-icon-position="" data-hide-link-title="0">
                           Bioinformatics
                          </a>
                          <span class="article-count">
                           (23507)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/biophysics" class="" data-icon-position="" data-hide-link-title="0">
                           Biophysics
                          </a>
                          <span class="article-count">
                           (11824)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/cancer-biology" class="" data-icon-position="" data-hide-link-title="0">
                           Cancer Biology
                          </a>
                          <span class="article-count">
                           (9242)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/cell-biology" class="" data-icon-position="" data-hide-link-title="0">
                           Cell Biology
                          </a>
                          <span class="article-count">
                           (13358)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/clinical-trials" class="" data-icon-position="" data-hide-link-title="0">
                           Clinical Trials
                          </a>
                          <span class="article-count">
                           (138)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/developmental-biology" class="" data-icon-position="" data-hide-link-title="0">
                           Developmental Biology
                          </a>
                          <span class="article-count">
                           (7456)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/ecology" class="" data-icon-position="" data-hide-link-title="0">
                           Ecology
                          </a>
                          <span class="article-count">
                           (11440)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/epidemiology" class="" data-icon-position="" data-hide-link-title="0">
                           Epidemiology
                          </a>
                          <span class="article-count">
                           (2066)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/evolutionary-biology" class="" data-icon-position="" data-hide-link-title="0">
                           Evolutionary Biology
                          </a>
                          <span class="article-count">
                           (15189)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/genetics" class="" data-icon-position="" data-hide-link-title="0">
                           Genetics
                          </a>
                          <span class="article-count">
                           (10460)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/genomics" class="" data-icon-position="" data-hide-link-title="0">
                           Genomics
                          </a>
                          <span class="article-count">
                           (14058)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/immunology" class="" data-icon-position="" data-hide-link-title="0">
                           Immunology
                          </a>
                          <span class="article-count">
                           (9197)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/microbiology" class="" data-icon-position="" data-hide-link-title="0">
                           Microbiology
                          </a>
                          <span class="article-count">
                           (22243)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/molecular-biology" class="" data-icon-position="" data-hide-link-title="0">
                           Molecular Biology
                          </a>
                          <span class="article-count">
                           (8840)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/neuroscience" class="" data-icon-position="" data-hide-link-title="0">
                           Neuroscience
                          </a>
                          <span class="article-count">
                           (47703)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/paleontology" class="" data-icon-position="" data-hide-link-title="0">
                           Paleontology
                          </a>
                          <span class="article-count">
                           (352)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/pathology" class="" data-icon-position="" data-hide-link-title="0">
                           Pathology
                          </a>
                          <span class="article-count">
                           (1433)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/pharmacology-and-toxicology" class="" data-icon-position="" data-hide-link-title="0">
                           Pharmacology and Toxicology
                          </a>
                          <span class="article-count">
                           (2493)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/physiology" class="" data-icon-position="" data-hide-link-title="0">
                           Physiology
                          </a>
                          <span class="article-count">
                           (3742)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/plant-biology" class="" data-icon-position="" data-hide-link-title="0">
                           Plant Biology
                          </a>
                          <span class="article-count">
                           (8103)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/scientific-communication-and-education" class="" data-icon-position="" data-hide-link-title="0">
                           Scientific Communication and Education
                          </a>
                          <span class="article-count">
                           (1439)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/synthetic-biology" class="" data-icon-position="" data-hide-link-title="0">
                           Synthetic Biology
                          </a>
                          <span class="article-count">
                           (2230)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child">
                         <div class="data-wrapper">
                          <a href="/collection/systems-biology" class="" data-icon-position="" data-hide-link-title="0">
                           Systems Biology
                          </a>
                          <span class="article-count">
                           (6050)
                          </span>
                         </div>
                        </li>
                        <li class="outer collection depth-2 child last">
                         <div class="data-wrapper">
                          <a href="/collection/zoology" class="" data-icon-position="" data-hide-link-title="0">
                           Zoology
                          </a>
                          <span class="article-count">
                           (1259)
                          </span>
                         </div>
                        </li>
                       </ul>
                      </div>
                     </div>
                    </div>
                   </div>
                  </div>
                 </div>
                </div>
               </div>
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </section>
  </div>
  <div class="region region-page-bottom" id="region-page-bottom">
   <div class="region-inner region-page-bottom-inner">
   </div>
  </div>
  <script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__VNH5GD8Zz7g6_hGCOZjVjdMndxxC6naiExSSJKX3k_A__kzmEdtbqCsJx5Z9yg2Qy0PptB__ufXm-TxUOl0KZzUw__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.js">
  </script>
  <script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__2WRbxlwOW0MEUc_hSWU5MBepQg6Lch6O5SZwefpJ6IE__HCL0YQJqLkOhrLPZZYGqosGvtFsEHMGghHIkSx4y9vA__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.js" defer="defer">
  </script>
  <script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__N7ERJBYsOWyRnJgyoM125_Aiez2MOJGaUofG1JdWWBg__2cpwCQ7-xzTVeVvg_KOzwA1jka23oWApDPpgjoZKDCY__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.js">
  </script>
  <script type="text/javascript" src="//d33xdlntwy0kbs.cloudfront.net/cshl_custom.js">
  </script>
  <script type="text/javascript" src="https://www.biorxiv.org/sites/default/files/advagg_js/js__BmqjBnkz3MgYCAoc25s1lDRMEjLhC3mEPVonUFIHi08__Unwv5-ZIuHBfFwytsjEx1niBVJ7n1T4lPws7VrkHXM4__9MyDscWUfs9SQBQB2wQzcEwFsvz4ksctP725Io-yBDU.js">
  </script>
  <script type="text/javascript">
   &lt;!--//--&gt;&lt;![CDATA[//&gt;&lt;!--
function euCookieComplianceLoadScripts() {}
//--&gt;&lt;!]]&gt;
  </script>
  <script type="text/javascript">
   &lt;!--//--&gt;&lt;![CDATA[//&gt;&lt;!--
var eu_cookie_compliance_cookie_name = "";
//--&gt;&lt;!]]&gt;
  </script>
 </body>
</html>
