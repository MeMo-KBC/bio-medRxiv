{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.supervision import Labeler\n",
    "from fonduer.supervision.models import GoldLabel\n",
    "from fonduer.features import Featurizer\n",
    "from fonduer.candidates.models import Candidate\n",
    "from fonduer.parser.models import Document\n",
    "\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "from MeMoKBC.pipeline.utils import get_session, load_candidates, match_label_matrix\n",
    "from MeMoKBC.definitions.candidates import NameFullAbbr, NameAbbrTask\n",
    "from MeMoKBC.pipeline.lfs.name_short_long_lfs import short_long_lfs\n",
    "from MeMoKBC.pipeline.lfs.name_short_task_lfs import name_abbr_task_lfs\n",
    "from MeMoKBC.gold_label_matcher import match_gold_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_session(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [NameFullAbbr, NameAbbrTask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load goldlabels from json file and compare to candidates in database\n",
    "gold_labels = match_gold_label(\n",
    "    \"pipeline6\",\n",
    "    \"/data/Goldlabel_biomedRxiv/goldlabel1_docs801-840_laura/goldlabel_authorlong_short_task_medRxiv.json\",\n",
    "    [NameAbbrTask, NameFullAbbr]\n",
    ")\n",
    "\n",
    "# filter potential goldlabels after candidate class\n",
    "nat_cands = []\n",
    "nfa_cands = []\n",
    "for cand in gold_labels:\n",
    "    if type(cand) == NameAbbrTask:\n",
    "        # remove candidates where short and long name are not in the same sentence\n",
    "        if cand[0].context.sentence.id == cand[1].context.sentence.id:\n",
    "            # append the id of the candidate to the list\n",
    "            nat_cands.append(cand.id)\n",
    "    elif type(cand) == NameFullAbbr:\n",
    "        # append the id of the candidate to the list\n",
    "        nfa_cands.append(cand.id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that know which Candidates are our GoldLabels we still need to inform Fonduer of this. To be able to write this information into the db Fonduer needs a funktion that labels these candidates as Goldlabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labeler object\n",
    "labeler = Labeler(session, candidates)\n",
    "\n",
    "# write function that returns gold label for a candidate\n",
    "def gold(c: Candidate) -> int:\n",
    "\n",
    "    if type(c) == NameAbbrTask:\n",
    "\n",
    "        # check if the candidate id is inside the list of goldlabel candidate id's\n",
    "        if c.id in nat_cands:\n",
    "            return 1\n",
    "\n",
    "    elif type(c) == NameFullAbbr:\n",
    "        \n",
    "        # check if the candidate id is inside the list of goldlabel candidate id's\n",
    "        if c.id in nfa_cands:\n",
    "            return 1\n",
    "\n",
    "    # if the candidate id is not inside the list of goldlabel candidate id's return FALSE\n",
    "    return 0\n",
    "\n",
    "# Apply the gold label function for each candidate class\n",
    "labeler.apply(lfs=[[gold], [gold]], table=GoldLabel, train=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to train our Model we need to tell it a set of candidates and their Goldlabels. This input is in the form of two list where each entry represents a candidate. For each candidate their are multiple entries that represent the outcome of the different Labeling functions. The same applies to the GoldLabels, we need a complete list of all candidates where each entry represents the output of the gold function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load candidates\n",
    "train_cands = load_candidates(session, 0, candidates)\n",
    "\n",
    "# match the candidates with the outcome of the labeling functions to generate input for the label model\n",
    "L_train_NFA, L_train_NAT = match_label_matrix(session, candidates, 0) \n",
    "\n",
    " # load gold labels list\n",
    "L_gold_train_NFA, L_gold_train_NAT = labeler.get_gold_labels(train_cands)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Label Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "gen_model_NFA = LabelModel(cardinality=2)\n",
    "gen_model_NAT = LabelModel(cardinality=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the output of the gold labels is 2 dimensional e.g [[0], [1], [0]] we still need to reshape the goldlabels with .reshape(-1) to reduce 1 dimension which will result in -> [0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_NFA.fit(L_train=L_train_NFA, Y_dev=L_gold_train_NFA.reshape(-1), n_epochs=500, log_freq=100)\n",
    "gen_model_NFA.save(destination=\"models/label_model_NFA.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_NAT.fit(L_train=L_train_NAT, Y_dev=L_gold_train_NAT.reshape(-1), n_epochs=500, log_freq=100)\n",
    "gen_model_NAT.save(destination=\"models/label_model_NAT.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating train marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals_NFA = gen_model_NFA.predict_proba(L_train_NFA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals_NAT = gen_model_NAT.predict_proba(L_train_NAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(12)\n",
    "fig.set_tight_layout(\"w_pad\")\n",
    "\n",
    "bins=20\n",
    "\n",
    "ax[0].hist(np.max(train_marginals_NFA, axis=1), bins=bins)\n",
    "ax[0].set_title(\"NFA(TRUE)\")\n",
    "\n",
    "ax[1].hist(np.max(train_marginals_NAT, axis=1), bins=bins)\n",
    "ax[1].set_title(\"NAT(TRUE)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate on LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = Labeler(session, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cands = load_candidates(session, split=1, candidate_list=candidates)\n",
    "\n",
    "L_dev_NFA, L_dev_NAT = match_label_matrix(session, candidates, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# calculate the diff between true and false prediction probability of each candidate\n",
    "# the bigger the difference the more certain the model is\n",
    "# Example True = 0.4 False = 0.6\n",
    "# diff = 0.6 - 0.4 = 0.2 --> model is very unsure \n",
    "diffs_NAT = train_marginals_NAT.max(axis=1) - train_marginals_NAT.min(axis=1)\n",
    "diffs_NFA = train_marginals_NFA.max(axis=1) - train_marginals_NFA.min(axis=1)\n",
    "\n",
    "# filter out all candidates where labelmodel is very unsure\n",
    "# unsure is a diff of smaller then 0.000001\n",
    "train_idxs_NAT = np.where(diffs_NAT > 0.2)[0].astype(np.int64)\n",
    "train_idxs_NFA = np.where(diffs_NFA > 0.2)[0].astype(np.int64)\n",
    "\n",
    "filtered_NAT = train_marginals_NAT[train_idxs_NAT, 1]\n",
    "filtered_NFA = train_marginals_NFA[train_idxs_NFA, 1]\n",
    "\n",
    "# Cast continous values to binary for logistic regression model\n",
    "y_NAT = np.where(filtered_NAT > 0.5, 1, 0)\n",
    "y_NFA = np.where(filtered_NFA > 0.5, 1, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature matrix and filter with previous filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = Featurizer(session, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_train_NFA, F_train_NAT = featurizer.get_feature_matrices(train_cands)\n",
    "X_NFA = F_train_NFA[train_idxs_NFA, :]\n",
    "X_NAT = F_train_NAT[train_idxs_NAT, :]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, BayesianRidge\n",
    "\n",
    "clf = LogisticRegression(max_iter=200).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cands = load_candidates(session, 2, candidates)\n",
    "F_test_NFT = featurizer.get_feature_matrices(test_cands)[0]\n",
    "\n",
    "preds = clf.predict(F_test_NFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = BayesianRidge().fit(X.toarray(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
