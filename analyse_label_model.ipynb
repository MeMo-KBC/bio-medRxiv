{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelfunction and model analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.supervision import Labeler\n",
    "from fonduer.features import Featurizer\n",
    "from fonduer.candidates.models import Candidate\n",
    "\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "from MeMoKBC.pipeline.utils import get_session, load_candidates, match_label_matrix\n",
    "from MeMoKBC.definitions.candidates import NameFullAbbr, NameAbbrTask\n",
    "from MeMoKBC.pipeline.lfs.name_short_long_lfs import short_long_lfs\n",
    "from MeMoKBC.pipeline.lfs.name_short_task_lfs import name_abbr_task_lfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-09 14:55:39,667][INFO] fonduer.meta:49 - Setting logging directory to: /tmp/2023-06-09_14-55-39\n"
     ]
    }
   ],
   "source": [
    "session = get_session(db_name=\"pipeline2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define candidates and Labeler object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [NameAbbrTask, NameFullAbbr]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load candidates and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/bio-medRxiv/MeMoKBC/src/MeMoKBC/pipeline/utils.py:55: SAWarning: Coercing Subquery object into a select() for use in IN(); please pass a select() construct explicitly\n",
      "  cands = (session.query(candidate_class).filter(candidate_class.id.in_(sub_query)).order_by(candidate_class.id).all())\n"
     ]
    }
   ],
   "source": [
    "L_train_NAT, L_train_NFA = match_label_matrix(session, candidates, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LF analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NameFull + Abrreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = Labeler(session, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>check_all_uppercase_letters</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name_full_in_top_percentile_sentence_wise</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.256985</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name_short_outside_half_percentile_sentence_wise</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.016922</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>small_letter_count</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.069067</td>\n",
       "      <td>0.050964</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>4</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.097993</td>\n",
       "      <td>0.054309</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  j Polarity  Coverage  \\\n",
       "check_all_uppercase_letters                       0       []  0.000000   \n",
       "name_full_in_top_percentile_sentence_wise         1      [1]  0.256985   \n",
       "name_short_outside_half_percentile_sentence_wise  2      [1]  0.016922   \n",
       "small_letter_count                                3      [1]  0.069067   \n",
       "word_count                                        4      [1]  0.097993   \n",
       "\n",
       "                                                  Overlaps  Conflicts  \n",
       "check_all_uppercase_letters                       0.000000        0.0  \n",
       "name_full_in_top_percentile_sentence_wise         0.042700        0.0  \n",
       "name_short_outside_half_percentile_sentence_wise  0.006887        0.0  \n",
       "small_letter_count                                0.050964        0.0  \n",
       "word_count                                        0.054309        0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(\n",
    "    L_train_NFA,\n",
    "    lfs=sorted(short_long_lfs, key=lambda lf: lf.name)\n",
    ").lf_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NameAbbr + Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lf_length_more_than_three_words</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_name_short_in_first_words</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 j Polarity  Coverage  Overlaps  Conflicts\n",
       "lf_length_more_than_three_words  0       []  0.000000       0.0        0.0\n",
       "lf_name_short_in_first_words     1      [0]  0.018817       0.0        0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(\n",
    "    L_train_NAT,\n",
    "    lfs=sorted(name_abbr_task_lfs, key=lambda lf: lf.name)\n",
    ").lf_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis !! continue when LFs are ready !!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of models\n",
    "\n",
    "| Modelname | Candidate | description | n_epochs |\n",
    "| --------- | --------- | ----------- | -------- |\n",
    "| label_model_v1_NFA | NameFullAbbr | label model with random label functions | 500 |\n",
    "| label_model_v1_NFT | NameFullTask | label model with random label functions | 500 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L_train_NFT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoaded Models\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     gen_model_NFT\u001b[39m.\u001b[39mfit(L_train_NFT, n_epochs\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, log_freq\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     13\u001b[0m     gen_model_NFT\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mmodels/label_model_NFT_v1.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     gen_model_NFA\u001b[39m.\u001b[39mfit(L_train_NFA, n_epochs\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, log_freq\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L_train_NFT' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "gen_model_NFT = LabelModel(cardinality=2)\n",
    "gen_model_NFA = LabelModel(cardinality=2)\n",
    "\n",
    "\n",
    "if Path(\"models/label_model_NFA_v1.pkl\").is_file() and Path(\"models/label_model_NFT_v1.pkl\").is_file():\n",
    "    gen_model_NFA.load(source=\"models/label_model_NFA_v1.pkl\")\n",
    "    gen_model_NFT.load(source=\"models/label_model_NFT_v1.pkl\")\n",
    "    print(\"Loaded Models\")\n",
    "else:\n",
    "    gen_model_NFT.fit(L_train_NFT, n_epochs=500, log_freq=100)\n",
    "    gen_model_NFT.save(\"models/label_model_NFT_v1.pkl\")\n",
    "\n",
    "    gen_model_NFA.fit(L_train_NFA, n_epochs=500, log_freq=100)\n",
    "    gen_model_NFA.save(\"models/label_model_NFA_v1.pkl\")\n",
    "    print(\"Fit and saved models\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating train marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals_NFA = gen_model_NFA.predict_proba(L_train_NFA)\n",
    "train_marginals_NFT = gen_model_NFT.predict_proba(L_train_NFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(12)\n",
    "fig.set_tight_layout(\"w_pad\")\n",
    "\n",
    "bins=20\n",
    "\n",
    "ax[0].hist(train_marginals_NFA[:, 0], bins=bins)\n",
    "ax[0].set_title(\"NFA(TRUE)\")\n",
    "\n",
    "ax[1].hist(train_marginals_NFT[:, 0], bins=bins)\n",
    "ax[1].set_title(\"NFT(TRUE)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate on LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cands = load_candidates(session, split=1, candidate_list=candidates)\n",
    "\n",
    "L_dev_NFA, L_dev_NFT = labeler.get_label_matrices(dev_cands)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract words from train_cands and count them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# calculate the diff between true and false prediction probability of each candidate\n",
    "# the bigger the difference the more certain the model is\n",
    "# Example True = 0.4 False = 0.6\n",
    "# diff = 0.6 - 0.4 = 0.2 --> model is very unsure \n",
    "diffs = train_marginals_NFT.max(axis=1) - train_marginals_NFT.min(axis=1)\n",
    "\n",
    "# filter out all candidates where labelmodel is very unsure\n",
    "# unsure is a diff of smaller then 0.000001\n",
    "train_idxs = np.where(diffs > 0.2)[0].astype(np.int64)\n",
    "filtered = train_marginals_NFT[train_idxs, 1]\n",
    "\n",
    "# Cast continous values to binary for logistic regression model\n",
    "y = np.where(filtered > 0.5, 1, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature matrix and filter with previous filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = Featurizer(session, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_train_NFT = featurizer.get_feature_matrices(train_cands)[0]\n",
    "X = F_train_NFT[train_idxs, :]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, BayesianRidge\n",
    "\n",
    "clf = LogisticRegression(max_iter=200).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cands = load_candidates(session, 2, candidates)\n",
    "F_test_NFT = featurizer.get_feature_matrices(test_cands)[0]\n",
    "\n",
    "preds = clf.predict(F_test_NFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = BayesianRidge().fit(X.toarray(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
